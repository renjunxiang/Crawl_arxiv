[
    {
        "title": "Rethinking Video Generation Model for the Embodied World",
        "authors": [
            "Yufan Deng",
            "Zilin Pan",
            "Hongyu Zhang",
            "Xiaojie Li",
            "Ruoqing Hu",
            "Yufei Ding",
            "Yiming Zou",
            "Yan Zeng",
            "Daquan Zhou"
        ],
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15282v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15282v1",
        "summary": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.",
        "category": "RBench任务评测",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15282v1【RBench任务评测】Rethinking Video Generation Model for the Embodied World.pdf",
        "institution": "北京大学、字节跳动"
    },
    {
        "title": "Iterative Refinement Improves Compositional Image Generation",
        "authors": [
            "Shantanu Jaiswal",
            "Mihir Prabhudesai",
            "Nikash Bhardwaj",
            "Zheyang Qin",
            "Amir Zadeh",
            "Chuan Li",
            "Katerina Fragkiadaki",
            "Deepak Pathak"
        ],
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15286v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15286v1",
        "summary": "Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/",
        "category": "迭代自修正生成",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15286v1【迭代自修正生成】Iterative Refinement Improves Compositional Image Generation.pdf",
        "institution": "Carnegie Mellon University、Lambda AI"
    },
    {
        "title": "Walk through Paintings: Egocentric World Models from Internet Priors",
        "authors": [
            "Anurag Bagchi",
            "Zhipeng Bao",
            "Homanga Bharadhwaj",
            "Yu-Xiong Wang",
            "Pavel Tokmakov",
            "Martial Hebert"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15284v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15284v1",
        "summary": "What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation world models, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.",
        "category": "模型架构",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15284v1【模型架构】Walk through Paintings_ Egocentric World Models from Internet Priors.pdf",
        "institution": "Carnegie Mellon University、University of Illinois Urbana-Champaign、Toyota Research Institute"
    },
    {
        "title": "StableWorld: Towards Stable and Consistent Long Interactive Video Generation",
        "authors": [
            "Ying Yang",
            "Zhengyao Lv",
            "Tianlin Pan",
            "Haofan Wang",
            "Binxin Yang",
            "Hubery Yin",
            "Chen Li",
            "Ziwei Liu",
            "Chenyang Si"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15281v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15281v1",
        "summary": "In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \\textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \\eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.",
        "category": "Agent",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15281v1【Agent】StableWorld_ Towards Stable and Consistent Long Interactive Video Generation.pdf",
        "institution": "PRLab、NJU、HKU、UCAS、WeChat、Tencent Inc.、NTU"
    },
    {
        "title": "Large-Scale Multidimensional Knowledge Profiling of Scientific Literature",
        "authors": [
            "Zhucun Xue",
            "Jiangning Zhang",
            "Juntao Jiang",
            "Jinzhuo Liu",
            "Haoyang He",
            "Teng Hu",
            "Xiaobin Hu",
            "Guangming Yao",
            "Yi Yuan",
            "Yong Liu"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15170v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15170v1",
        "summary": "The rapid expansion of research across machine learning, vision, and language has produced a volume of publications that is increasingly difficult to synthesize. Traditional bibliometric tools rely mainly on metadata and offer limited visibility into the semantic content of papers, making it hard to track how research themes evolve over time or how different areas influence one another. To obtain a clearer picture of recent developments, we compile a unified corpus of more than 100,000 papers from 22 major conferences between 2020 and 2025 and construct a multidimensional profiling pipeline to organize and analyze their textual content. By combining topic clustering, LLM-assisted parsing, and structured retrieval, we derive a comprehensive representation of research activity that supports the study of topic lifecycles, methodological transitions, dataset and model usage patterns, and institutional research directions. Our analysis highlights several notable shifts, including the growth of safety, multimodal reasoning, and agent-oriented studies, as well as the gradual stabilization of areas such as neural machine translation and graph-based methods. These findings provide an evidence-based view of how AI research is evolving and offer a resource for understanding broader trends and identifying emerging directions. Code and dataset: https://github.com/xzc-zju/Profiling_Scientific_Literature",
        "category": "Agent",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15170v1【Agent】Large-Scale Multidimensional Knowledge Profiling of Scientific Literature.pdf",
        "institution": "浙江大学、上海交通大学、新加坡国立大学、蚂蚁集团"
    },
    {
        "title": "Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning",
        "authors": [
            "Yuval Kansal",
            "Niraj K. Jha"
        ],
        "categories": [
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15160v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15160v1",
        "summary": "Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a \"compositional bridge\", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.",
        "category": "强化学习",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15160v1【强化学习】Knowledge Graphs are Implicit Reward Models_ Path-Derived Signals Enable Compositional Reasoning.pdf",
        "institution": "Princeton University"
    },
    {
        "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
        "authors": [
            "Zanlin Ni",
            "Shenzhi Wang",
            "Yang Yue",
            "Tianyu Yu",
            "Weilin Zhao",
            "Yeguo Hua",
            "Tianyi Chen",
            "Jun Song",
            "Cheng Yu",
            "Bo Zheng",
            "Gao Huang"
        ],
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15165v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15165v1",
        "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap",
        "category": "强化学习",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15165v1【强化学习】The Flexibility Trap_ Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models.pdf",
        "institution": "Tsinghua University、Alibaba Group"
    },
    {
        "title": "CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning",
        "authors": [
            "Tianshi Xu",
            "Yuteng Chen",
            "Meng Li"
        ],
        "categories": [
            "cs.LG"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15141v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15141v1",
        "summary": "Agentic Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to utilize tools like Python interpreters for complex problem-solving. However, for parameter-constrained models (e.g., 4B--7B), the exploration phase is often plagued by frequent execution failures, creating noisy trajectories that hinder policy optimization. Under standard outcome-based reward settings, this noise leads to a critical credit assignment issue, where erroneous actions are inadvertently reinforced alongside successful outcomes. Existing mitigations face a dilemma: dense rewards often trigger reward hacking, while supersampling incurs prohibitive computational costs. To address these challenges, we propose CLEANER. Distinct from external filtering methods, CLEANER exploits the model's intrinsic self-correction capabilities to eliminate error-contaminated context directly during data collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR) mechanism autonomously constructs clean, purified trajectories by retrospectively replacing failures with successful self-corrections. Based on semantic similarity, SAAR adaptively regulates replacement granularity from shallow execution repairs to deep reasoning substitutions. By training on these self-purified paths, the model internalizes correct reasoning patterns rather than error-recovery loops. Empirical results on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains of 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art performance using only one-third of the training steps, highlighting trajectory purification as a scalable solution for efficient agentic RL. Our models and code are available at GitHub",
        "category": "Agent强化学习",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15141v1【Agent强化学习】CLEANER_ Self-Purified Trajectories Boost Agentic Reinforcement Learning.pdf",
        "institution": "北京大学、南洋理工大学"
    },
    {
        "title": "The Pictorial Cortex: Zero-Shot Cross-Subject fMRI-to-Image Reconstruction via Compositional Latent Modeling",
        "authors": [
            "Jingyang Huo",
            "Yikai Wang",
            "Yanwei Fu",
            "Jianfeng Feng"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15071v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15071v1",
        "summary": "Decoding visual experiences from human brain activity remains a central challenge at the intersection of neuroscience, neuroimaging, and artificial intelligence. A critical obstacle is the inherent variability of cortical responses: neural activity elicited by the same visual stimulus differs across individuals and trials due to anatomical, functional, cognitive, and experimental factors, making fMRI-to-image reconstruction non-injective. In this paper, we tackle a challenging yet practically meaningful problem: zero-shot cross-subject fMRI-to-image reconstruction, where the visual experience of a previously unseen individual must be reconstructed without subject-specific training. To enable principled evaluation, we present a unified cortical-surface dataset -- UniCortex-fMRI, assembled from multiple visual-stimulus fMRI datasets to provide broad coverage of subjects and stimuli. Our UniCortex-fMRI is particularly processed by standardized data formats to make it possible to explore this possibility in the zero-shot scenario of cross-subject fMRI-to-image reconstruction. To tackle the modeling challenge, we propose PictorialCortex, which models fMRI activity using a compositional latent formulation that structures stimulus-driven representations under subject-, dataset-, and trial-related variability. PictorialCortex operates in a universal cortical latent space and implements this formulation through a latent factorization-composition module, reinforced by paired factorization and re-factorizing consistency regularization. During inference, surrogate latents synthesized under multiple seen-subject conditions are aggregated to guide diffusion-based image synthesis for unseen subjects. Extensive experiments show that PictorialCortex improves zero-shot cross-subject visual reconstruction, highlighting the benefits of compositional latent modeling and multi-dataset training.",
        "category": "模型架构",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15071v1【模型架构】The Pictorial Cortex_ Zero-Shot Cross-Subject fMRI-to-Image Reconstruction via Compositional Latent Modeling.pdf",
        "institution": "复旦大学、上海交通大学、浙江大学、南洋理工大学"
    },
    {
        "title": "Visual and Cognitive Demands of a Large Language Model-Powered In-vehicle Conversational Agent",
        "authors": [
            "Chris Monk",
            "Allegra Ayala",
            "Christine S. P. Yu",
            "Gregory M. Fitch",
            "Dara Gruber"
        ],
        "categories": [
            "cs.HC",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15034v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15034v1",
        "summary": "Driver distraction remains a leading contributor to motor vehicle crashes, necessitating rigorous evaluation of new in-vehicle technologies. This study assessed the visual and cognitive demands associated with an advanced Large Language Model (LLM) conversational agent (Gemini Live) during on-road driving, comparing it against handsfree phone calls, visual turn-by-turn guidance (low load baseline), and the Operation Span (OSPAN) task (high load anchor). Thirty-two licensed drivers completed five secondary tasks while visual and cognitive demands were measured using the Detection Response Task (DRT) for cognitive load, eye-tracking for visual attention, and subjective workload ratings. Results indicated that Gemini Live interactions (both single-turn and multi-turn) and hands-free phone calls shared similar levels of cognitive load, between that of visual turn-by-turn guidance and OSPAN. Exploratory analysis showed that cognitive load remained stable across extended multi-turn conversations. All tasks maintained mean glance durations well below the well-established 2-second safety threshold, confirming low visual demand. Furthermore, drivers consistently dedicated longer glances to the roadway between brief off-road glances toward the device during task completion, particularly during voice-based interactions, rendering longer total-eyes-off-road time findings less consequential. Subjective ratings mirrored objective data, with participants reporting low effort, demands, and perceived distraction for Gemini Live. These findings demonstrate that advanced LLM conversational agents, when implemented via voice interfaces, impose cognitive and visual demands comparable to established, low-risk hands-free benchmarks, supporting their safe deployment in the driving environment.",
        "category": "Agent",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15034v1【Agent】Visual and Cognitive Demands of a Large Language Model-Powered In-vehicle Conversational Agent.pdf",
        "institution": "Google、Inc."
    },
    {
        "title": "LiViBench: An Omnimodal Benchmark for Interactive Livestream Video Understanding",
        "authors": [
            "Xiaodong Wang",
            "Langling Huang",
            "Zhirong Wu",
            "Xu Zhao",
            "Teng Xu",
            "Xuhong Xia",
            "Peixi Peng"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15016v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15016v1",
        "summary": "The development of multimodal large language models (MLLMs) has advanced general video understanding. However, existing video evaluation benchmarks primarily focus on non-interactive videos, such as movies and recordings. To fill this gap, this paper proposes the first omnimodal benchmark for interactive livestream videos, LiViBench. It features a diverse set of 24 tasks, highlighting the perceptual, reasoning, and livestream-specific challenges. To efficiently construct the dataset, we design a standardized semi-automatic annotation workflow that incorporates the human-in-the-loop at multiple stages. The workflow leverages multiple MLLMs to form a multi-agent system for comprehensive video description and uses a seed-question-driven method to construct high-quality annotations. All interactive videos in the benchmark include audio, speech, and real-time comments modalities. To enhance models' understanding of interactive videos, we design tailored two-stage instruction-tuning and propose a Video-to-Comment Retrieval (VCR) module to improve the model's ability to utilize real-time comments. Based on these advancements, we develop LiVi-LLM-7B, an MLLM with enhanced knowledge of interactive livestreams. Experiments show that our model outperforms larger open-source models with up to 72B parameters, narrows the gap with leading proprietary models on LiViBench, and achieves enhanced performance on general video benchmarks, including VideoMME, LongVideoBench, MLVU, and VideoEval-Pro.",
        "category": "LiViBench任务评测",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15016v1【LiViBench任务评测】LiViBench_ An Omnimodal Benchmark for Interactive Livestream Video Understanding.pdf",
        "institution": "Peking University、Douyin Group"
    },
    {
        "title": "CorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning",
        "authors": [
            "Zhiyuan Lu",
            "Chenliang Li",
            "Yingcheng Shi",
            "Weizhou Shen",
            "Ming Yan",
            "Fei Huang"
        ],
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14952v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14952v1",
        "summary": "While large language models now handle million-token contexts, their capacity for reasoning across entire document repositories remains largely untested. Existing benchmarks are inadequate, as they are mostly limited to single long texts or rely on a \"sparse retrieval\" assumption-that answers can be derived from a few relevant chunks. This assumption fails for true corpus-level analysis, where evidence is highly dispersed across hundreds of documents and answers require global integration, comparison, and statistical aggregation. To address this critical gap, we introduce CorpusQA, a new benchmark scaling up to 10 million tokens, generated via a novel data synthesis framework. By decoupling reasoning from textual representation, this framework creates complex, computation-intensive queries with programmatically guaranteed ground-truth answers, challenging systems to perform holistic reasoning over vast, unstructured text without relying on fallible human annotation. We further demonstrate the utility of our framework beyond evaluation, showing that fine-tuning on our synthesized data effectively enhances an LLM's general long-context reasoning capabilities. Extensive experiments reveal that even state-of-the-art long-context LLMs struggle as input length increases, and standard retrieval-augmented generation systems collapse entirely. Our findings indicate that memory-augmented agentic architectures offer a more robust alternative, suggesting a critical shift is needed from simply extending context windows to developing advanced architectures for global information synthesis.",
        "category": "10百万token语料库评测",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14952v1【10百万token语料库评测】CorpusQA_ A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning.pdf",
        "institution": "阿里、Alibaba Group"
    },
    {
        "title": "InstructTime++: Time Series Classification with Multimodal Language Modeling via Implicit Feature Enhancement",
        "authors": [
            "Mingyue Cheng",
            "Xiaoyu Tao",
            "Huajian Zhang",
            "Qi Liu",
            "Enhong Chen"
        ],
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14968v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14968v1",
        "summary": "Most existing time series classification methods adopt a discriminative paradigm that maps input sequences directly to one-hot encoded class labels. While effective, this paradigm struggles to incorporate contextual features and fails to capture semantic relationships among classes. To address these limitations, we propose InstructTime, a novel framework that reformulates time series classification as a multimodal generative task. Specifically, continuous numerical sequences, contextual textual features, and task instructions are treated as multimodal inputs, while class labels are generated as textual outputs by tuned language models. To bridge the modality gap, InstructTime introduces a time series discretization module that converts continuous sequences into discrete temporal tokens, together with an alignment projection layer and a generative self-supervised pre-training strategy to enhance cross-modal representation alignment. Building upon this framework, we further propose InstructTime++, which extends InstructTime by incorporating implicit feature modeling to compensate for the limited inductive bias of language models. InstructTime++ leverages specialized toolkits to mine informative implicit patterns from raw time series and contextual inputs, including statistical feature extraction and vision-language-based image captioning, and translates them into textual descriptions for seamless integration. Extensive experiments on multiple benchmark datasets demonstrate the superior performance of InstructTime++.",
        "category": "模型架构",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14968v1【模型架构】InstructTime++_ Time Series Classification with Multimodal Language Modeling via Implicit Feature Enhancement.pdf",
        "institution": "中国科学技术大学、State Key Laboratory of Cognitive Intelligence、University of Science and Technology of China"
    },
    {
        "title": "Improving Regret Approximation for Unsupervised Dynamic Environment Generation",
        "authors": [
            "Harry Mead",
            "Bruno Lacerda",
            "Jakob Foerster",
            "Nick Hawes"
        ],
        "categories": [
            "cs.LG"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14957v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14957v1",
        "summary": "Unsupervised Environment Design (UED) seeks to automatically generate training curricula for reinforcement learning (RL) agents, with the goal of improving generalisation and zero-shot performance. However, designing effective curricula remains a difficult problem, particularly in settings where small subsets of environment parameterisations result in significant increases in the complexity of the required policy. Current methods struggle with a difficult credit assignment problem and rely on regret approximations that fail to identify challenging levels, both of which are compounded as the size of the environment grows. We propose Dynamic Environment Generation for UED (DEGen) to enable a denser level generator reward signal, reducing the difficulty of credit assignment and allowing for UED to scale to larger environment sizes. We also introduce a new regret approximation, Maximised Negative Advantage (MNA), as a significantly improved metric to optimise for, that better identifies more challenging levels. We show empirically that MNA outperforms current regret approximations and when combined with DEGen, consistently outperforms existing methods, especially as the size of the environment grows. We have made all our code available here: https://github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED.",
        "category": "强化学习",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14957v1【强化学习】Improving Regret Approximation for Unsupervised Dynamic Environment Generation.pdf",
        "institution": "University of Oxford"
    },
    {
        "title": "Towards Holistic Modeling for Video Frame Interpolation with Auto-regressive Diffusion Transformers",
        "authors": [
            "Xinyu Peng",
            "Han Li",
            "Yuyang Huang",
            "Ziyang Zheng",
            "Yaoming Wang",
            "Xin Chen",
            "Wenrui Dai",
            "Chenglin Li",
            "Junni Zou",
            "Hongkai Xiong"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14959v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14959v1",
        "summary": "Existing video frame interpolation (VFI) methods often adopt a frame-centric approach, processing videos as independent short segments (e.g., triplets), which leads to temporal inconsistencies and motion artifacts. To overcome this, we propose a holistic, video-centric paradigm named \\textbf{L}ocal \\textbf{D}iffusion \\textbf{F}orcing for \\textbf{V}ideo \\textbf{F}rame \\textbf{I}nterpolation (LDF-VFI). Our framework is built upon an auto-regressive diffusion transformer that models the entire video sequence to ensure long-range temporal coherence. To mitigate error accumulation inherent in auto-regressive generation, we introduce a novel skip-concatenate sampling strategy that effectively maintains temporal stability. Furthermore, LDF-VFI incorporates sparse, local attention and tiled VAE encoding, a combination that not only enables efficient processing of long sequences but also allows generalization to arbitrary spatial resolutions (e.g., 4K) at inference without retraining. An enhanced conditional VAE decoder, which leverages multi-scale features from the input video, further improves reconstruction fidelity. Empirically, LDF-VFI achieves state-of-the-art performance on challenging long-sequence benchmarks, demonstrating superior per-frame quality and temporal consistency, especially in scenes with large motion. The source code is available at https://github.com/xypeng9903/LDF-VFI.",
        "category": "模型架构",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14959v1【模型架构】Towards Holistic Modeling for Video Frame Interpolation with Auto-regressive Diffusion Transformers.pdf",
        "institution": "上海交通大学、美团"
    },
    {
        "title": "What Should I Cite? A RAG Benchmark for Academic Citation Prediction",
        "authors": [
            "Leqi Zheng",
            "Jiajun Zhang",
            "Canzhi Chen",
            "Chaokun Wang",
            "Hongwei Li",
            "Yuying Li",
            "Yaoxin Mao",
            "Shannan Yan",
            "Zixin Song",
            "Zhiyuan Feng",
            "Zhaolu Kang",
            "Zirong Chen",
            "Hang Zhang",
            "Qiang Liu",
            "Liang Wang",
            "Ziyang Liu"
        ],
        "categories": [
            "cs.IR"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14949v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14949v1",
        "summary": "With the rapid growth of Web-based academic publications, more and more papers are being published annually, making it increasingly difficult to find relevant prior work. Citation prediction aims to automatically suggest appropriate references, helping scholars navigate the expanding scientific literature. Here we present \\textbf{CiteRAG}, the first comprehensive retrieval-augmented generation (RAG)-integrated benchmark for evaluating large language models on academic citation prediction, featuring a multi-level retrieval strategy, specialized retrievers, and generators. Our benchmark makes four core contributions: (1) We establish two instances of the citation prediction task with different granularity. Task 1 focuses on coarse-grained list-specific citation prediction, while Task 2 targets fine-grained position-specific citation prediction. To enhance these two tasks, we build a dataset containing 7,267 instances for Task 1 and 8,541 instances for Task 2, enabling comprehensive evaluation of both retrieval and generation. (2) We construct a three-level large-scale corpus with 554k papers spanning many major subfields, using an incremental pipeline. (3) We propose a multi-level hybrid RAG approach for citation prediction, fine-tuning embedding models with contrastive learning to capture complex citation relationships, paired with specialized generation models. (4) We conduct extensive experiments across state-of-the-art language models, including closed-source APIs, open-source models, and our fine-tuned generators, demonstrating the effectiveness of our framework. Our open-source toolkit enables reproducible evaluation and focuses on academic literature, providing the first comprehensive evaluation framework for citation prediction and serving as a methodological template for other scientific domains. Our source code and data are released at https://github.com/LQgdwind/CiteRAG.",
        "category": "RAG任务评测",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14949v1【RAG任务评测】What Should I Cite_ A RAG Benchmark for Academic Citation Prediction.pdf",
        "institution": "清华大学、中国科学技术大学、北京理工大学、北京大学、中国科学院自动化研究所"
    },
    {
        "title": "Multi-Behavior Sequential Modeling with Transition-Aware Graph Attention Network for E-Commerce Recommendation",
        "authors": [
            "Hanqi Jin",
            "Gaoming Yang",
            "Zhangming Chan",
            "Yapeng Yuan",
            "Longbin Li",
            "Fei Sun",
            "Yeqiu Yang",
            "Jian Wu",
            "Yuning Jiang",
            "Bo Zheng"
        ],
        "categories": [
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14955v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14955v1",
        "summary": "User interactions on e-commerce platforms are inherently diverse, involving behaviors such as clicking, favoriting, adding to cart, and purchasing. The transitions between these behaviors offer valuable insights into user-item interactions, serving as a key signal for understanding evolving preferences. Consequently, there is growing interest in leveraging multi-behavior data to better capture user intent. Recent studies have explored sequential modeling of multi-behavior data, many relying on transformer-based architectures with polynomial time complexity. While effective, these approaches often incur high computational costs, limiting their applicability in large-scale industrial systems with long user sequences. To address this challenge, we propose the Transition-Aware Graph Attention Network (TGA), a linear-complexity approach for modeling multi-behavior transitions. Unlike traditional transformers that treat all behavior pairs equally, TGA constructs a structured sparse graph by identifying informative transitions from three perspectives: (a) item-level transitions, (b) category-level transitions, and (c) neighbor-level transitions. Built upon the structured graph, TGA employs a transition-aware graph Attention mechanism that jointly models user-item interactions and behavior transition types, enabling more accurate capture of sequential patterns while maintaining computational efficiency. Experiments show that TGA outperforms all state-of-the-art models while significantly reducing computational cost. Notably, TGA has been deployed in a large-scale industrial production environment, where it leads to impressive improvements in key business metrics.",
        "category": "模型架构",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14955v1【模型架构】Multi-Behavior Sequential Modeling with Transition-Aware Graph Attention Network for E-Commerce Recommendation.pdf",
        "institution": "阿里、中国科学院大学"
    },
    {
        "title": "CodeDelegator: Mitigating Context Pollution via Role Separation in Code-as-Action Agents",
        "authors": [
            "Tianxiang Fei",
            "Cheng Chen",
            "Yue Pan",
            "Mao Zheng",
            "Mingyang Song"
        ],
        "categories": [
            "cs.CL"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14914v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14914v1",
        "summary": "Recent advances in large language models (LLMs) allow agents to represent actions as executable code, offering greater expressivity than traditional tool-calling. However, real-world tasks often demand both strategic planning and detailed implementation. Using a single agent for both leads to context pollution from debugging traces and intermediate failures, impairing long-horizon performance. We propose CodeDelegator, a multi-agent framework that separates planning from implementation via role specialization. A persistent Delegator maintains strategic oversight by decomposing tasks, writing specifications, and monitoring progress without executing code. For each sub-task, a new Coder agent is instantiated with a clean context containing only its specification, shielding it from prior failures. To coordinate between agents, we introduce Ephemeral-Persistent State Separation (EPSS), which isolates each Coder's execution state while preserving global coherence, preventing debugging traces from polluting the Delegator's context. Experiments on various benchmarks demonstrate the effectiveness of CodeDelegator across diverse scenarios.",
        "category": "Agent",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14914v1【Agent】CodeDelegator_ Mitigating Context Pollution via Role Separation in Code-as-Action Agents.pdf",
        "institution": "Tencent"
    },
    {
        "title": "PodBench: A Comprehensive Benchmark for Instruction-Aware Audio-Oriented Podcast Script Generation",
        "authors": [
            "Chenning Xu",
            "Mao Zheng",
            "Mingyu Zheng",
            "Mingyang Song"
        ],
        "categories": [
            "cs.CL"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14903v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14903v1",
        "summary": "Podcast script generation requires LLMs to synthesize structured, context-grounded dialogue from diverse inputs, yet systematic evaluation resources for this task remain limited. To bridge this gap, we introduce PodBench, a benchmark comprising 800 samples with inputs up to 21K tokens and complex multi-speaker instructions. We propose a multifaceted evaluation framework that integrates quantitative constraints with LLM-based quality assessment. Extensive experiments reveal that while proprietary models generally excel, open-source models equipped with explicit reasoning demonstrate superior robustness in handling long contexts and multi-speaker coordination compared to standard baselines. However, our analysis uncovers a persistent divergence where high instruction following does not guarantee high content substance. PodBench offers a reproducible testbed to address these challenges in long-form, audio-centric generation.",
        "category": "PodBench任务评测",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14903v1【PodBench任务评测】PodBench_ A Comprehensive Benchmark for Instruction-Aware Audio-Oriented Podcast Script Generation.pdf",
        "institution": "Tencent"
    },
    {
        "title": "HiNS: Hierarchical Negative Sampling for More Comprehensive Memory Retrieval Embedding Model",
        "authors": [
            "Motong Tian",
            "Allen P. Wong",
            "Mingjun Mao",
            "Wangchunshu Zhou"
        ],
        "categories": [
            "cs.CL"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14857v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14857v1",
        "summary": "Memory-augmented language agents rely on embedding models for effective memory retrieval. However, existing training data construction overlooks a critical limitation: the hierarchical difficulty of negative samples and their natural distribution in human-agent interactions. In practice, some negatives are semantically close distractors while others are trivially irrelevant, and natural dialogue exhibits structured proportions of these types. Current approaches using synthetic or uniformly sampled negatives fail to reflect this diversity, limiting embedding models' ability to learn nuanced discrimination essential for robust memory retrieval. In this work, we propose a principled data construction framework HiNS that explicitly models negative sample difficulty tiers and incorporates empirically grounded negative ratios derived from conversational data, enabling the training of embedding models with substantially improved retrieval fidelity and generalization in memory-intensive tasks. Experiments show significant improvements: on LoCoMo, F1/BLEU-1 gains of 3.27%/3.30%(MemoryOS) and 1.95%/1.78% (Mem0); on PERSONAMEM, total score improvements of 1.19% (MemoryOS) and 2.55% (Mem0).",
        "category": "Agent",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14857v1【Agent】HiNS_ Hierarchical Negative Sampling for More Comprehensive Memory Retrieval Embedding Model.pdf",
        "institution": "OPPO、浙江大学"
    },
    {
        "title": "What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study",
        "authors": [
            "Keyu Lv",
            "Manyi Zhang",
            "Xiaobo Xia",
            "Jingchen Ni",
            "Shannan Yan",
            "Xianzhi Yu",
            "Lu Hou",
            "Chun Yuan",
            "Haoli Bai"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14888v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14888v1",
        "summary": "Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.",
        "category": "量化感知训练",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14888v1【量化感知训练】What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs_ A Systematic Study.pdf",
        "institution": "清华大学、华为技术、新加坡国立大学"
    },
    {
        "title": "RECAP: Resistance Capture in Text-based Mental Health Counseling with Large Language Models",
        "authors": [
            "Anqi Li",
            "Yuqian Chen",
            "Yu Lu",
            "Zhaoming Chen",
            "Yuan Xie",
            "Zhenzhong Lan"
        ],
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14780v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14780v1",
        "summary": "Recognizing and navigating client resistance is critical for effective mental health counseling, yet detecting such behaviors is particularly challenging in text-based interactions. Existing NLP approaches oversimplify resistance categories, ignore the sequential dynamics of therapeutic interventions, and offer limited interpretability.\n  To address these limitations, we propose PsyFIRE, a theoretically grounded framework capturing 13 fine-grained resistance behaviors alongside collaborative interactions. Based on PsyFIRE, we construct the ClientResistance corpus with 23,930 annotated utterances from real-world Chinese text-based counseling, each supported by context-specific rationales. Leveraging this dataset, we develop RECAP, a two-stage framework that detects resistance and fine-grained resistance types with explanations.\n  RECAP achieves 91.25% F1 for distinguishing collaboration and resistance and 66.58% macro-F1 for fine-grained resistance categories classification, outperforming leading prompt-based LLM baselines by over 20 points. Applied to a separate counseling dataset and a pilot study with 62 counselors, RECAP reveals the prevalence of resistance, its negative impact on therapeutic relationships and demonstrates its potential to improve counselors' understanding and intervention strategies.",
        "category": "RECAP框架",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14780v1【RECAP框架】RECAP_ Resistance Capture in Text-based Mental Health Counseling with Large Language Models.pdf",
        "institution": "浙江大学、西湖大学"
    },
    {
        "title": "FunCineForge: A Unified Dataset Toolkit and Model for Zero-Shot Movie Dubbing in Diverse Cinematic Scenes",
        "authors": [
            "Jiaxuan Liu",
            "Yang Xiang",
            "Han Zhao",
            "Xiangang Li",
            "Zhenhua Ling"
        ],
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14777v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14777v1",
        "summary": "Movie dubbing is the task of synthesizing speech from scripts conditioned on video scenes, requiring accurate lip sync, faithful timbre transfer, and proper modeling of character identity and emotion. However, existing methods face two major limitations: (1) high-quality multimodal dubbing datasets are limited in scale, suffer from high word error rates, contain sparse annotations, rely on costly manual labeling, and are restricted to monologue scenes, all of which hinder effective model training; (2) existing dubbing models rely solely on the lip region to learn audio-visual alignment, which limits their applicability to complex live-action cinematic scenes, and exhibit suboptimal performance in lip sync, speech quality, and emotional expressiveness. To address these issues, we propose FunCineForge, which comprises an end-to-end production pipeline for large-scale dubbing datasets and an MLLM-based dubbing model designed for diverse cinematic scenes. Using the pipeline, we construct the first Chinese television dubbing dataset with rich annotations, and demonstrate the high quality of these data. Experiments across monologue, narration, dialogue, and multi-speaker scenes show that our dubbing model consistently outperforms SOTA methods in audio quality, lip sync, timbre transfer, and instruction following. Code and demos are available at https://anonymous.4open.science/w/FunCineForge.",
        "category": "MLLM-based dubbing model",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14777v1【MLLM-based dubbing model】FunCineForge_ A Unified Dataset Toolkit and Model for Zero-Shot Movie Dubbing in Diverse Cinematic Scenes.pdf",
        "institution": "Alibaba Group、University of Science and Technology of China"
    },
    {
        "title": "DeepMoLM: Leveraging Visual and Geometric Structural Information for Molecule-Text Modeling",
        "authors": [
            "Jing Lan",
            "Hexiao Ding",
            "Hongzhao Chen",
            "Yufeng Jiang",
            "Nga-Chun Ng",
            "Gwing Kei Yip",
            "Gerald W. Y. Cheng",
            "Yunlin Mao",
            "Jing Cai",
            "Liang-ting Lin",
            "Jung Sun Yoo"
        ],
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.MM"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14732v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14732v1",
        "summary": "AI models for drug discovery and chemical literature mining must interpret molecular images and generate outputs consistent with 3D geometry and stereochemistry. Most molecular language models rely on strings or graphs, while vision-language models often miss stereochemical details and struggle to map continuous 3D structures into discrete tokens. We propose DeepMoLM: Deep Molecular Language M odeling, a dual-view framework that grounds high-resolution molecular images in geometric invariants derived from molecular conformations. DeepMoLM preserves high-frequency evidence from 1024 $\\times$ 1024 inputs, encodes conformer neighborhoods as discrete Extended 3-Dimensional Fingerprints, and fuses visual and geometric streams with cross-attention, enabling physically grounded generation without atom coordinates. DeepMoLM improves PubChem captioning with a 12.3% relative METEOR gain over the strongest generalist baseline while staying competitive with specialist methods. It produces valid numeric outputs for all property queries and attains MAE 13.64 g/mol on Molecular Weight and 37.89 on Complexity in the specialist setting. On ChEBI-20 description generation from images, it exceeds generalist baselines and matches state-of-the-art vision-language models. Code is available at https://github.com/1anj/DeepMoLM.",
        "category": "模型架构",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14732v1【模型架构】DeepMoLM_ Leveraging Visual and Geometric Structural Information for Molecule-Text Modeling.pdf",
        "institution": "香港理工大学、香港圣约翰医院、伊利沙伯医院"
    },
    {
        "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
        "authors": [
            "Yifan Wang",
            "Shiyu Li",
            "Peiming Li",
            "Xiaochen Yang",
            "Yang Tang",
            "Zheng Wei"
        ],
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14750v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14750v1",
        "summary": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT",
        "category": "模型架构",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14750v1【模型架构】Render-of-Thought_ Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning.pdf",
        "institution": "Tencent BAC、Tsinghua Shenzhen International Graduate School、Tsinghua University、Peking University、University of Glasgow"
    },
    {
        "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
        "authors": [
            "Haowei Zhang",
            "Shudong Yang",
            "Jinlan Fu",
            "See-Kiong Ng",
            "Xipeng Qiu"
        ],
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14724v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14724v1",
        "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10$\\times$ faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.",
        "category": "模型架构",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14724v1【模型架构】HERMES_ KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding.pdf",
        "institution": "复旦大学、上海创新研究院、新加坡国立大学"
    },
    {
        "title": "PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning",
        "authors": [
            "Yao Lu",
            "Dengdong Fan",
            "Jianzheng Nie",
            "Fan Xu",
            "Jie Chen",
            "Bin Zhou",
            "Yonghong Tian"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14716v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14716v1",
        "summary": "We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.",
        "category": "强化学习",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14716v1【强化学习】PCL-Reasoner-V1.5_ Advancing Math Reasoning with Offline Reinforcement Learning.pdf",
        "institution": "Peng Cheng Laboratory、Peking University"
    },
    {
        "title": "AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving",
        "authors": [
            "Zecong Tang",
            "Zixu Wang",
            "Yifei Wang",
            "Weitong Lian",
            "Tianjian Gao",
            "Haoran Li",
            "Tengju Ru",
            "Lingyi Meng",
            "Zhejun Cui",
            "Yichen Zhu",
            "Qi Kang",
            "Kaixuan Wang",
            "Yu Zhang"
        ],
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14702v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14702v1",
        "summary": "Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.",
        "category": "AutoDriDM数据集",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14702v1【AutoDriDM数据集】AutoDriDM_ An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving.pdf",
        "institution": "浙江大学、香港大学"
    },
    {
        "title": "DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs",
        "authors": [
            "Mingxuan Song",
            "Yusen Huo",
            "Bohan Zhou",
            "Shenglin Yin",
            "Zhen Xiao",
            "Jieyi Long",
            "Zhilin Zhang",
            "Chuan Yu"
        ],
        "categories": [
            "cs.AI",
            "cs.LG"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14711v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14711v1",
        "summary": "Optimizing the advertiser's cumulative value of winning impressions under budget constraints poses a complex challenge in online advertising, under the paradigm of AI-Generated Bidding (AIGB). Advertisers often have personalized objectives but limited historical interaction data, resulting in few-shot scenarios where traditional reinforcement learning (RL) methods struggle to perform effectively. Large Language Models (LLMs) offer a promising alternative for AIGB by leveraging their in-context learning capabilities to generalize from limited data. However, they lack the numerical precision required for fine-grained optimization. To address this limitation, we introduce GRPO-Adaptive, an efficient LLM post-training strategy that enhances both reasoning and numerical precision by dynamically updating the reference policy during training. Built upon this foundation, we further propose DARA, a novel dual-phase framework that decomposes the decision-making process into two stages: a few-shot reasoner that generates initial plans via in-context prompting, and a fine-grained optimizer that refines these plans using feedback-driven reasoning. This separation allows DARA to combine LLMs' in-context learning strengths with precise adaptability required by AIGB tasks. Extensive experiments on both real-world and synthetic data environments demonstrate that our approach consistently outperforms existing baselines in terms of cumulative advertiser value under budget constraints.",
        "category": "强化学习与大模型结合",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14711v1【强化学习与大模型结合】DARA_ Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs.pdf",
        "institution": "北京大学、阿里巴巴集团"
    },
    {
        "title": "ClaimDB: A Fact Verification Benchmark over Large Structured Data",
        "authors": [
            "Michael Theologitis",
            "Preetam Prabhu Srikar Dammu",
            "Chirag Shah",
            "Dan Suciu"
        ],
        "categories": [
            "cs.CL"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14698v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14698v1",
        "summary": "Despite substantial progress in fact-verification benchmarks, claims grounded in large-scale structured data remain underexplored. In this work, we introduce ClaimDB, the first fact-verification benchmark where the evidence for claims is derived from compositions of millions of records and multiple tables. ClaimDB consists of 80 unique real-life databases covering a wide range of domains, from governance and healthcare to media, education and the natural sciences. At this scale, verification approaches that rely on \"reading\" the evidence break down, forcing a timely shift toward reasoning in executable programs. We conduct extensive experiments with 30 state-of-the-art proprietary and open-source (below 70B) LLMs and find that none exceed 83% accuracy, with more than half below 55%. Our analysis also reveals that both closed- and open-source models struggle with abstention -- the ability to admit that there is no evidence to decide -- raising doubts about their reliability in high-stakes data analysis. We release the benchmark, code, and the LLM leaderboard at https://claimdb.github.io .",
        "category": "大模型评测基准",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14698v1【大模型评测基准】ClaimDB_ A Fact Verification Benchmark over Large Structured Data.pdf",
        "institution": "University of Washington"
    },
    {
        "title": "CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation",
        "authors": [
            "Yutong Chen",
            "Jiandong Gao",
            "Ji Wu"
        ],
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14695v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14695v1",
        "summary": "Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM's ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM's reasoning ability.",
        "category": "强化学习",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14695v1【强化学习】CoScale-RL_ Efficient Post-Training by Co-Scaling Data and Computation.pdf",
        "institution": "清华大学、清华大学、北京信息科学与技术国家研究中心"
    },
    {
        "title": "LaVR: Scene Latent Conditioned Generative Video Trajectory Re-Rendering using Large 4D Reconstruction Models",
        "authors": [
            "Mingyang Xie",
            "Numair Khan",
            "Tianfu Wang",
            "Naina Dhingra",
            "Seonghyeon Nam",
            "Haitao Yang",
            "Zhuo Hui",
            "Christopher Metzler",
            "Andrea Vedaldi",
            "Hamed Pirsiavash",
            "Lei Luo"
        ],
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14674v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14674v1",
        "summary": "Given a monocular video, the goal of video re-rendering is to generate views of the scene from a novel camera trajectory. Existing methods face two distinct challenges. Geometrically unconditioned models lack spatial awareness, leading to drift and deformation under viewpoint changes. On the other hand, geometrically-conditioned models depend on estimated depth and explicit reconstruction, making them susceptible to depth inaccuracies and calibration errors.\n  We propose to address these challenges by using the implicit geometric knowledge embedded in the latent space of a large 4D reconstruction model to condition the video generation process. These latents capture scene structure in a continuous space without explicit reconstruction. Therefore, they provide a flexible representation that allows the pretrained diffusion prior to regularize errors more effectively. By jointly conditioning on these latents and source camera poses, we demonstrate that our model achieves state-of-the-art results on the video re-rendering task. Project webpage is https://lavr-4d-scene-rerender.github.io/",
        "category": "大模型架构",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14674v1【大模型架构】LaVR_ Scene Latent Conditioned Generative Video Trajectory Re-Rendering using Large 4D Reconstruction Models.pdf",
        "institution": "Meta Reality Labs、University of Oxford"
    },
    {
        "title": "INFA-Guard: Mitigating Malicious Propagation via Infection-Aware Safeguarding in LLM-Based Multi-Agent Systems",
        "authors": [
            "Yijin Zhou",
            "Xiaoya Lu",
            "Dongrui Liu",
            "Junchi Yan",
            "Jing Shao"
        ],
        "categories": [
            "cs.MA",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14667v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14667v1",
        "summary": "The rapid advancement of Large Language Model (LLM)-based Multi-Agent Systems (MAS) has introduced significant security vulnerabilities, where malicious influence can propagate virally through inter-agent communication. Conventional safeguards often rely on a binary paradigm that strictly distinguishes between benign and attack agents, failing to account for infected agents i.e., benign entities converted by attack agents. In this paper, we propose Infection-Aware Guard, INFA-Guard, a novel defense framework that explicitly identifies and addresses infected agents as a distinct threat category. By leveraging infection-aware detection and topological constraints, INFA-Guard accurately localizes attack sources and infected ranges. During remediation, INFA-Guard replaces attackers and rehabilitates infected ones, avoiding malicious propagation while preserving topological integrity. Extensive experiments demonstrate that INFA-Guard achieves state-of-the-art performance, reducing the Attack Success Rate (ASR) by an average of 33%, while exhibiting cross-model robustness, superior topological generalization, and high cost-effectiveness.",
        "category": "模型安全",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14667v1【模型安全】INFA-Guard_ Mitigating Malicious Propagation via Infection-Aware Safeguarding in LLM-Based Multi-Agent Systems.pdf",
        "institution": "上海交通大学、上海人工智能实验室、上海创新研究院"
    },
    {
        "title": "MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks",
        "authors": [
            "Zixuan Ke",
            "Yifei Ming",
            "Austin Xu",
            "Ryan Chin",
            "Xuan-Phi Nguyen",
            "Prathyusha Jwalapuram",
            "Semih Yavuz",
            "Caiming Xiong",
            "Shafiq Joty"
        ],
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.MA"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14652v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14652v1",
        "summary": "While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.",
        "category": "Agent",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14652v1【Agent】MAS-Orchestra_ Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks.pdf",
        "institution": "Salesforce Research、Massachusetts Institute of Technology"
    },
    {
        "title": "SearchGym: Bootstrapping Real-World Search Agents via Cost-Effective and High-Fidelity Environment Simulation",
        "authors": [
            "Xichen Zhang",
            "Ziyi He",
            "Yinghao Zhu",
            "Sitong Wu",
            "Shaozuo Yu",
            "Meng Chu",
            "Wenhu Zhang",
            "Haoru Tan",
            "Jiaya Jia"
        ],
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14615v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14615v1",
        "summary": "Search agents have emerged as a pivotal paradigm for solving open-ended, knowledge-intensive reasoning tasks. However, training these agents via Reinforcement Learning (RL) faces a critical dilemma: interacting with live commercial Web APIs is prohibitively expensive, while relying on static data snapshots often introduces noise due to data misalignment. This misalignment generates corrupted reward signals that destabilize training by penalizing correct reasoning or rewarding hallucination. To address this, we propose SearchGym, a simulation environment designed to bootstrap robust search agents. SearchGym employs a rigorous generative pipeline to construct a verifiable knowledge graph and an aligned document corpus, ensuring that every reasoning task is factually grounded and strictly solvable. Building on this controllable environment, we introduce SearchGym-RL, a curriculum learning methodology that progressively optimizes agent policies through purified feedback, evolving from basic interactions to complex, long-horizon planning. Extensive experiments across the Llama and Qwen families demonstrate strong Sim-to-Real generalization. Notably, our Qwen2.5-7B-Base model trained within SearchGym surpasses the web-enhanced ASearcher baseline across nine diverse benchmarks by an average relative margin of 10.6%. Our results validate that high-fidelity simulation serves as a scalable and highly cost-effective methodology for developing capable search agents.",
        "category": "Agent",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14615v1【Agent】SearchGym_ Bootstrapping Real-World Search Agents via Cost-Effective and High-Fidelity Environment Simulation.pdf",
        "institution": "香港科技大学、香港大学、香港中文大学"
    },
    {
        "title": "LFS: Learnable Frame Selector for Event-Aware and Temporally Diverse Video Captioning",
        "authors": [
            "Lianying Chao",
            "Linfeng Yin",
            "Peiyu Ren",
            "Yifan Jiang",
            "Qiaoyu Ren",
            "Dingcheng Shan",
            "Jing-cheng Pang",
            "Sijie Wu",
            "Xubin Li",
            "Kai Zhang"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14594v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14594v1",
        "summary": "Video captioning models convert frames into visual tokens and generate descriptions with large language models (LLMs). Since encoding all frames is prohibitively expensive, uniform sampling is the default choice, but it enforces equal temporal coverage while ignoring the uneven events distribution. This motivates a Learnable Frame Selector (LFS) that selects temporally diverse and event-relevant frames. LFS explicitly models temporal importance to balance temporal diversity and event relevance, and employs a stratified strategy to ensure temporal coverage while avoiding clustering. Crucially, LFS leverages caption feedback from frozen video-LLMs to learn frame selection that directly optimizes downstream caption quality. Additionally, we identify the gap between existing benchmark and human's cognition. Thus, we introduce ICH-CC built from carefully designed questions by annotators that reflect human-consistent understanding of video. Experiments indicate that LFS consistently improves detailed video captioning across two representative community benchmarks and ICH-CC, achieving up to 2.0% gains on VDC and over 4% gains on ICH-CC. Moreover, we observe that enhanced captions with LFS leads to improved performance on video question answering. Overall, LFS provides an effective and easy-to-integrate solution for detailed video captioning.",
        "category": "模型架构",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14594v1【模型架构】LFS_ Learnable Frame Selector for Event-Aware and Temporally Diverse Video Captioning.pdf",
        "institution": "华为 Technologies Co., Ltd."
    },
    {
        "title": "Social Caption: Evaluating Social Understanding in Multimodal Models",
        "authors": [
            "Bhaavanaa Thumu",
            "Leena Mathur",
            "Youssouf Kebe",
            "Louis-Philippe Morency"
        ],
        "categories": [
            "cs.CL",
            "cs.LG"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14569v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14569v1",
        "summary": "Social understanding abilities are crucial for multimodal large language models (MLLMs) to interpret human social interactions. We introduce Social Caption, a framework grounded in interaction theory to evaluate social understanding abilities of MLLMs along three dimensions: Social Inference (SI), the ability to make accurate inferences about interactions; Holistic Social Analysis (HSA), the ability to generate comprehensive descriptions of interactions; Directed Social Analysis (DSA), the ability to extract relevant social information from interactions. We analyze factors influencing model performance in social understanding, such as scale, architectural design, and spoken context. Experiments with MLLM judges contribute insights about scaling automated evaluation of multimodal social understanding.",
        "category": "Social Caption任务评测",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14569v1【Social Caption任务评测】Social Caption_ Evaluating Social Understanding in Multimodal Models.pdf",
        "institution": "卡内基梅隆大学"
    },
    {
        "title": "Rethinking Reinforcement fine-tuning of LLMs: A Multi-armed Bandit Learning Perspective",
        "authors": [
            "Xiao Hu",
            "Hong Xie",
            "Tao Tan",
            "Defu Lian",
            "Jianyu Han"
        ],
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14599v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14599v1",
        "summary": "A large number of heuristics have been proposed to optimize the reinforcement fine-tuning of LLMs. However, inconsistent claims are made from time to time, making this area elusive. Reflecting on this situation, two fundamental questions still lack a clear understanding: 1) what is the role of each optimizing choice? 2) which ones are the bottlenecks? This paper aims to shed light on them, and it faces the challenge of several entangled confounding factors in the fine-tuning process. To tackle this challenge, we propose a bottom-up experiment pipeline. The bottom layer is composed of a minimalist configuration: one training data, one rollout per round and the reward directly serve as the learning signal without advantage function design. This minimalist configuration connects to multi-armed bandit learning with extremely large discrete action space, which offers theories to corroborate the experiment findings. The up procedure of the experiment pipeline expanding the minimalist configuration layer by layer, examining the role of each design choice. Experimental results on three LLMs and two reasoning datasets not only reveal new understanding of the design choice but also yield essential insights to shape the area.",
        "category": "强化学习",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14599v1【强化学习】Rethinking Reinforcement fine-tuning of LLMs_ A Multi-armed Bandit Learning Perspective.pdf",
        "institution": "中国科学技术大学、IFlyTek (中国)"
    },
    {
        "title": "Designing KRIYA: An AI Companion for Wellbeing Self-Reflection",
        "authors": [
            "Shanshan Zhu",
            "Wenxuan Song",
            "Jiayue Melissa Shi",
            "Dong Whi Yoo",
            "Karthik S. Bhat",
            "Koustuv Saha"
        ],
        "categories": [
            "cs.HC",
            "cs.AI",
            "cs.CL",
            "cs.CY"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14589v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14589v1",
        "summary": "Most personal wellbeing apps present summative dashboards of health and physical activity metrics, yet many users struggle to translate this information into meaningful understanding. These apps commonly support engagement through goals, reminders, and structured targets, which can reinforce comparison, judgment, and performance anxiety. To explore a complementary approach that prioritizes self-reflection, we design KRIYA, an AI wellbeing companion that supports co-interpretive engagement with personal wellbeing data. KRIYA aims to collaborate with users to explore questions, explanations, and future scenarios through features such as Comfort Zone, Detective Mode, and What-If Planning. We conducted semi-structured interviews with 18 college students interacting with a KRIYA prototype using hypothetical data. Our findings show that through KRIYA interaction, users framed engaging with wellbeing data as interpretation rather than performance, experienced reflection as supportive or pressuring depending on emotional framing, and developed trust through transparency. We discuss design implications for AI companions that support curiosity, self-compassion, and reflective sensemaking of personal health data.",
        "category": "Agent",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14589v1【Agent】Designing KRIYA_ An AI Companion for Wellbeing Self-Reflection.pdf",
        "institution": "University of Illinois at Urbana-Champaign、Indiana University Indianapolis、Drexel University"
    },
    {
        "title": "QMC: Efficient SLM Edge Inference via Outlier-Aware Quantization and Emergent Memories Co-Design",
        "authors": [
            "Nilesh Prasad Pandey",
            "Jangseon Park",
            "Onat Gungor",
            "Flavio Ponzina",
            "Tajana Rosing"
        ],
        "categories": [
            "cs.LG"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14549v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14549v1",
        "summary": "Deploying Small Language Models (SLMs) on edge platforms is critical for real-time, privacy-sensitive generative AI, yet constrained by memory, latency, and energy budgets. Quantization reduces model size and cost but suffers from device noise in emerging non-volatile memories, while conventional memory hierarchies further limit efficiency. SRAM provides fast access but has low density, DRAM must simultaneously accommodate static weights and dynamic KV caches, which creates bandwidth contention, and Flash, although dense, is primarily used for initialization and remains inactive during inference. These limitations highlight the need for hybrid memory organizations tailored to LLM inference. We propose Outlier-aware Quantization with Memory Co-design (QMC), a retraining-free quantization with a novel heterogeneous memory architecture. QMC identifies inlier and outlier weights in SLMs, storing inlier weights in compact multi-level Resistive-RAM (ReRAM) while preserving critical outliers in high-precision on-chip Magnetoresistive-RAM (MRAM), mitigating noise-induced degradation. On language modeling and reasoning benchmarks, QMC outperforms and matches state-of-the-art quantization methods using advanced algorithms and hybrid data formats, while achieving greater compression under both algorithm-only evaluation and realistic deployment settings. Specifically, compared against SoTA quantization methods on the latest edge AI platform, QMC reduces memory usage by 6.3x-7.3x, external data transfers by 7.6x, energy by 11.7x, and latency by 12.5x when compared to FP16, establishing QMC as a scalable, deployment-ready co-design for efficient on-device inference.",
        "category": "模型架构",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14549v1【模型架构】QMC_ Efficient SLM Edge Inference via Outlier-Aware Quantization and Emergent Memories Co-Design.pdf",
        "institution": "University of California, San Diego、San Diego State University"
    }
]