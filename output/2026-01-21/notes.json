[
    {
        "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
        "authors": [
            "Zanlin Ni",
            "Shenzhi Wang",
            "Yang Yue",
            "Tianyu Yu",
            "Weilin Zhao",
            "Yeguo Hua",
            "Tianyi Chen",
            "Jun Song",
            "Cheng Yu",
            "Bo Zheng",
            "Gao Huang"
        ],
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15165v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15165v1",
        "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap",
        "category": "å¼ºåŒ–å­¦ä¹ ",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15165v1ã€å¼ºåŒ–å­¦ä¹ ã€‘The Flexibility Trap_ Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models.pdf",
        "institution": "Tsinghua Universityã€Alibaba Group",
        "note": "ğŸ“–æ ‡é¢˜ï¼šThe Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models\nğŸŒæ¥æºï¼šarXiv, 2601.15165v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ”¾å¼ƒçµæ´»é¡ºåºæ›´ä¼˜\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šä»»æ„é¡ºåºç”Ÿæˆæ˜¯å¦çœŸçš„èƒ½æå‡æ‰©æ•£è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæ­ç¤ºäº†ä»»æ„é¡ºåºç”Ÿæˆä¼šé™åˆ¶æ¨ç†æ½œåŠ›ï¼Œæå‡ºé€šè¿‡å›å½’è‡ªå›å½’é¡ºåºè®­ç»ƒæ¥æå‡dLLMsçš„æ¨ç†æ€§èƒ½ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å‘ç°å½“å‰dLLMsåœ¨ä»»æ„é¡ºåºç”Ÿæˆä¸­å€¾å‘äºè·³è¿‡é«˜ä¸ç¡®å®šæ€§é€»è¾‘åˆ†æ”¯è¯ï¼ˆå¦‚â€œå› æ­¤â€â€œå› ä¸ºâ€ï¼‰ï¼Œå¯¼è‡´è§£ç©ºé—´è¿‡æ—©åç¼©ã€‚  \nğŸ”¸å¼•å…¥Pass@kä½œä¸ºæ¨ç†æ½œåŠ›çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå‘ç°å›ºå®šè‡ªå›å½’é¡ºåºæ¯”ä»»æ„é¡ºåºå…·æœ‰æ›´é«˜çš„è§£ç©ºé—´è¦†ç›–ç‡ã€‚  \nğŸ”¸æå‡ºJustGRPOæ–¹æ³•ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒé˜¶æ®µæ”¾å¼ƒä»»æ„é¡ºåºï¼Œé‡‡ç”¨æ ‡å‡†ARé¡ºåºè¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚  \nğŸ”¸å°†dLLMåœ¨è®­ç»ƒæ—¶è§†ä¸ºè‡ªå›å½’æ¨¡å‹ï¼Œä½¿GRPOå¯ç›´æ¥åº”ç”¨ï¼Œé¿å…å¤æ‚è½¨è¿¹å»ºæ¨¡ä¸ä¼¼ç„¶è¿‘ä¼¼é—®é¢˜ã€‚  \nğŸ”¸ä¿ç•™dLLMåœ¨æ¨ç†æ—¶çš„å¹¶è¡Œè§£ç èƒ½åŠ›ï¼Œå…¼é¡¾è®­ç»ƒç¨³å®šæ€§å’Œæ¨ç†æ•ˆç‡ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸å®éªŒæ˜¾ç¤ºARé¡ºåºåœ¨å¤šä¸ªåŸºå‡†ä¸ŠPass@kæ˜¾è‘—é«˜äºä»»æ„é¡ºåºï¼Œè¯´æ˜å…¶æ¨ç†æ½œåŠ›æ›´å¤§ã€‚  \nğŸ”¸ä»»æ„é¡ºåºç”Ÿæˆçš„æ­£ç¡®è§£å‡ ä¹è¢«ARé¡ºåºå®Œå…¨è¦†ç›–ï¼Œä¸”ARèƒ½è§£å†³æ›´å¤šéš¾é¢˜ã€‚  \nğŸ”¸ç»Ÿè®¡æ˜¾ç¤ºé€»è¾‘è¿æ¥è¯å¸¸è¢«ä»»æ„é¡ºåºè·³è¿‡ï¼Œè¿™äº›ä½ç½®ç†µå€¼æ˜æ˜¾ä¸‹é™ï¼ŒéªŒè¯â€œç†µé€€åŒ–â€ç°è±¡ã€‚  \nğŸ”¸JustGRPOåœ¨GSM8Kä¸Šè¾¾åˆ°89.1%å‡†ç¡®ç‡ï¼Œä¼˜äºå„ç±»å¤æ‚æ‰©æ•£ä¸“ç”¨RLæ–¹æ³•ã€‚  \nğŸ”¸æ¨¡å‹åœ¨æ¨ç†æ—¶ä»æ”¯æŒå¹¶è¡Œé‡‡æ ·ï¼Œä¸”éšç€å¹¶è¡Œåº¦å¢åŠ ï¼Œæ€§èƒ½ä¼˜åŠ¿è¿›ä¸€æ­¥æ‰©å¤§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°æ€§åœ°æŒ‡å‡ºâ€œçµæ´»æ€§é™·é˜±â€â€”â€”çœ‹ä¼¼æ›´å¼ºçš„ç”Ÿæˆè‡ªç”±åº¦åè€ŒæŠ‘åˆ¶äº†æ¢ç´¢èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒæ´è§æ˜¯ï¼šå¼ºåˆ¶é¢å¯¹é«˜ç†µå†³ç­–ç‚¹æœ‰åŠ©äºä¿æŒæ¨ç†å¤šæ ·æ€§ã€‚JustGRPOä»¥æç®€è®¾è®¡å®ç°ä¼˜è¶Šæ€§èƒ½ï¼ŒæŒ‘æˆ˜äº†å¿…é¡»ä¸ºdLLMså®šåˆ¶å¤æ‚RLç®—æ³•çš„å…±è¯†ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†æ–°æ–¹å‘ã€‚\n    "
    },
    {
        "title": "CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning",
        "authors": [
            "Tianshi Xu",
            "Yuteng Chen",
            "Meng Li"
        ],
        "categories": [
            "cs.LG"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15141v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15141v1",
        "summary": "Agentic Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to utilize tools like Python interpreters for complex problem-solving. However, for parameter-constrained models (e.g., 4B--7B), the exploration phase is often plagued by frequent execution failures, creating noisy trajectories that hinder policy optimization. Under standard outcome-based reward settings, this noise leads to a critical credit assignment issue, where erroneous actions are inadvertently reinforced alongside successful outcomes. Existing mitigations face a dilemma: dense rewards often trigger reward hacking, while supersampling incurs prohibitive computational costs. To address these challenges, we propose CLEANER. Distinct from external filtering methods, CLEANER exploits the model's intrinsic self-correction capabilities to eliminate error-contaminated context directly during data collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR) mechanism autonomously constructs clean, purified trajectories by retrospectively replacing failures with successful self-corrections. Based on semantic similarity, SAAR adaptively regulates replacement granularity from shallow execution repairs to deep reasoning substitutions. By training on these self-purified paths, the model internalizes correct reasoning patterns rather than error-recovery loops. Empirical results on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains of 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art performance using only one-third of the training steps, highlighting trajectory purification as a scalable solution for efficient agentic RL. Our models and code are available at GitHub",
        "category": "Agentå¼ºåŒ–å­¦ä¹ ",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15141v1ã€Agentå¼ºåŒ–å­¦ä¹ ã€‘CLEANER_ Self-Purified Trajectories Boost Agentic Reinforcement Learning.pdf",
        "institution": "åŒ—äº¬å¤§å­¦ã€å—æ´‹ç†å·¥å¤§å­¦",
        "note": "ğŸ“–æ ‡é¢˜ï¼šCLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning\nğŸŒæ¥æºï¼šarXiv, 2601.15141v1\n\nç¬”è®°æ ‡é¢˜ï¼šè½¨è¿¹å‡€åŒ–æå‡ä»£ç†å¼ºåŒ–å­¦ä¹   \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è§£å†³å‚æ•°å—é™å¤§æ¨¡å‹åœ¨å·¥å…·è°ƒç”¨å‹å¼ºåŒ–å­¦ä¹ ä¸­å› é¢‘ç¹æ‰§è¡Œå¤±è´¥å¯¼è‡´çš„å™ªå£°è½¨è¿¹ä¸ä¿¡ç”¨åˆ†é…å¤±å‡†é—®é¢˜ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºCLEANERæ¡†æ¶ï¼Œé€šè¿‡æ¨¡å‹å†…åœ¨è‡ªä¿®æ­£èƒ½åŠ›åœ¨æ•°æ®é‡‡é›†é˜¶æ®µä¸»åŠ¨å‡€åŒ–è½¨è¿¹ï¼Œå®ç°é«˜æ•ˆã€ä½å¼€é”€çš„ä»£ç†å¼å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡Similarity-Aware Adaptive Rollbackï¼ˆSAARï¼‰æœºåˆ¶ï¼Œåœ¨æ£€æµ‹åˆ°ä»£ç æ‰§è¡Œå¤±è´¥åæš‚ä¸è®°å½•é”™è¯¯ï¼Œè½¬è€Œè§¦å‘æ¨¡å‹è‡ªç”Ÿæˆä¿®æ­£æ–¹æ¡ˆã€‚  \nğŸ”¸åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨difflib.SequenceMatcherï¼‰åŠ¨æ€åˆ¤æ–­é”™è¯¯ç±»å‹ï¼šé«˜ç›¸ä¼¼æ—¶ä»…æ›¿æ¢ä»£ç ï¼ˆæµ…å±‚ä¿®å¤ï¼‰ï¼Œä½ç›¸ä¼¼æ—¶åŒæ­¥æ›¿æ¢æ¨ç†é“¾ä¸ä»£ç ï¼ˆæ·±å±‚ä¿®å¤ï¼‰ã€‚  \nğŸ”¸é‡‡ç”¨RadixAttentioné‡è®¡ç®—ä¿®æ­£åŠ¨ä½œåœ¨å‡€åŒ–ä¸Šä¸‹æ–‡ä¸­çš„log-probï¼Œç¡®ä¿ç­–ç•¥æ›´æ–°å› æœæ­£ç¡®ï¼›å¹¶å¼•å…¥è¯¾ç¨‹æ··åˆç­–ç•¥ï¼ˆ70%åº”ç”¨SAAR+30%ä¿ç•™åŸå§‹è½¨è¿¹ï¼‰å…¼é¡¾é²æ£’æ€§ã€‚  \nğŸ”¸åœ¨GRPOç¨€ç–å¥–åŠ±æ¡†æ¶ä¸‹ï¼Œç›´æ¥ä»¥å‡€åŒ–åçš„è½¨è¿¹æ›¿ä»£åŸå§‹å™ªå£°è½¨è¿¹è¿›è¡Œç­–ç•¥è®­ç»ƒï¼Œä½¿æ¨¡å‹å†…åŒ–æ­£ç¡®æ¨ç†æ¨¡å¼è€Œéé”™è¯¯æ¢å¤å¾ªç¯ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨AIME24/25ã€GPQAã€LiveCodeBenchä¸Šå¹³å‡æå‡å‡†ç¡®ç‡6%ã€3%ã€5%ï¼Œæ˜¾è‘—ä¼˜äºDAPOåŸºçº¿ä¸”åª²ç¾SOTAæ–¹æ³•ã€‚  \nğŸ”¸ä»…éœ€ä¸‰åˆ†ä¹‹ä¸€è®­ç»ƒæ­¥æ•°å³è¾¾SOTAæ€§èƒ½ï¼ŒéªŒè¯è½¨è¿¹å‡€åŒ–æ¯”å¢åŠ é‡‡æ ·æˆ–å¯†é›†å¥–åŠ±æ›´é«˜æ•ˆã€æ›´å¯æ‰©å±•ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¡¨æ˜ï¼šSAARå¯¹å°æ¨¡å‹ï¼ˆ4Bâ€“7Bï¼‰å¢ç›Šå°¤ä¸ºçªå‡ºï¼Œå·¥å…·é›†æˆ+å‡€åŒ–å¸¦æ¥è¿œè¶…å•çº¯å·¥å…·å¢å¼ºçš„æ”¶ç›Šã€‚  \nğŸ”¸å…³é—­SAARè¯„ä¼°æ—¶æ€§èƒ½ä¸‹é™æå°ï¼ˆå¦‚AIME24 Pass@1ä»…é™0.6%ï¼‰ï¼Œè¯æ˜æ¨¡å‹å·²å°†çº é”™é€»è¾‘å†…åŒ–ä¸ºå›ºæœ‰ç­–ç•¥ï¼Œæ— éœ€æ¨ç†æ—¶ä¾èµ–å¤–éƒ¨æœºåˆ¶ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºè·³å‡ºâ€œå¤–éƒ¨è¿‡æ»¤â€æˆ–â€œå¥–åŠ±å·¥ç¨‹â€ä¼ ç»Ÿè·¯å¾„ï¼Œè½¬å‘â€œå†…åœ¨å‡€åŒ–â€èŒƒå¼â€”â€”å°†æ¨¡å‹è‡ªä¿®æ­£èƒ½åŠ›è½¬åŒ–ä¸ºæ•°æ®æ„é€ èƒ½åŠ›ï¼Œä»¥è½»é‡çº§å›æ»šæ“ä½œæ›¿ä»£é«˜æˆæœ¬è¶…é‡‡æ ·ï¼Œåœ¨æºå¤´æ¶ˆé™¤å™ªå£°è€ŒéåæœŸåŠ æƒçŸ«æ­£ï¼Œå…¼å…·ç†è®ºç®€æ´æ€§ä¸å·¥ç¨‹å®ç”¨æ€§ã€‚\n    "
    },
    {
        "title": "Iterative Refinement Improves Compositional Image Generation",
        "authors": [
            "Shantanu Jaiswal",
            "Mihir Prabhudesai",
            "Nikash Bhardwaj",
            "Zheyang Qin",
            "Amir Zadeh",
            "Chuan Li",
            "Katerina Fragkiadaki",
            "Deepak Pathak"
        ],
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15286v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15286v1",
        "summary": "Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/",
        "category": "è¿­ä»£è‡ªä¿®æ­£ç”Ÿæˆ",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15286v1ã€è¿­ä»£è‡ªä¿®æ­£ç”Ÿæˆã€‘Iterative Refinement Improves Compositional Image Generation.pdf",
        "institution": "Carnegie Mellon Universityã€Lambda AI",
        "note": "ğŸ“–æ ‡é¢˜ï¼šIterative Refinement Improves Compositional Image Generation\nğŸŒæ¥æºï¼šarXiv, 2601.15286v1\n\nè¿­ä»£å¼ä¼˜åŒ–æå‡å›¾åƒç”Ÿæˆ\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹\nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•æå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å¤æ‚æç¤ºä¸‹çš„ç”Ÿæˆå‡†ç¡®æ€§ï¼Ÿ\nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹åé¦ˆçš„è¿­ä»£ä¼˜åŒ–æ–¹æ³•ï¼Œæ˜¾è‘—æå‡å¤æ‚åœºæ™¯ä¸‹çš„å›¾åƒç”Ÿæˆè´¨é‡ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯\nğŸ”¸ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç”Ÿæˆåˆå§‹å›¾åƒï¼Œå¹¶å¼•å…¥è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä½œä¸ºæ‰¹è¯„è€…è¯„ä¼°å½“å‰å›¾åƒä¸åŸå§‹æç¤ºçš„ä¸€è‡´æ€§ã€‚\nğŸ”¸è®¾è®¡åŒ…å«ç»§ç»­ã€å›é€€ã€é‡å¯å’Œåœæ­¢å››ç§åŠ¨ä½œçš„ç­–ç•¥ç©ºé—´ï¼ŒæŒ‡å¯¼æ¨¡å‹æ ¹æ®æ‰¹è¯„åé¦ˆé€‰æ‹©ä¸‹ä¸€æ­¥æ“ä½œã€‚\nğŸ”¸é€šè¿‡å›¾åƒç¼–è¾‘æ¨¡å‹æ‰§è¡Œå…·ä½“ä¿®æ”¹ï¼Œé€æ­¥ä¿®æ­£é”™è¯¯ï¼Œåˆ†è§£å¤æ‚ä»»åŠ¡ä¸ºå¤šä¸ªå¯ç®¡ç†çš„å­æ­¥éª¤ã€‚\nğŸ”¸åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹å¹³è¡¡è¿­ä»£ä¼˜åŒ–ä¸å¹¶è¡Œé‡‡æ ·ï¼Œå®ç°æ·±åº¦ä¼˜åŒ–ä¸å¹¿åº¦æ¢ç´¢çš„æƒè¡¡ã€‚\n\nğŸ”åˆ†ææ€»ç»“\nğŸ”¸åœ¨ConceptMixã€T2I-CompBenchç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”å¹¶è¡Œé‡‡æ ·å¹³å‡æå‡12%ä»¥ä¸Šå‡†ç¡®ç‡ã€‚\nğŸ”¸äººç±»è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œ58.7%æƒ…å†µä¸‹ç”¨æˆ·æ›´åå¥½æœ¬æ–¹æ³•ç”Ÿæˆçš„ç»“æœï¼Œå°¤å…¶åœ¨ç©ºé—´å…³ç³»å’Œæ•°é‡æ¨ç†æ–¹é¢ä¼˜åŠ¿æ˜æ˜¾ã€‚\nğŸ”¸æ¶ˆèå®éªŒè¡¨æ˜ï¼Œå®Œæ•´çš„åŠ¨ä½œç©ºé—´ï¼ˆå«Backtrackå’ŒRestartï¼‰å¯¹æ€§èƒ½è‡³å…³é‡è¦ï¼Œä¸”æ›´å¼ºçš„VLMæ‰¹è¯„è€…å¸¦æ¥è¿›ä¸€æ­¥å¢ç›Šã€‚\nğŸ”¸åœ¨Visual Jengaåœºæ™¯åˆ†è§£ä»»åŠ¡ä¸­ï¼Œå…¨åºåˆ—æ­£ç¡®ç‡ä»64.29%æå‡è‡³76.79%ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨å¤šæ­¥æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹\nè®ºæ–‡åˆ›æ–°åœ°å°†å¤§è¯­è¨€æ¨¡å‹ä¸­çš„é“¾å¼æ€ç»´æ¨ç†è¿ç§»åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸï¼Œé€šè¿‡æ„å»ºâ€œç”Ÿæˆ-æ‰¹è¯„-ç¼–è¾‘â€é—­ç¯ç³»ç»Ÿï¼Œä½¿æ¨¡å‹å…·å¤‡è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›ã€‚å…¶æ— éœ€é¢å¤–è®­ç»ƒã€å…¼å®¹å¤šç§åŸºç¡€æ¨¡å‹çš„è®¾è®¡ç†å¿µå…·æœ‰å¾ˆå¼ºå®ç”¨æ€§ã€‚æ ¸å¿ƒçªç ´åœ¨äºç”¨è½»é‡çº§VLMæ›¿ä»£ä¼ ç»Ÿå¤æ‚çš„å·¥å…·é“¾ï¼Œè¯æ˜äº†ç®€å•åé¦ˆæœºåˆ¶å³å¯æ˜¾è‘—æå‡ç»„åˆç”Ÿæˆè¡¨ç°ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†æä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "StableWorld: Towards Stable and Consistent Long Interactive Video Generation",
        "authors": [
            "Ying Yang",
            "Zhengyao Lv",
            "Tianlin Pan",
            "Haofan Wang",
            "Binxin Yang",
            "Hubery Yin",
            "Chen Li",
            "Ziwei Liu",
            "Chenyang Si"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15281v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15281v1",
        "summary": "In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \\textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \\eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.",
        "category": "Agent",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15281v1ã€Agentã€‘StableWorld_ Towards Stable and Consistent Long Interactive Video Generation.pdf",
        "institution": "PRLabã€NJUã€HKUã€UCASã€WeChatã€Tencent Inc.ã€NTU",
        "note": "ğŸ“–æ ‡é¢˜ï¼šStableWorld: Towards Stable and Consistent Long Interactive Video Generation\nğŸŒæ¥æºï¼šarXiv, 2601.15281v1\n\nç¬”è®°æ ‡é¢˜ï¼šåŠ¨æ€å¸§é©±é€æå‡è§†é¢‘ç¨³å®šæ€§  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è§£å†³é•¿æ—¶äº¤äº’å¼è§†é¢‘ç”Ÿæˆä¸­å› å¸§é—´æ¼‚ç§»ç´¯ç§¯å¯¼è‡´çš„åœºæ™¯å´©æºƒä¸æ—¶é—´ä¸ä¸€è‡´é—®é¢˜ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºStableWorldæ–¹æ³•ï¼Œé€šè¿‡åŸºäºORBå‡ ä½•ç›¸ä¼¼æ€§çš„åŠ¨æ€å¸§é©±é€æœºåˆ¶ï¼Œåœ¨æºå¤´æŠ‘åˆ¶è¯¯å·®ç´¯ç§¯ï¼Œæ˜¾è‘—æå‡é•¿äº¤äº’è§†é¢‘çš„ç¨³å®šæ€§ä¸æ—¶é—´ä¸€è‡´æ€§ã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸é¦–å…ˆå®šé‡åˆ†æé™æ€åœºæ™¯ä¸‹å¸§é—´MSEæ¼‚ç§»ï¼Œå‘ç°å³ä½¿æ— åŠ¨ä½œè¾“å…¥ï¼Œå¾®å°åå·®ä¹Ÿä¼šéšæ—¶é—´æŒ‡æ•°çº§ç´¯ç§¯ï¼Œæ˜¯åœºæ™¯å´©æºƒçš„ä¸»å› ã€‚  \nğŸ”¸ç»§è€ŒéªŒè¯æ‰©å¤§KVç¼“å­˜çª—å£å¯ç¼“è§£æ¼‚ç§»ï¼Œä½†å‘ç°çœŸæ­£èµ·ä½œç”¨çš„æ˜¯ä¿ç•™æ—©æœŸé«˜è´¨é‡å¸§ï¼Œè€Œéå•çº¯å¢åŠ å†å²é•¿åº¦ã€‚  \nğŸ”¸æ®æ­¤è®¾è®¡åŠ¨æ€å¸§é©±é€æœºåˆ¶ï¼šä»¥é¦–å¸§ä¸ºå‚è€ƒï¼Œç”¨ORBç‰¹å¾åŒ¹é…+RANSACè®¡ç®—ä¸­é—´å¸§ä¸å‚è€ƒå¸§çš„å‡ ä½•ç›¸ä¼¼åº¦ï¼ˆå–Homographyä¸FundamentalçŸ©é˜µçš„æœ€é«˜å†…ç‚¹ç‡ï¼‰ã€‚  \nğŸ”¸æ»‘åŠ¨çª—å£æ›´æ–°æ—¶ï¼Œä¼˜å…ˆé©±é€ç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼çš„â€œå†—ä½™ä¸­é—´å¸§â€ï¼Œä¿ç•™æœ€è¿‘å¸§ä¿éšœè¿åŠ¨è¿ç»­æ€§ï¼Œä»…æ·˜æ±°å·²å‘ç”Ÿå‡ ä½•æ¼‚ç§»çš„æ—©æœŸå¸§ã€‚  \nğŸ”¸è¯¥æœºåˆ¶å®Œå…¨æ¨¡å‹æ— å…³ï¼Œé€‚é…Matrix-Gameã€Open-Oasisã€Hunyuan-GameCraftç­‰å¤šç§äº¤äº’è§†é¢‘æ¡†æ¶ï¼Œä¸”ä»…å¼•å…¥1.01â€“1.02Ã—å¾®å°å»¶è¿Ÿã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨VBench-Longè¯„æµ‹ä¸­ï¼ŒStableWorldä½¿Aesthetic Qualityæœ€é«˜æå‡14.61%ï¼ŒImage Qualityæå‡7.38%ï¼Œè¯æ˜å…¶æœ‰æ•ˆå¢å¼ºè§†è§‰ä¿çœŸåº¦ã€‚  \nğŸ”¸ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œè¶…85%å‚ä¸è€…æ›´åå¥½StableWorldç”Ÿæˆç»“æœï¼Œåœ¨è§†é¢‘è´¨é‡ã€æ—¶é—´ä¸€è‡´æ€§å’Œè¿åŠ¨å¹³æ»‘æ€§ä¸‰æ–¹é¢å‡è·å‹å€’æ€§ä¼˜åŠ¿ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ORBç›¸ä¼¼åº¦ä¼˜äºSSIMï¼ˆæŠ—è§†è§’å˜åŒ–ï¼‰å’Œä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆé˜²è¯¯ä¿ç•™æ—§åœºæ™¯å¸§ï¼‰ï¼Œé˜ˆå€¼0.75å®ç°æœ€ä½³æƒè¡¡ã€‚  \nğŸ”¸åœ¨æ•°åƒå¸§é•¿åºåˆ—ä¸­ï¼Œæ— è®ºå°å¹…åº¦ç¼“æ…¢ç§»åŠ¨æˆ–å¤§å¹…åº¦è§†è§’åˆ‡æ¢ï¼ŒStableWorldå‡èƒ½æŒç»­ç»´æŒåœºæ™¯ç»“æ„ç¨³å®šï¼Œé¿å…å´©æºƒã€‚  \nğŸ”¸è¯¥æœºåˆ¶åŒæ ·é€‚ç”¨äºè‡ªå›å½’è§†é¢‘ç”Ÿæˆï¼ˆå¦‚Self-Forcingï¼‰ï¼Œè¯å®å…¶å¯¹å¹¿ä¹‰é•¿è§†é¢‘ç”Ÿæˆçš„æ™®é€‚æœ‰æ•ˆæ€§ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºè·³å‡ºä¼ ç»Ÿâ€œå¢å¼ºå»ºæ¨¡èƒ½åŠ›â€æ€è·¯ï¼Œè½¬è€Œä»ç”Ÿæˆæµç¨‹åº•å±‚â€”â€”å†å²å¸§ç®¡ç†ç­–ç•¥å…¥æ‰‹ï¼Œå°†è®¡ç®—æœºè§†è§‰ä¸­çš„é²æ£’å‡ ä½•åŒ¹é…ï¼ˆORB+RANSACï¼‰å·§å¦™åµŒå…¥è§†é¢‘ç”Ÿæˆç¼“å­˜æœºåˆ¶ï¼Œä»¥è½»é‡ã€å¯è§£é‡Šã€å³æ’å³ç”¨çš„æ–¹å¼æ ¹æ²»è¯¯å·®ä¼ æ’­é“¾ï¼Œå…¼å…·ç†è®ºæ´å¯ŸåŠ›ä¸å·¥ç¨‹å®ç”¨æ€§ã€‚\n    "
    },
    {
        "title": "Walk through Paintings: Egocentric World Models from Internet Priors",
        "authors": [
            "Anurag Bagchi",
            "Zhipeng Bao",
            "Homanga Bharadhwaj",
            "Yu-Xiong Wang",
            "Pavel Tokmakov",
            "Martial Hebert"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15284v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15284v1",
        "summary": "What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation world models, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.",
        "category": "æ¨¡å‹æ¶æ„",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15284v1ã€æ¨¡å‹æ¶æ„ã€‘Walk through Paintings_ Egocentric World Models from Internet Priors.pdf",
        "institution": "Carnegie Mellon Universityã€University of Illinois Urbana-Champaignã€Toyota Research Institute",
        "note": "ğŸ“–æ ‡é¢˜ï¼šWalk through Paintings: Egocentric World Models from Internet Priors\nğŸŒæ¥æºï¼šarXiv, 2601.15284v1\n\nç¬”è®°æ ‡é¢˜ï¼šè½»é‡åŠ¨ä½œæ³¨å…¥çš„ä¸–ç•Œæ¨¡å‹  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•æ— éœ€ä»å¤´è®­ç»ƒï¼Œä»…ç”¨å°‘é‡åŠ¨ä½œ-è§‚æµ‹æ•°æ®ï¼Œå°†äº’è”ç½‘é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹è½¬åŒ–ä¸ºé«˜ä¿çœŸã€å¼ºæ³›åŒ–çš„åŠ¨ä½œæ¡ä»¶ä¸–ç•Œæ¨¡å‹ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºEgoWMæ¡†æ¶ï¼Œé€šè¿‡åœ¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ—¶é—´æ­¥åµŒå…¥è·¯å¾„ä¸­è½»é‡æ³¨å…¥åŠ¨ä½œä¿¡å·ï¼Œå®ç°è·¨æ¶æ„ã€è·¨å®ä½“ï¼ˆ3-DoFè‡³25-DoFï¼‰ã€è·¨åŸŸï¼ˆçœŸå®åœºæ™¯åˆ°ç»˜ç”»ï¼‰çš„åŠ¨ä½œå¯æ§ä¸–ç•Œå»ºæ¨¡ã€‚  \n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡åŠ¨ä½œæŠ•å½±æ¨¡å—ï¼šå°†ä»»æ„ç»´åº¦åŠ¨ä½œåºåˆ—ï¼ˆå¦‚å…³èŠ‚è§’æˆ–ä½å§¿ï¼‰ç»MLPç¼–ç ä¸ºæ—¶åºå¯¹é½çš„éšç©ºé—´åµŒå…¥ï¼Œå¹¶é€šè¿‡1Då·ç§¯é€‚é…ä¸åŒæ½œå˜é‡å‹ç¼©ç‡ã€‚  \nğŸ”¸å¤ç”¨æ—¶é—´æ­¥è°ƒåˆ¶é€šè·¯ï¼šä¸ä¿®æ”¹ä¸»å¹²ç½‘ç»œï¼Œåœ¨æ¯ä¸ªæ—¶é—´æ­¥åµŒå…¥è°ƒåˆ¶ä½ç½®å¤„ç›´æ¥å åŠ åŠ¨ä½œåµŒå…¥ï¼ˆ+åˆå§‹çŠ¶æ€åµŒå…¥ç”¨äºäººå½¢ï¼‰ï¼Œå®ç°æ¶æ„æ— å…³çš„æ¡ä»¶æ³¨å…¥ã€‚  \nğŸ”¸å¼•å…¥ç»“æ„ä¸€è‡´æ€§åˆ†æ•°ï¼ˆSCSï¼‰ï¼šåŸºäºSAM2åˆ†å‰²ä¸ç¨ å¯†ç‚¹è·Ÿè¸ªï¼Œè‡ªåŠ¨è¯†åˆ«ç¨³å®šåœºæ™¯ç»“æ„å¹¶è®¡ç®—é¢„æµ‹/çœŸå®è§†é¢‘ä¸­å¯¹åº”æ©ç çš„IoUå‡å€¼ï¼Œè§£è€¦ç‰©ç†ä¸€è‡´æ€§ä¸å¤–è§‚ä¿çœŸåº¦ã€‚  \nğŸ”¸éªŒè¯å¼€æ”¾æ³›åŒ–èƒ½åŠ›ï¼šåœ¨çœŸå®å¯¼èˆªã€25-DoFäººå½¢å¯¼èˆªä¸æ“ä½œã€ä¹ƒè‡³éç‰©ç†çš„ç»˜ç”»å†…å¯¼èˆªç­‰æç«¯åˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯ä¸­ç»Ÿä¸€éªŒè¯æ¨¡å‹è¡¨ç°ã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸EgoWMåœ¨SCSæŒ‡æ ‡ä¸Šè¾ƒNavigation World Modelsï¼ˆNWMï¼‰æå‡æœ€é«˜è¾¾80%ï¼Œå°¤å…¶åœ¨é•¿æ—¶åºé¢„æµ‹ä¸­ä¼˜åŠ¿æ›´æ˜¾è‘—ï¼Œè¯æ˜å…¶åŠ¨ä½œè·Ÿéšèƒ½åŠ›æ›´å¼ºã€‚  \nğŸ”¸æ¨ç†å»¶è¿Ÿä»…ä¸ºNWMçš„1/6ï¼Œå› é‡‡ç”¨å—å¼ç”Ÿæˆè€Œéå¸§çº§è‡ªå›å½’ï¼Œä¸”æ”¯æŒæ›´é«˜åˆ†è¾¨ç‡ï¼ˆ512Ã—512 vs 224Ã—224ï¼‰ã€‚  \nğŸ”¸åœ¨25-DoFäººå½¢ä»»åŠ¡ä¸­ï¼Œä»…éœ€å¾®è°ƒå³è¶…è¶Šä»é›¶è®­ç»ƒçš„åŸºçº¿ï¼ŒéªŒè¯äº’è”ç½‘å…ˆéªŒå¯¹é«˜ç»´åŠ¨åŠ›å­¦å»ºæ¨¡çš„å…³é”®ä»·å€¼ã€‚  \nğŸ”¸SCSèƒ½å‡†ç¡®è¯†åˆ«ç‰©ç†ä¸€è‡´ä½†æ¨¡ç³Šçš„é¢„æµ‹ï¼ˆé«˜åˆ†ï¼‰ä¸è§†è§‰é”åˆ©ä½†åŠ¨ä½œæ¼‚ç§»çš„é¢„æµ‹ï¼ˆä½åˆ†ï¼‰ï¼Œè€ŒLPIPSç­‰æ„ŸçŸ¥æŒ‡æ ‡æ˜“äº§ç”Ÿè¯¯å¯¼æ€§é«˜åˆ†ã€‚  \n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°åœ¨äºâ€œæç®€ä¸»ä¹‰â€è®¾è®¡å“²å­¦ï¼šæ‹’ç»é‡è®­å¤§æ¨¡å‹æˆ–å®šåˆ¶æ–°æ¶æ„ï¼Œè½¬è€ŒæŒ–æ˜ç°æœ‰è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­è¢«å¿½è§†çš„é€šç”¨æ—¶é—´æ­¥è°ƒåˆ¶æœºåˆ¶ï¼Œå°†å…¶è½¬åŒ–ä¸ºåŠ¨ä½œæ§åˆ¶æ¥å£ã€‚è¿™ç§å¯¹é¢„è®­ç»ƒè¡¨å¾çš„å°Šé‡ä¸é«˜æ•ˆå¤ç”¨ï¼Œå…¼é¡¾äº†æ€§èƒ½ã€æ•ˆç‡ä¸æ³›åŒ–æ€§ï¼Œä¸ºæ„å»ºå¯æ‰©å±•çš„é€šç”¨ä¸–ç•Œæ¨¡å‹æä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "Large-Scale Multidimensional Knowledge Profiling of Scientific Literature",
        "authors": [
            "Zhucun Xue",
            "Jiangning Zhang",
            "Juntao Jiang",
            "Jinzhuo Liu",
            "Haoyang He",
            "Teng Hu",
            "Xiaobin Hu",
            "Guangming Yao",
            "Yi Yuan",
            "Yong Liu"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15170v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15170v1",
        "summary": "The rapid expansion of research across machine learning, vision, and language has produced a volume of publications that is increasingly difficult to synthesize. Traditional bibliometric tools rely mainly on metadata and offer limited visibility into the semantic content of papers, making it hard to track how research themes evolve over time or how different areas influence one another. To obtain a clearer picture of recent developments, we compile a unified corpus of more than 100,000 papers from 22 major conferences between 2020 and 2025 and construct a multidimensional profiling pipeline to organize and analyze their textual content. By combining topic clustering, LLM-assisted parsing, and structured retrieval, we derive a comprehensive representation of research activity that supports the study of topic lifecycles, methodological transitions, dataset and model usage patterns, and institutional research directions. Our analysis highlights several notable shifts, including the growth of safety, multimodal reasoning, and agent-oriented studies, as well as the gradual stabilization of areas such as neural machine translation and graph-based methods. These findings provide an evidence-based view of how AI research is evolving and offer a resource for understanding broader trends and identifying emerging directions. Code and dataset: https://github.com/xzc-zju/Profiling_Scientific_Literature",
        "category": "Agent",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15170v1ã€Agentã€‘Large-Scale Multidimensional Knowledge Profiling of Scientific Literature.pdf",
        "institution": "æµ™æ±Ÿå¤§å­¦ã€ä¸Šæµ·äº¤é€šå¤§å­¦ã€æ–°åŠ å¡å›½ç«‹å¤§å­¦ã€èš‚èšé›†å›¢",
        "note": "ğŸ“–æ ‡é¢˜ï¼šLarge-Scale Multidimensional Knowledge Profiling of Scientific Literature\nğŸŒæ¥æºï¼šarXiv, 2601.15170v1\n\nç¬”è®°æ ‡é¢˜ï¼šå¤šç»´çŸ¥è¯†ç”»åƒåˆ†æAIç ”ç©¶æ¼”è¿›  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ç³»ç»Ÿæ€§åœ°åˆ»ç”»å¤§è§„æ¨¡AIè®ºæ–‡åœ¨ä¸»é¢˜ã€æ–¹æ³•ã€æ•°æ®ã€ç®—åŠ›å’Œæœºæ„ç­‰å¤šä¸ªç»´åº¦ä¸Šçš„åŠ¨æ€æ¼”åŒ–è§„å¾‹ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæ„å»ºäº†é¦–ä¸ªè¦†ç›–10ä¸‡+è®ºæ–‡ã€èåˆèšç±»åˆ†æã€å¤§æ¨¡å‹è¯­ä¹‰è§£æä¸åˆ†å±‚æ£€ç´¢çš„å¤šç»´çŸ¥è¯†ç”»åƒæ¡†æ¶ï¼Œå®ç°äº†å¯¹AIç ”ç©¶ç”Ÿæ€çš„ç»†ç²’åº¦ã€è¯æ®é©±åŠ¨çš„çºµå‘åˆ†æã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸åŸºäº22ä¸ªé¡¶ä¼š2020â€“2025å¹´è¶…10ä¸‡ç¯‡è®ºæ–‡æ„å»ºç»Ÿä¸€è¯­æ–™åº“ï¼Œå¹¶ä½¿ç”¨minerUå°†PDFè½¬ä¸ºç»“æ„åŒ–Markdownï¼›  \nğŸ”¸é‡‡ç”¨BERTopicå¼æµç¨‹ï¼ˆæ–‡æœ¬ç¼–ç â†’UMAPé™ç»´â†’HDBSCANèšç±»ï¼‰ç”Ÿæˆ300+è¯­ä¹‰ä¸€è‡´çš„ä¸»é¢˜ç°‡ï¼Œå†ç”±ChatGPT-5ç”Ÿæˆå±‚çº§åŒ–ä¸»é¢˜å‘½åä¸é€»è¾‘å…³ç³»ï¼›  \nğŸ”¸è®¾è®¡LLMè¾…åŠ©çš„å¤šç»´è¯­ä¹‰è§£ææµæ°´çº¿ï¼ˆDeepseek-R1-32Bï¼‰ï¼ŒæŒ‰Info/Summary/Technical/Analysis/Systemäº”ç±»å­—æ®µæå–ç»“æ„åŒ–çŸ¥è¯†ï¼›  \nğŸ”¸æå‡ºæ„å›¾é©±åŠ¨çš„åˆ†å±‚æ£€ç´¢æœºåˆ¶ï¼šå…ˆå…ƒæ•°æ®è¿‡æ»¤ç¼©å°å€™é€‰é›†ï¼Œå†å¯¹æ‘˜è¦ã€æ–¹æ³•ã€æ•°æ®ç­‰å­—æ®µåŠ æƒè¯­ä¹‰æœç´¢ï¼Œç¡®ä¿ç»“æœå¯è¿½æº¯ã€å¯éªŒè¯ï¼›  \nğŸ”¸å»ºç«‹å››è±¡é™ä¸»é¢˜ç”Ÿå‘½å‘¨æœŸæ¨¡å‹ï¼ˆCAGR vs. å¹³å‡å‘è¡¨å¹´ï¼‰ï¼Œç»“åˆåŠ æƒå½±å“æŒ‡æ ‡ï¼ˆå¼•ç”¨+è®ºæ–‡æ•°ï¼‰é‡åŒ–ä¸»é¢˜å…´è¡°ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸å®‰å…¨å¯æ§ã€å¤šæ¨¡æ€æ¨ç†ä¸æ™ºèƒ½ä½“ç ”ç©¶å‘ˆçˆ†å‘å¼å¢é•¿ï¼Œè€Œç¥ç»æœºå™¨ç¿»è¯‘ã€å›¾ç¥ç»ç½‘ç»œç­‰æ–¹å‘è¶‹äºæˆç†Ÿæˆ–è¡°é€€ï¼›  \nğŸ”¸è®¡ç®—èµ„æºæ¶ˆè€—æŒç»­æ”€å‡ï¼Œç”Ÿæˆæ¨¡å‹ã€å¤šæ¨¡æ€ä¸æ™ºèƒ½ä½“é¢†åŸŸå æ®A100ç­‰æ•ˆç®—åŠ›ä¸»å¯¼åœ°ä½ï¼Œå‘ˆç°â€œé«˜æŠ•å…¥é«˜äº§å‡ºâ€æ­£å¾ªç¯ï¼›  \nğŸ”¸è¯„æµ‹åŸºå‡†è®ºæ–‡å æ¯”ä»2022å¹´10%å‡è‡³2025å¹´18%ï¼Œåæ˜ ç ”ç©¶èŒƒå¼è½¬å‘å¯¹å…¬å¹³æ€§ã€é²æ£’æ€§ç­‰éæ€§èƒ½æŒ‡æ ‡çš„ç³»ç»Ÿè¯„ä¼°ï¼›  \nğŸ”¸ç»å…¸è§†è§‰æ•°æ®é›†ï¼ˆå¦‚ImageNetï¼‰ä½¿ç”¨è¾¾å³°åè¶‹ç¨³ï¼Œè€ŒCOCOç­‰è¢«å¹¿æ³›ç”¨äºè·¨æ¨¡æ€èƒ½åŠ›è¯„æµ‹ï¼Œä½“ç°æ•°æ®å¤ç”¨èŒƒå¼å‡çº§ï¼›  \nğŸ”¸å­¦æœ¯æœºæ„èšç„¦åŸºç¡€ç®—æ³•ä¸ç†è®ºæœºåˆ¶ï¼ˆå¦‚æ¸…åçš„çŸ¥è¯†è’¸é¦ã€CMUçš„å› æœå‘ç°ï¼‰ï¼Œå·¥ä¸šç•Œä¾§é‡éƒ¨ç½²ä¼˜åŒ–ä¸ç³»ç»Ÿçº§åº”ç”¨ï¼ˆå¦‚å¾®è½¯çš„RAGã€è°·æ­Œçš„è”é‚¦å­¦ä¹ ï¼‰ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ¨äºçªç ´ä¼ ç»Ÿæ–‡çŒ®è®¡é‡å±€é™ï¼Œä»¥â€œè¯­ä¹‰ç†è§£+ç»“æ„åŒ–çŸ¥è¯†+æ—¶åºå»ºæ¨¡â€ä¸‰ä½ä¸€ä½“æ–¹å¼ï¼Œå°†æµ·é‡è®ºæ–‡è½¬åŒ–ä¸ºå¯æŸ¥è¯¢ã€å¯éªŒè¯ã€å¯æ¼”åŒ–çš„åŠ¨æ€çŸ¥è¯†å›¾è°±ï¼›å…¶æ ¸å¿ƒä»·å€¼ä¸ä»…æ˜¯å‘ç°è¶‹åŠ¿ï¼Œæ›´æ˜¯æ„å»ºäº†ä¸€å¥—é€æ˜ã€å¯å¤ç°ã€å¤šç²’åº¦çš„AIç§‘ç ”å…ƒåˆ†æåŸºç¡€è®¾æ–½ã€‚\n    "
    },
    {
        "title": "Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning",
        "authors": [
            "Yuval Kansal",
            "Niraj K. Jha"
        ],
        "categories": [
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15160v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15160v1",
        "summary": "Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a \"compositional bridge\", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.",
        "category": "å¼ºåŒ–å­¦ä¹ ",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15160v1ã€å¼ºåŒ–å­¦ä¹ ã€‘Knowledge Graphs are Implicit Reward Models_ Path-Derived Signals Enable Compositional Reasoning.pdf",
        "institution": "Princeton University",
        "note": "ğŸ“–æ ‡é¢˜ï¼šKnowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nğŸŒæ¥æºï¼šarXiv, 2601.15160v1\n\nç¬”è®°æ ‡é¢˜ï¼šçŸ¥è¯†å›¾è°±ä½œä¸ºéšå¼å¥–åŠ±æ¨¡å‹  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸ä¾èµ–æ˜‚è´µäººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œè§„æ¨¡åŒ–åœ°è®­ç»ƒå¤§æ¨¡å‹è¿›è¡Œå¯éªŒè¯çš„å¤šè·³ç»„åˆæ¨ç†ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºå°†çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä½œä¸ºéšå¼å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡è·¯å¾„æ´¾ç”Ÿçš„å¥–åŠ±ä¿¡å·å¼•å¯¼å¼ºåŒ–å­¦ä¹ ï¼Œä½¿æ¨¡å‹å­¦ä¼šåŸºäºå…¬ç†äº‹å®ç»„åˆæ¨ç†ï¼Œæ˜¾è‘—æå‡é›¶æ ·æœ¬é•¿é“¾æ³›åŒ–èƒ½åŠ›ã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸é‡‡ç”¨â€œåŸºåº§æ¨¡å‹â†’ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰â†’å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰â€ä¸‰çº§åè®­ç»ƒæµç¨‹ï¼Œå…¶ä¸­SFTé˜¶æ®µç”¨1â€“3è·³KGè·¯å¾„ç”Ÿæˆçš„æ•°æ®æ³¨å…¥é¢†åŸŸåŸå­çŸ¥è¯†ï¼ŒRLé˜¶æ®µèšç„¦è¿‡ç¨‹ç›‘ç£ã€‚  \nğŸ”¸è®¾è®¡è·¯å¾„å¯¹é½å¥–åŠ±R_pathï¼šå°†æ¨¡å‹æ¨ç†é“¾ä¸­æå–çš„å®ä½“ä¸çœŸå®KGè·¯å¾„Pè¿›è¡Œè¦†ç›–åº¦åŒ¹é…ï¼ˆcoverageï¼‰ï¼Œå¹¶è®¾ç½®æœ€å°å‘½ä¸­çº¦æŸï¼ˆâ‰¥2ä¸ªä¸åŒè·¯å¾„å®ä½“ï¼‰å’Œé‡å¤æƒ©ç½šï¼Œç¡®ä¿é€»è¾‘ç»„åˆè€Œéè¡¨é¢å¤è¿°ã€‚  \nğŸ”¸ç»“åˆè´Ÿé‡‡æ ·äºŒå…ƒæ­£ç¡®æ€§å¥–åŠ±R_binï¼ˆé”™è¯¯ç­”æ¡ˆå¤§å¹…æƒ©ç½šï¼‰ï¼Œæ„å»ºå¤åˆå¥–åŠ±R_total = R_bin + R_pathï¼Œå…¼é¡¾ç»“æœæ­£ç¡®æ€§ä¸æ¨ç†è¿‡ç¨‹å¯éªŒè¯æ€§ã€‚  \nğŸ”¸å…¨ç¨‹ä½¿ç”¨åŒä¸€åŒ»å­¦KGï¼ˆUMLSï¼‰ç»Ÿä¸€æ•°æ®ç”Ÿæˆã€å¥–åŠ±è®¡ç®—ä¸è¯„ä¼°ï¼Œé¿å…åˆ†å¸ƒåç§»ï¼›RLé˜¶æ®µä»…ç”¨5ké«˜è´¨é‡æ ·æœ¬ï¼Œä»¥å°é¢„ç®—å®ç°é«˜æ•ˆèƒ½åŠ›è·ƒè¿ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸æ¨¡å‹åœ¨æœªè§è¿‡çš„4â€“5è·³åŒ»ç–—é—®ç­”ä¸Šç›¸å¯¹SFTåŸºçº¿æå‡7.5%â€“11.1%ï¼Œä¸”éš¾åº¦è¶Šé«˜å¢ç›Šè¶Šæ˜¾è‘—ï¼ˆ5çº§éš¾é¢˜å‡†ç¡®ç‡ä»48.93%å‡è‡³56.75%ï¼‰ï¼Œè¯å®è·¯å¾„å¥–åŠ±æ„å»ºäº†çœŸæ­£çš„â€œç»„åˆæ¡¥æ¢â€ã€‚  \nğŸ”¸åœ¨ICD-10å…¨éƒ¨15ä¸ªä¸´åºŠå­é¢†åŸŸå‡ç¨³å®šé¢†å…ˆï¼Œå°¤å…¶åœ¨é«˜é£é™©é¢†åŸŸï¼ˆå¦‚è¡€æ¶²/å…ç–«ã€å¾ªç¯ç³»ç»Ÿç–¾ç—…ï¼‰æå‡çªå‡ºï¼Œè¯´æ˜KGè·¯å¾„å¥–åŠ±èƒ½æ³›åŒ–è‡³å¤æ‚è¯æ®é“¾åœºæ™¯ã€‚  \nğŸ”¸é¢å¯¹é€‰é¡¹é¡ºåºæ‰°åŠ¨ç­‰æ ¼å¼æ”»å‡»ï¼Œæ€§èƒ½ä¸‹é™ä»…1.17%ï¼Œè¿œä¼˜äºGPT-5.2ç­‰å‰æ²¿æ¨¡å‹ï¼ˆé™å¹…4â€“6%ï¼‰ï¼Œè¡¨æ˜æ¨¡å‹ä¾èµ–é€»è¾‘è·¯å¾„è€Œéä½ç½®æ·å¾„ã€‚  \nğŸ”¸14Bæ¨¡å‹åœ¨5è·³ä»»åŠ¡ä¸Šå‡†ç¡®ç‡è¾¾89.33%ï¼Œè¶…è¶ŠGPT-5.2ã€Gemini 3 ProåŠ32Bä¸“å®¶æ¨¡å‹QwQ-Med-3ï¼ŒéªŒè¯â€œä¼˜è´¨å¥–åŠ±+å°æ¨¡å‹â€ä¼˜äºå•çº¯æ‰©å¤§å‚æ•°è§„æ¨¡ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºé¢ è¦†ä¼ ç»ŸRLå¥–åŠ±è®¾è®¡èŒƒå¼ï¼šä¸å†ä¾èµ–äººç±»åå¥½æˆ–è¡¨é¢ç›¸ä¼¼æ€§ï¼Œè€Œæ˜¯å°†ç»“æ„åŒ–çŸ¥è¯†å›¾è°±å‡æ ¼ä¸ºè‡ªåŠ¨ã€å¯æ‰©å±•ã€å¯éªŒè¯çš„â€œè¿‡ç¨‹ç›‘ç£è€…â€ã€‚å…¶æ ¸å¿ƒæ´è§â€”â€”çŸ¥è¯†å›¾è°±å¤©ç„¶è•´å«æ¨ç†é€»è¾‘çš„å…¬ç†åŒ–è·¯å¾„â€”â€”ä¸ºç§‘å­¦é¢†åŸŸå¯ä¿¡æ¨ç†æä¾›äº†æ™®é€‚æ€§æ¡†æ¶ï¼Œä¸”æ–¹æ³•è®ºå¯è¿ç§»è‡³åŒ–å­¦ã€æ³•å¾‹ç­‰ä»»ä½•å…·å¤‡ç»“æ„åŒ–çŸ¥è¯†ä½“ç³»çš„é¢†åŸŸã€‚\n    "
    },
    {
        "title": "Visual and Cognitive Demands of a Large Language Model-Powered In-vehicle Conversational Agent",
        "authors": [
            "Chris Monk",
            "Allegra Ayala",
            "Christine S. P. Yu",
            "Gregory M. Fitch",
            "Dara Gruber"
        ],
        "categories": [
            "cs.HC",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15034v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15034v1",
        "summary": "Driver distraction remains a leading contributor to motor vehicle crashes, necessitating rigorous evaluation of new in-vehicle technologies. This study assessed the visual and cognitive demands associated with an advanced Large Language Model (LLM) conversational agent (Gemini Live) during on-road driving, comparing it against handsfree phone calls, visual turn-by-turn guidance (low load baseline), and the Operation Span (OSPAN) task (high load anchor). Thirty-two licensed drivers completed five secondary tasks while visual and cognitive demands were measured using the Detection Response Task (DRT) for cognitive load, eye-tracking for visual attention, and subjective workload ratings. Results indicated that Gemini Live interactions (both single-turn and multi-turn) and hands-free phone calls shared similar levels of cognitive load, between that of visual turn-by-turn guidance and OSPAN. Exploratory analysis showed that cognitive load remained stable across extended multi-turn conversations. All tasks maintained mean glance durations well below the well-established 2-second safety threshold, confirming low visual demand. Furthermore, drivers consistently dedicated longer glances to the roadway between brief off-road glances toward the device during task completion, particularly during voice-based interactions, rendering longer total-eyes-off-road time findings less consequential. Subjective ratings mirrored objective data, with participants reporting low effort, demands, and perceived distraction for Gemini Live. These findings demonstrate that advanced LLM conversational agents, when implemented via voice interfaces, impose cognitive and visual demands comparable to established, low-risk hands-free benchmarks, supporting their safe deployment in the driving environment.",
        "category": "Agent",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15034v1ã€Agentã€‘Visual and Cognitive Demands of a Large Language Model-Powered In-vehicle Conversational Agent.pdf",
        "institution": "Googleã€Inc.",
        "note": "ğŸ“–æ ‡é¢˜ï¼šVisual and Cognitive Demands of a Large Language Model-Powered In-vehicle Conversational Agent\nğŸŒæ¥æºï¼šarXiv, 2601.15034v1\n\nç¬”è®°æ ‡é¢˜ï¼šLLMè½¦è½½å¯¹è¯ä»£ç†çš„å®‰å…¨æ€§éªŒè¯  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„è½¦è½½è¯­éŸ³å¯¹è¯ä»£ç†ï¼ˆå¦‚Gemini Liveï¼‰åœ¨çœŸå®é©¾é©¶ä¸­æ˜¯å¦å¼•å‘è¿‡é«˜çš„è§†è§‰æˆ–è®¤çŸ¥è´Ÿè·ï¼Œä»è€Œå¨èƒè¡Œè½¦å®‰å…¨ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šé¦–æ¬¡é€šè¿‡å®è½¦é“è·¯å®éªŒç³»ç»Ÿè¯„ä¼°LLM-poweredå¯¹è¯ä»£ç†çš„è®¤çŸ¥ä¸è§†è§‰è´Ÿè·ï¼Œè¯å®å…¶è´Ÿè·æ°´å¹³ä¸å·²çŸ¥ä½é£é™©çš„å…æé€šè¯ç›¸å½“ï¼Œæ”¯æŒå…¶å®‰å…¨éƒ¨ç½²ã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸é‡‡ç”¨äº”ä»»åŠ¡å¯¹ç…§è®¾è®¡ï¼šGemini Liveå•è½®/å¤šè½®å¯¹è¯ã€å…æç”µè¯ã€è§†è§‰å¯¼èˆªï¼ˆä½è´Ÿè·åŸºçº¿ï¼‰ã€OSPANå·¥ä½œè®°å¿†ä»»åŠ¡ï¼ˆé«˜è´Ÿè·é”šç‚¹ï¼‰ï¼Œæ§åˆ¶å˜é‡ä¸¥æ ¼ã€‚  \nğŸ”¸å¤šæ¨¡æ€å®¢è§‚æµ‹é‡ï¼šä½¿ç”¨æ£€æµ‹ååº”ä»»åŠ¡ï¼ˆDRTï¼‰é‡åŒ–è®¤çŸ¥è´Ÿè·ï¼Œçœ¼åŠ¨è¿½è¸ªåˆ†æå¹³å‡æ³¨è§†æ—¶é•¿ï¼ˆMGDï¼‰å’Œæ€»ç¦»è·¯æ—¶é—´ï¼ˆTEORTï¼‰ï¼Œç»“åˆNASA-TLXç­‰ä¸»è§‚é‡è¡¨äº¤å‰éªŒè¯ã€‚  \nğŸ”¸èšç„¦çœŸå®é©¾é©¶åœºæ™¯ï¼šåœ¨å…¬å…±é“è·¯å¼€å±•å®è½¦æµ‹è¯•ï¼Œ32åæŒè¯å¸æœºå®Œæˆå…¨éƒ¨ä»»åŠ¡ï¼Œè§„é¿æ¨¡æ‹Ÿå™¨ç”Ÿæ€æ•ˆåº¦å±€é™ã€‚  \nğŸ”¸åˆ›æ–°æ€§æ¢ç´¢åŠ¨æ€è´Ÿè·ï¼šå¯¹å¤šè½®å¯¹è¯è¿›è¡Œæ—¶åºåˆ†æï¼Œæ£€éªŒè®¤çŸ¥è´Ÿè·æ˜¯å¦éšå¯¹è¯å»¶é•¿è€Œç´¯ç§¯ä¸Šå‡ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸Gemini Liveå•è½®ä¸å¤šè½®å¯¹è¯çš„è®¤çŸ¥è´Ÿè·æ˜¾è‘—é«˜äºè§†è§‰å¯¼èˆªï¼Œä½†ä¸å…æç”µè¯æ— å·®å¼‚ï¼Œä¸”å‡æ˜¾è‘—ä½äºOSPANï¼Œç¡®è®¤å…¶å¤„äºâ€œä¸­ä½é£é™©â€åŒºé—´ã€‚  \nğŸ”¸æ‰€æœ‰è¯­éŸ³ä»»åŠ¡å¹³å‡æ³¨è§†æ—¶é•¿å‡è¿œä½äºNHTSA 2ç§’å®‰å…¨é˜ˆå€¼ï¼ˆæœ€é«˜ä»…0.7ç§’ï¼‰ï¼Œè§†è§‰è´Ÿè·æä½ï¼›å¤šè½®å¯¹è¯ä¸­é©¾é©¶å‘˜ä¸»åŠ¨å»¶é•¿é“è·¯æ³¨è§†é—´éš”ï¼Œæœ‰æ•ˆé‡å»ºæƒ…å¢ƒæ„è¯†ã€‚  \nğŸ”¸TEORTå—ä»»åŠ¡æ—¶é•¿å½±å“æ˜¾è‘—ï¼ŒGemini Liveå•è½®ï¼ˆ35ç§’ï¼‰å’Œå¤šè½®ï¼ˆ~2åˆ†é’Ÿï¼‰å› æŒç»­æ—¶é—´çŸ­è€ŒTEORTæ›´ä½ï¼Œä½†å³ä½¿å¤šè½®ä»»åŠ¡ä¹Ÿæ»¡è¶³è¡Œä¸š20ç§’æ ‡å‡†ã€‚  \nğŸ”¸ä¸»è§‚è¯„ä»·ä¸å®¢è§‚æ•°æ®é«˜åº¦ä¸€è‡´ï¼šå‚ä¸è€…æ™®éæŠ¥å‘ŠGemini Liveä½¿ç”¨è½»æ¾ã€å¯é ã€ä¸åˆ†æ•£æ³¨æ„åŠ›ï¼Œ94%æ„¿å†æ¬¡ä½¿ç”¨ï¼Œä¸”è®¤ä¸ºå…¶è®¤çŸ¥åŠªåŠ›ç¨‹åº¦ä¸å…æé€šè¯ç›¸å½“ã€‚  \nğŸ”¸å¤šè½®å¯¹è¯å…¨ç¨‹DRTè¡¨ç°ç¨³å®šï¼Œæ— missç‡æˆ–ååº”æ—¶é€’å¢è¶‹åŠ¿ï¼Œè¯æ˜å»¶é•¿äº¤äº’ä¸ä¼šå¯¼è‡´è®¤çŸ¥è´Ÿè·ç´¯ç§¯ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡æ ¸å¿ƒåˆ›æ–°åœ¨äºçªç ´ä¼ ç»Ÿè¯­éŸ³åŠ©æ‰‹è¯„ä¼°èŒƒå¼ï¼Œé¦–æ¬¡å°†å‰æ²¿LLMå¯¹è¯ç³»ç»Ÿç½®äºçœŸå®é©¾é©¶ç¯å¢ƒè¿›è¡Œå¤šç»´è´Ÿè·éªŒè¯ï¼Œå¹¶ä»¥å…æé€šè¯ä¸ºå…³é”®å‚ç…§ç³»â€”â€”è¿™æ¯”å•çº¯å¯¹æ¯”â€œæœ‰æ— ç•Œé¢â€æ›´å…·ç°å®æŒ‡å¯¼æ„ä¹‰ã€‚ç ”ç©¶è®¾è®¡ä¸¥è°¨å›åº”äº†ç›‘ç®¡ç©ºç™½ï¼ˆNHTSAå°šæœªå‘å¸ƒè¯­éŸ³æ¥å£æŒ‡å—ï¼‰ï¼Œç”¨å®è¯è¡¨æ˜ï¼šå½“LLMé€šè¿‡çº¯è¯­éŸ³äº¤äº’å®ç°æ—¶ï¼Œå…¶äººç±»ä¸­å¿ƒåŒ–å¯¹è¯èƒ½åŠ›ï¼ˆå¦‚ä¸Šä¸‹æ–‡ç†è§£ã€çº é”™ï¼‰å¹¶æœªé¢å¤–å¢åŠ é©¾é©¶è´Ÿæ‹…ï¼Œåè€Œå¯èƒ½é€šè¿‡è‡ªç„¶äº¤äº’é™ä½æ“ä½œæ‘©æ“¦ã€‚è¿™ä¸€ç»“è®ºä¸ºæ™ºèƒ½åº§èˆ±äººæœºååŒè®¾è®¡æä¾›äº†å…³é”®å®‰å…¨åŸºå‡†ã€‚\n    "
    },
    {
        "title": "The Pictorial Cortex: Zero-Shot Cross-Subject fMRI-to-Image Reconstruction via Compositional Latent Modeling",
        "authors": [
            "Jingyang Huo",
            "Yikai Wang",
            "Yanwei Fu",
            "Jianfeng Feng"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15071v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15071v1",
        "summary": "Decoding visual experiences from human brain activity remains a central challenge at the intersection of neuroscience, neuroimaging, and artificial intelligence. A critical obstacle is the inherent variability of cortical responses: neural activity elicited by the same visual stimulus differs across individuals and trials due to anatomical, functional, cognitive, and experimental factors, making fMRI-to-image reconstruction non-injective. In this paper, we tackle a challenging yet practically meaningful problem: zero-shot cross-subject fMRI-to-image reconstruction, where the visual experience of a previously unseen individual must be reconstructed without subject-specific training. To enable principled evaluation, we present a unified cortical-surface dataset -- UniCortex-fMRI, assembled from multiple visual-stimulus fMRI datasets to provide broad coverage of subjects and stimuli. Our UniCortex-fMRI is particularly processed by standardized data formats to make it possible to explore this possibility in the zero-shot scenario of cross-subject fMRI-to-image reconstruction. To tackle the modeling challenge, we propose PictorialCortex, which models fMRI activity using a compositional latent formulation that structures stimulus-driven representations under subject-, dataset-, and trial-related variability. PictorialCortex operates in a universal cortical latent space and implements this formulation through a latent factorization-composition module, reinforced by paired factorization and re-factorizing consistency regularization. During inference, surrogate latents synthesized under multiple seen-subject conditions are aggregated to guide diffusion-based image synthesis for unseen subjects. Extensive experiments show that PictorialCortex improves zero-shot cross-subject visual reconstruction, highlighting the benefits of compositional latent modeling and multi-dataset training.",
        "category": "æ¨¡å‹æ¶æ„",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15071v1ã€æ¨¡å‹æ¶æ„ã€‘The Pictorial Cortex_ Zero-Shot Cross-Subject fMRI-to-Image Reconstruction via Compositional Latent Modeling.pdf",
        "institution": "å¤æ—¦å¤§å­¦ã€ä¸Šæµ·äº¤é€šå¤§å­¦ã€æµ™æ±Ÿå¤§å­¦ã€å—æ´‹ç†å·¥å¤§å­¦",
        "note": "ğŸ“–æ ‡é¢˜ï¼šThe Pictorial Cortex: Zero-Shot Cross-Subject fMRI-to-Image Reconstruction via Compositional Latent Modeling\nğŸŒæ¥æºï¼šarXiv, 2601.15071v1\n\nç¬”è®°æ ‡é¢˜ï¼š compositional latent modeling for zero-shot fMRI decoding  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸ä½¿ç”¨ä»»ä½•æ–°å—è¯•è€…è®­ç»ƒæ•°æ®çš„å‰æä¸‹ï¼Œä»å…¶fMRIä¿¡å·ä¸­å‡†ç¡®é‡å»ºå…¶æ‰€è§å›¾åƒï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºäº†é¦–ä¸ªé¢å‘é›¶æ ·æœ¬è·¨è¢«è¯•fMRI-to-imageé‡å»ºçš„å¯è§£é‡ŠåŒ–ç»„åˆæ½œå˜é‡å»ºæ¨¡æ¡†æ¶PictorialCortexï¼Œå¹¶é…å¥—æ„å»ºäº†ç»Ÿä¸€çš®å±‚è¡¨é¢æ•°æ®é›†UniCortex-fMRIã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºUniCortex-fMRIæ•°æ®é›†ï¼Œæ•´åˆNSDã€BOLD5000ã€NODå’ŒHCP-Movieå››å¤§è§†è§‰fMRIæ•°æ®é›†ï¼Œç»Ÿä¸€æ˜ å°„è‡³fsLR-32kçš®å±‚è¡¨é¢å¹¶æ ‡å‡†åŒ–é¢„å¤„ç†ï¼Œæ”¯æŒå¯æ§çš„â€œå·²è§/æœªè§â€è¢«è¯•åˆ’åˆ†ã€‚  \nğŸ”¸æå‡ºPictorialCortexæ¡†æ¶ï¼Œåœ¨é€šç”¨çš®å±‚æ½œç©ºé—´ä¸­å¯¹æ¯ä¸ªfMRIè§‚æµ‹è¿›è¡Œå››å› å­åˆ†è§£ï¼šåˆºæ¿€é©±åŠ¨å› å­ï¼ˆå…±äº«è¯­ä¹‰ï¼‰ã€è¢«è¯•å› å­ã€æ•°æ®é›†å› å­å’Œæ®‹å·®å¹²æ‰°å› å­ã€‚  \nğŸ”¸è®¾è®¡æ½œå˜é‡åˆ†è§£â€“åˆæˆæ¨¡å—ï¼ˆLFCMï¼‰ï¼Œé€šè¿‡é…å¯¹åˆ†è§£é‡å»ºï¼ˆPFRï¼‰ä¸å†åˆ†è§£ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼ˆReFCRï¼‰è”åˆçº¦æŸï¼Œç¡®ä¿åˆºæ¿€è¡¨å¾è·¨è¢«è¯•ä¸å˜æ€§ã€‚  \nğŸ”¸æ¨ç†é˜¶æ®µå¯¹æœªè§è¢«è¯•fMRIå…ˆåšåˆæ­¥åˆ†è§£ï¼Œå†åˆ©ç”¨å¤šä¸ªå·²è§è¢«è¯•æ¡ä»¶åˆæˆä»£ç†æ½œå˜é‡å¹¶é‡åˆ†è§£ï¼Œèšåˆå¾—åˆ°é²æ£’çš„åˆºæ¿€é©±åŠ¨ç ï¼Œé©±åŠ¨IP-Adapteræ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨NSDã€BOLD5000ã€NODå’ŒHCP-Movieå››ä¸ªæ•°æ®é›†ä¸Šï¼ŒPictorialCortexæ˜¾è‘—è¶…è¶ŠMindBridgeã€MindEye2å’ŒNeuroPictorç­‰åŸºçº¿æ–¹æ³•ï¼Œå¹³å‡PixCorræå‡è¶…100%ï¼ŒCLIPåˆ†ç±»å‡†ç¡®ç‡æå‡è¶…14%ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šç§»é™¤ä»»ä¸€å› å­ï¼ˆè¢«è¯•/æ•°æ®é›†/å¹²æ‰°ï¼‰å‡å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå…¶ä¸­å¿½ç•¥å¹²æ‰°å› å­æŸå®³æœ€å¤§ï¼Œè¯´æ˜æ˜¾å¼å»ºæ¨¡trial-wiseå˜å¼‚æ€§å¯¹è§£è€¦è‡³å…³é‡è¦ã€‚  \nğŸ”¸å¤šæ•°æ®é›†è”åˆè®­ç»ƒæ¯”å•æ•°æ®é›†è®­ç»ƒæ›´ä¼˜ï¼Œå°¤å…¶æå‡AlexNetå’ŒCLIPç­‰é«˜å±‚è¯­ä¹‰æŒ‡æ ‡ï¼ŒéªŒè¯æ•°æ®å¤šæ ·æ€§å¯¹æ³›åŒ–èƒ½åŠ›çš„å…³é”®ä½œç”¨ã€‚  \nğŸ”¸è¢«è¯•æ•°é‡ä¸æ€§èƒ½å‘ˆå¼ºæ­£ç›¸å…³ï¼Œ10â†’25äººé˜¶æ®µå¢ç›Šæœ€æ˜¾è‘—ï¼Œ209äººæ—¶è¶‹äºé¥±å’Œï¼Œè¡¨æ˜è¶³å¤Ÿè¢«è¯•è§„æ¨¡æ˜¯å­¦ä¹ ç¨³å®šè·¨è¢«è¯•è¡¨å¾çš„å‰æã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡çš„åˆ›æ–°ç‚¹åœ¨äºå°†ç¥ç»è§£ç é—®é¢˜å½¢å¼åŒ–ä¸ºå¯è§£é‡Šçš„ç»“æ„åŒ–æ½œå˜é‡åˆ†ç¦»ä»»åŠ¡ï¼Œçªç ´äº†ä»¥å¾€ç«¯åˆ°ç«¯é»‘ç®±å»ºæ¨¡èŒƒå¼ï¼›å…¶ç»„åˆå»ºæ¨¡æ€æƒ³ï¼ˆstimulus+subject+dataset+nuisanceï¼‰å…¼å…·ç¥ç»åˆç†æ€§ä¸å·¥ç¨‹å¯æ“ä½œæ€§ï¼Œä¸”é€šè¿‡ä»£ç†æ½œå˜é‡èšåˆç­–ç•¥å·§å¦™ç»•è¿‡é›¶æ ·æœ¬åˆ†å¸ƒåç§»éš¾é¢˜ï¼Œä¸ºè„‘æœºæ¥å£ä¸­çš„ä¸ªä½“æ— å…³å»ºæ¨¡æä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "Rethinking Video Generation Model for the Embodied World",
        "authors": [
            "Yufan Deng",
            "Zilin Pan",
            "Hongyu Zhang",
            "Xiaojie Li",
            "Ruoqing Hu",
            "Yufei Ding",
            "Yiming Zou",
            "Yan Zeng",
            "Daquan Zhou"
        ],
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15282v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15282v1",
        "summary": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.",
        "category": "RBenchä»»åŠ¡è¯„æµ‹",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15282v1ã€RBenchä»»åŠ¡è¯„æµ‹ã€‘Rethinking Video Generation Model for the Embodied World.pdf",
        "institution": "åŒ—äº¬å¤§å­¦ã€å­—èŠ‚è·³åŠ¨",
        "note": "ğŸ“–æ ‡é¢˜ï¼šRethinking Video Generation Model for the Embodied World\nğŸŒæ¥æºï¼šarXiv, 2601.15282v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºå…·èº«è§†é¢‘ç”Ÿæˆæ–°åŸºå‡†\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•æœ‰æ•ˆè¯„ä¼°å’Œæå‡é¢å‘å…·èº«æœºå™¨äººçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ç‰©ç†çœŸå®æ€§å’Œä»»åŠ¡å®Œæˆèƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºé¦–ä¸ªé¢å‘æœºå™¨äººè§†é¢‘ç”Ÿæˆçš„ç»¼åˆåŸºå‡†RBenchä¸å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†RoVid-Xï¼Œæ¨åŠ¨å…·èº«AIå‘å±•ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡RBenchåŸºå‡†ï¼Œæ¶µç›–5ç±»ä»»åŠ¡ï¼ˆæ“ä½œã€é•¿ç¨‹è§„åˆ’ã€å¤šå®ä½“åä½œã€ç©ºé—´å…³ç³»ã€è§†è§‰æ¨ç†ï¼‰å’Œ4ç§æœºå™¨äººå½¢æ€ï¼Œå…±650ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹ã€‚  \nğŸ”¸æ„å»ºç»†ç²’åº¦è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ä»»åŠ¡å®Œæˆåº¦ï¼ˆç‰©ç†è¯­ä¹‰åˆç†æ€§ã€ä»»åŠ¡ä¸€è‡´æ€§ï¼‰å’Œè§†è§‰è´¨é‡ï¼ˆè¿åŠ¨å¹…åº¦ã€å¹³æ»‘æ€§ã€ä¸»ä½“ç¨³å®šæ€§ï¼‰ã€‚  \nğŸ”¸é€šè¿‡å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰å®ç°é›¶æ ·æœ¬è§†é¢‘è¯„ä¼°ï¼Œç»“åˆä½å±‚è¿åŠ¨ç»Ÿè®¡å¢å¼ºåˆ¤åˆ«åŠ›ã€‚  \nğŸ”¸å»ºç«‹å››é˜¶æ®µæ•°æ®ç®¡é“ï¼šæœºå™¨äººè§†é¢‘æ”¶é›†ã€è´¨é‡è¿‡æ»¤ã€ä»»åŠ¡åˆ†å‰²ä¸æè¿°ã€ç‰©ç†å±æ€§æ ‡æ³¨ï¼Œæ„å»ºRoVid-Xæ•°æ®é›†ã€‚  \nğŸ”¸å‘å¸ƒå«400ä¸‡æ ‡æ³¨è§†é¢‘ç‰‡æ®µçš„RoVid-Xï¼Œè¦†ç›–åƒçº§ä»»åŠ¡ç±»å‹ï¼Œå¹¶æä¾›å…‰å­¦æµã€æ·±åº¦å›¾ç­‰ç‰©ç†æ³¨é‡Šã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸25ä¸ªä¸»æµè§†é¢‘æ¨¡å‹åœ¨RBenchä¸Šè¡¨ç°æ™®éä¸ä½³ï¼Œå°¤å…¶åœ¨ç‰©ç†åˆç†æ€§å’ŒåŠ¨ä½œå®Œæ•´æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚  \nğŸ”¸å•†ä¸šé—­æºæ¨¡å‹æ•´ä½“ä¼˜äºå¼€æºæ¨¡å‹ï¼Œä½†Soraç³»åˆ—åœ¨ç‰©ç†çœŸå®æ€§ä»»åŠ¡ä¸­è¡¨ç°æ¬ ä½³ï¼Œæ­ç¤ºåª’ä½“ç”Ÿæˆä¸å…·èº«æ¨¡æ‹Ÿçš„å·®è·ã€‚  \nğŸ”¸RBenchè¯„åˆ†ä¸äººç±»åå¥½é«˜åº¦ç›¸å…³ï¼ˆSpearman Ï=0.96ï¼‰ï¼ŒéªŒè¯å…¶æœ‰æ•ˆæ€§ä¸å¯é æ€§ã€‚  \nğŸ”¸ä½¿ç”¨RoVid-Xå¾®è°ƒæ¨¡å‹åï¼Œåœ¨å„ç±»ä»»åŠ¡å’Œå½¢æ€ä¸Šå‡å–å¾—ç¨³å®šæ€§èƒ½æå‡ï¼Œè¯æ˜æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚  \nğŸ”¸å½“å‰æ¨¡å‹åœ¨è®¤çŸ¥æ¨ç†ä¸ç²¾ç»†æ“æ§ä»»åŠ¡ä¸Šç“¶é¢ˆæ˜æ˜¾ï¼Œè€Œç²—ç²’åº¦è¿åŠ¨ï¼ˆå¦‚å››è¶³è¡Œèµ°ï¼‰ç›¸å¯¹æ›´æ˜“ç”Ÿæˆã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡åˆ›æ–°æ€§åœ°å°†è§†é¢‘ç”Ÿæˆä»â€œè§†è§‰ä¿çœŸâ€æ¨å‘â€œç‰©ç†æ™ºèƒ½â€ï¼Œæå‡ºå…¼å…·ä»»åŠ¡é€»è¾‘ä¸ç‰©ç†è§„å¾‹çš„è¯„ä¼°ä½“ç³»RBenchï¼Œå¹¶é…å¥—æ„å»ºè¶…å¤§è§„æ¨¡å…·èº«æ•°æ®é›†RoVid-Xã€‚å…¶æ ¸å¿ƒä»·å€¼åœ¨äºå»ºç«‹äº†å¯å¤ç°ã€å¯é‡åŒ–çš„è¯„ä»·æ ‡å‡†ï¼Œå¡«è¡¥äº†å…·èº«è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„å…³é”®ç©ºç™½ã€‚åŒæ—¶ï¼Œæ•°æ®ä¸è¯„ä¼°ååŒæ¨è¿›çš„èŒƒå¼ä¸ºåç»­ç ”ç©¶æä¾›äº†åšå®åŸºç¡€ï¼Œæœ‰æœ›åŠ é€Ÿé€šç”¨å…·èº«æ™ºèƒ½çš„å‘å±•ã€‚\n    "
    },
    {
        "title": "LiViBench: An Omnimodal Benchmark for Interactive Livestream Video Understanding",
        "authors": [
            "Xiaodong Wang",
            "Langling Huang",
            "Zhirong Wu",
            "Xu Zhao",
            "Teng Xu",
            "Xuhong Xia",
            "Peixi Peng"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.15016v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15016v1",
        "summary": "The development of multimodal large language models (MLLMs) has advanced general video understanding. However, existing video evaluation benchmarks primarily focus on non-interactive videos, such as movies and recordings. To fill this gap, this paper proposes the first omnimodal benchmark for interactive livestream videos, LiViBench. It features a diverse set of 24 tasks, highlighting the perceptual, reasoning, and livestream-specific challenges. To efficiently construct the dataset, we design a standardized semi-automatic annotation workflow that incorporates the human-in-the-loop at multiple stages. The workflow leverages multiple MLLMs to form a multi-agent system for comprehensive video description and uses a seed-question-driven method to construct high-quality annotations. All interactive videos in the benchmark include audio, speech, and real-time comments modalities. To enhance models' understanding of interactive videos, we design tailored two-stage instruction-tuning and propose a Video-to-Comment Retrieval (VCR) module to improve the model's ability to utilize real-time comments. Based on these advancements, we develop LiVi-LLM-7B, an MLLM with enhanced knowledge of interactive livestreams. Experiments show that our model outperforms larger open-source models with up to 72B parameters, narrows the gap with leading proprietary models on LiViBench, and achieves enhanced performance on general video benchmarks, including VideoMME, LongVideoBench, MLVU, and VideoEval-Pro.",
        "category": "LiViBenchä»»åŠ¡è¯„æµ‹",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.15016v1ã€LiViBenchä»»åŠ¡è¯„æµ‹ã€‘LiViBench_ An Omnimodal Benchmark for Interactive Livestream Video Understanding.pdf",
        "institution": "Peking Universityã€Douyin Group",
        "note": "ğŸ“–æ ‡é¢˜ï¼šLiViBench: An Omnimodal Benchmark for Interactive Livestream Video Understanding\nğŸŒæ¥æºï¼šarXiv, 2601.15016v1\n\nç¬”è®°æ ‡é¢˜ï¼šé¦–ä¸ªç›´æ’­è§†é¢‘å¤šæ¨¡æ€åŸºå‡†  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•æœ‰æ•ˆè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹å¯¹å®æ—¶äº¤äº’å¼ç›´æ’­è§†é¢‘çš„ç†è§£èƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºäº†é¦–ä¸ªé¢å‘äº¤äº’å¼ç›´æ’­è§†é¢‘çš„å…¨æ¨¡æ€è¯„æµ‹åŸºå‡†LiViBenchï¼Œå¹¶é…å¥—æ„å»ºé«˜è´¨é‡æ ‡æ³¨æµç¨‹ã€ä¸“ç”¨è®­ç»ƒæ–¹æ³•åŠé«˜æ€§èƒ½æ¨¡å‹LiVi-LLM-7Bã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡æ¶µç›–éŸ³é¢‘ã€è¯­éŸ³ï¼ˆASRï¼‰å’Œå®æ—¶å¼¹å¹•ä¸‰æ¨¡æ€çš„ç›´æ’­è§†é¢‘æ•°æ®é›†ï¼ŒåŒ…å«3168ä¸ªçœŸå®åœºæ™¯è§†é¢‘åŠ3175é“å¤šé€‰é¢˜ï¼Œè¦†ç›–9ç±»å‚ç›´é¢†åŸŸä¸24é¡¹ä»»åŠ¡ã€‚  \nğŸ”¸æå‡ºäººæœºååŒçš„åŠè‡ªåŠ¨æ ‡æ³¨æµæ°´çº¿ï¼šåŸºäºå¤šæ¨¡å‹ä»£ç†ç³»ç»Ÿç”Ÿæˆäº’è¡¥è§†é¢‘æè¿°ï¼Œç»“åˆç§å­é—®é¢˜åº“é©±åŠ¨é—®é¢˜ç”Ÿæˆï¼Œå¹¶åœ¨å¤šä¸ªç¯èŠ‚åµŒå…¥äººå·¥å®¡æ ¸ä¸ç²¾ä¿®ã€‚  \nğŸ”¸æ„å»ºä¸¤é˜¶æ®µæŒ‡ä»¤å¾®è°ƒç­–ç•¥ï¼šç¬¬ä¸€é˜¶æ®µç”¨37953æ¡åˆæˆæ•°æ®å¯¹é½ç›´æ’­åŸŸï¼Œç¬¬äºŒé˜¶æ®µç”¨11180æ¡äººå·¥ç²¾æ ‡æ•°æ®æå‡ç»†ç²’åº¦é²æ£’æ€§ã€‚  \nğŸ”¸å¼•å…¥è§†é¢‘åˆ°å¼¹å¹•æ£€ç´¢ï¼ˆVCRï¼‰æ¨¡å—ï¼Œåˆ©ç”¨è§†é¢‘å¸§ä¸å¼¹å¹•æ–‡æœ¬çš„è·¨æ¨¡æ€ç›¸ä¼¼åº¦ç­›é€‰å…³é”®è¯„è®ºï¼Œç¼“è§£é•¿ä¸Šä¸‹æ–‡å‹åŠ›å¹¶å¢å¼ºäº¤äº’ç†è§£ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸ç°æœ‰ä¸»æµæ¨¡å‹ï¼ˆåŒ…æ‹¬GPT-4oã€Gemini-2.5-Proï¼‰åœ¨LiViBenchä¸Šè¡¨ç°æ˜¾è‘—ä½äºå…¶åœ¨é€šç”¨è§†é¢‘åŸºå‡†ä¸Šçš„æ°´å¹³ï¼Œå°¤å…¶åœ¨ç›´æ’­ç‰¹æœ‰ä»»åŠ¡ï¼ˆå¦‚å¤šäººäº’åŠ¨ã€è¡Œä¸ºå½’å› ï¼‰ä¸Šå‡†ç¡®ç‡æœ€ä½ã€‚  \nğŸ”¸éŸ³é¢‘æ¨¡æ€å¯¹æ‰€æœ‰æ¨¡å‹å‡æœ‰æ­£å‘å¢ç›Šï¼Œå°¤å…¶åœ¨ç›´æ’­ç‰¹æœ‰ä»»åŠ¡ä¸­æå‡æœ€æ˜æ˜¾ï¼›è€Œè¯­éŸ³ï¼ˆASRï¼‰æ¨¡æ€åœ¨ç»†ç²’åº¦æ„ŸçŸ¥å’Œæ¨ç†ä»»åŠ¡ä¸­å¶æœ‰è´Ÿå‘å¹²æ‰°ï¼Œè¡¨æ˜å™ªå£°éœ€è¢«è°¨æ…å¤„ç†ã€‚  \nğŸ”¸åŸå§‹å¼¹å¹•è¾“å…¥ä¼šé™ä½å¤šæ•°æ¨¡å‹æ€§èƒ½ï¼Œè€ŒVCRæ¨¡å—èƒ½ç¨³å®šæå‡å„é•¿åº¦åŒºé—´å¼¹å¹•ä¸‹çš„å‡†ç¡®ç‡ï¼ŒéªŒè¯å…¶åœ¨ä¿¡æ¯å‹ç¼©ä¸ç›¸å…³æ€§å»ºæ¨¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚  \nğŸ”¸LiVi-LLM-7Bä»¥7Bå‚æ•°é‡è¶…è¶ŠQwen2.5-VL-72Bç­‰æ›´å¤§å¼€æºæ¨¡å‹ï¼Œåœ¨LiViBenchä¸Šè¾¾64.4%å‡†ç¡®ç‡ï¼Œé€¼è¿‘é¡¶å°–é—­æºæ¨¡å‹ï¼Œå¹¶åœ¨Video-MMEç­‰é€šç”¨åŸºå‡†ä¸ŠåŒæ­¥æå‡ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºç²¾å‡†é”šå®šâ€œäº¤äº’æ€§â€è¿™ä¸€ç›´æ’­è§†é¢‘æ ¸å¿ƒç‰¹è´¨ï¼Œä»è¯„æµ‹åŸºå‡†ã€æ•°æ®æ„å»ºèŒƒå¼ã€æ¨¡æ€èåˆæœºåˆ¶åˆ°æ¨¡å‹è®­ç»ƒç­–ç•¥å½¢æˆé—­ç¯åˆ›æ–°ï¼›å…¶å¤šä»£ç†+ç§å­é—®é¢˜+äººæœºååŒçš„æ ‡æ³¨æ¡†æ¶ä¸ºé«˜æˆæœ¬è§†é¢‘æ•°æ®å»ºè®¾æä¾›äº†å¯å¤ç”¨çš„æ–¹æ³•è®ºï¼ŒVCRæ¨¡å—åˆ™ä¸ºæµ·é‡å¼±ç›‘ç£äº¤äº’ä¿¡å·çš„æœ‰æ•ˆåˆ©ç”¨å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚\n    "
    },
    {
        "title": "InstructTime++: Time Series Classification with Multimodal Language Modeling via Implicit Feature Enhancement",
        "authors": [
            "Mingyue Cheng",
            "Xiaoyu Tao",
            "Huajian Zhang",
            "Qi Liu",
            "Enhong Chen"
        ],
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14968v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14968v1",
        "summary": "Most existing time series classification methods adopt a discriminative paradigm that maps input sequences directly to one-hot encoded class labels. While effective, this paradigm struggles to incorporate contextual features and fails to capture semantic relationships among classes. To address these limitations, we propose InstructTime, a novel framework that reformulates time series classification as a multimodal generative task. Specifically, continuous numerical sequences, contextual textual features, and task instructions are treated as multimodal inputs, while class labels are generated as textual outputs by tuned language models. To bridge the modality gap, InstructTime introduces a time series discretization module that converts continuous sequences into discrete temporal tokens, together with an alignment projection layer and a generative self-supervised pre-training strategy to enhance cross-modal representation alignment. Building upon this framework, we further propose InstructTime++, which extends InstructTime by incorporating implicit feature modeling to compensate for the limited inductive bias of language models. InstructTime++ leverages specialized toolkits to mine informative implicit patterns from raw time series and contextual inputs, including statistical feature extraction and vision-language-based image captioning, and translates them into textual descriptions for seamless integration. Extensive experiments on multiple benchmark datasets demonstrate the superior performance of InstructTime++.",
        "category": "æ¨¡å‹æ¶æ„",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14968v1ã€æ¨¡å‹æ¶æ„ã€‘InstructTime++_ Time Series Classification with Multimodal Language Modeling via Implicit Feature Enhancement.pdf",
        "institution": "ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦ã€State Key Laboratory of Cognitive Intelligenceã€University of Science and Technology of China",
        "note": "ğŸ“–æ ‡é¢˜ï¼šInstructTime++: Time Series Classification with Multimodal Language Modeling via Implicit Feature Enhancement\nğŸŒæ¥æºï¼šarXiv, 2601.14968v1\n\nç¬”è®°æ ‡é¢˜ï¼šå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å¢å¼ºæ—¶åºåˆ†ç±»  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•å…‹æœç°æœ‰æ—¶é—´åºåˆ—åˆ†ç±»æ–¹æ³•åœ¨å»ºæ¨¡ä¸Šä¸‹æ–‡ç‰¹å¾å’Œç±»é—´è¯­ä¹‰å…³ç³»ä¸Šçš„å›ºæœ‰å±€é™ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºInstructTime++æ¡†æ¶ï¼Œå°†æ—¶é—´åºåˆ—åˆ†ç±»é‡æ„ä¸ºèåˆæ˜¾å¼ä¸éšå¼ç‰¹å¾çš„å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ï¼Œæ˜¾è‘—æå‡åˆ†ç±»æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›ã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å°†è¿ç»­æ—¶é—´åºåˆ—é€šè¿‡å‘é‡é‡åŒ–ç½‘ç»œç¦»æ•£åŒ–ä¸ºæ—¶åºtokenï¼Œå¼¥åˆæ•°å€¼ä¸æ–‡æœ¬æ¨¡æ€é¸¿æ²Ÿã€‚  \nğŸ”¸è®¾è®¡å¯¹é½æŠ•å½±å±‚ä¸ç”Ÿæˆå¼è‡ªç›‘ç£é¢„è®­ç»ƒç­–ç•¥ï¼Œå¼ºåŒ–è·¨æ¨¡æ€è¡¨å¾å¯¹é½ã€‚  \nğŸ”¸å¼•å…¥ç»Ÿè®¡ç‰¹å¾æå–ä¸è§†è§‰-è¯­è¨€å›¾åƒæè¿°ä¸¤å¤§å·¥å…·åŒ…ï¼Œè‡ªåŠ¨æŒ–æ˜å¹¶æ–‡æœ¬åŒ–éšå¼æ—¶åºæ¨¡å¼ã€‚  \nğŸ”¸å°†éšå¼ç‰¹å¾ç¿»è¯‘ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œæ— ç¼é›†æˆè‡³æŒ‡ä»¤é©±åŠ¨çš„ç”Ÿæˆå¼promptä¸­ã€‚  \nğŸ”¸é‡‡ç”¨ä»»åŠ¡æŒ‡ä»¤å¾®è°ƒå¯¹é½åçš„è¯­è¨€æ¨¡å‹ï¼Œå¢å¼ºå…¶å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸è·¨åŸŸè‡ªç›‘ç£é¢„è®­ç»ƒæ˜¾è‘—æå‡æ¨¡å‹åœ¨EEGã€ECGç­‰å¤æ‚æ—¶åºæ•°æ®ä¸Šçš„æ³›åŒ–æ€§ä¸é²æ£’æ€§ã€‚  \nğŸ”¸æŒ‡ä»¤æ–‡æœ¬å¯¹æ¨¡æ€å¯¹é½è‡³å…³é‡è¦ï¼Œç§»é™¤åæ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œt-SNEå¯è§†åŒ–è¯å®å…¶æå‡è¯­ä¹‰ç»“æ„æ¸…æ™°åº¦ã€‚  \nğŸ”¸éšå¼ç‰¹å¾æ¨¡å—è´¡çŒ®æ˜ç¡®ï¼šè§†è§‰ç‰¹å¾å¯¹EEG/HARæå‡æ˜æ˜¾ï¼Œç»Ÿè®¡ç‰¹å¾å¯¹ECG/RWCæ›´å…³é”®ï¼ŒäºŒè€…äº’è¡¥ã€‚  \nğŸ”¸ä¸­ç­‰è§„æ¨¡éª¨å¹²æ¨¡å‹ï¼ˆQwen3-0.6Bï¼‰åœ¨å¤šæ•°æ•°æ®é›†ä¸Šè¡¨ç°æœ€ä¼˜ï¼Œè¡¨æ˜å®¹é‡ä¸ä»»åŠ¡å¤æ‚åº¦éœ€å¹³è¡¡ã€‚  \nğŸ”¸VQ tokenæ•°ä¸patchå°ºå¯¸å­˜åœ¨â€œç”œç‚¹â€åŒºé—´ï¼Œè¿‡å°æˆ–è¿‡å¤§å‡æŸå®³é‡å»ºè´¨é‡ä¸åˆ†ç±»ç²¾åº¦ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºä¸‰é‡èŒƒå¼çªç ´ï¼šä¸€æ˜¯å°†åˆ¤åˆ«å¼åˆ†ç±»è½¬å‘ç”Ÿæˆå¼å¤šæ¨¡æ€ç†è§£ï¼›äºŒæ˜¯é¦–æ¬¡ç³»ç»Ÿå¼•å…¥å¯è§£é‡Šã€å¯æ’æ‹”çš„éšå¼ç‰¹å¾æŒ–æ˜æœºåˆ¶ï¼Œå¼¥è¡¥LLMæ—¶åºå½’çº³åç½®ä¸è¶³ï¼›ä¸‰æ˜¯æ„å»ºç»Ÿä¸€æ–‡æœ¬åŒ–æ¥å£ï¼Œå®ç°æ˜¾å¼ä¸Šä¸‹æ–‡ã€ä»»åŠ¡æŒ‡ä»¤ä¸éšå¼æ¨¡å¼çš„ç«¯åˆ°ç«¯ååŒæ¨ç†ï¼Œä¸ºæ—¶åºAIæä¾›äº†å…¼å…·è¯­ä¹‰æ€§ã€å¯è§£é‡Šæ€§ä¸å¼ºæ³›åŒ–èƒ½åŠ›çš„æ–°è·¯å¾„ã€‚\n    "
    },
    {
        "title": "CorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning",
        "authors": [
            "Zhiyuan Lu",
            "Chenliang Li",
            "Yingcheng Shi",
            "Weizhou Shen",
            "Ming Yan",
            "Fei Huang"
        ],
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14952v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14952v1",
        "summary": "While large language models now handle million-token contexts, their capacity for reasoning across entire document repositories remains largely untested. Existing benchmarks are inadequate, as they are mostly limited to single long texts or rely on a \"sparse retrieval\" assumption-that answers can be derived from a few relevant chunks. This assumption fails for true corpus-level analysis, where evidence is highly dispersed across hundreds of documents and answers require global integration, comparison, and statistical aggregation. To address this critical gap, we introduce CorpusQA, a new benchmark scaling up to 10 million tokens, generated via a novel data synthesis framework. By decoupling reasoning from textual representation, this framework creates complex, computation-intensive queries with programmatically guaranteed ground-truth answers, challenging systems to perform holistic reasoning over vast, unstructured text without relying on fallible human annotation. We further demonstrate the utility of our framework beyond evaluation, showing that fine-tuning on our synthesized data effectively enhances an LLM's general long-context reasoning capabilities. Extensive experiments reveal that even state-of-the-art long-context LLMs struggle as input length increases, and standard retrieval-augmented generation systems collapse entirely. Our findings indicate that memory-augmented agentic architectures offer a more robust alternative, suggesting a critical shift is needed from simply extending context windows to developing advanced architectures for global information synthesis.",
        "category": "10ç™¾ä¸‡tokenè¯­æ–™åº“è¯„æµ‹",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14952v1ã€10ç™¾ä¸‡tokenè¯­æ–™åº“è¯„æµ‹ã€‘CorpusQA_ A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning.pdf",
        "institution": "é˜¿é‡Œã€Alibaba Group",
        "note": "ğŸ“–æ ‡é¢˜ï¼šCorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning\nğŸŒæ¥æºï¼šarXiv, 2601.14952v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºåƒä¸‡çº§è¯­æ–™æ¨ç†åŸºå‡†  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•æœ‰æ•ˆè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨è¶…å¤§è§„æ¨¡ã€é«˜è¯æ®åˆ†æ•£æ€§æ–‡æ¡£é›†åˆä¸Šçš„å…¨å±€æ¨ç†èƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºäº†é¦–ä¸ªé¢å‘è¯­æ–™çº§åˆ†æçš„1000ä¸‡tokenåŸºå‡†CorpusQAï¼Œé…å¥—å¯éªŒè¯çœŸå€¼çš„æ•°æ®åˆæˆæ¡†æ¶ï¼Œå¹¶æ­ç¤ºäº†RAGå¤±æ•ˆä¸è®°å¿†å¢å¼ºæ™ºèƒ½ä½“æ›´å…·é²æ£’æ€§çš„å…³é”®ç°è±¡ã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡ä¸‰åŸåˆ™é©±åŠ¨çš„åŸºå‡†ï¼š unprecedented scaleï¼ˆè¾¾10M tokenï¼‰ã€high evidence dispersionï¼ˆè¯æ®è·¨æ•°ç™¾æ–‡æ¡£é«˜åº¦ç¦»æ•£ï¼‰ã€guaranteed factual groundingï¼ˆé€šè¿‡ç»“æ„åŒ–schema+NL2SQLç¨‹åºåŒ–ç”ŸæˆçœŸå€¼ï¼‰ã€‚  \nğŸ”¸æå‡ºè§£è€¦å¼æ•°æ®åˆæˆæµç¨‹ï¼šå…ˆä»PDFæ–‡æ¡£ä¸­æå–ç»“æ„åŒ–JSON schemaï¼Œå†èšåˆä¸ºå…¨å±€æ•°æ®è¡¨ï¼Œæœ€åç”¨NL2SQLå°†æ¨¡æ¿åŒ–æŸ¥è¯¢è½¬ä¸ºå¯æ‰§è¡ŒSQLå¹¶æ‰§è¡Œè·å–çœŸå€¼ï¼Œå½»åº•è§„é¿äººå·¥æˆ–LLMæ ‡æ³¨è¯¯å·®ã€‚  \nğŸ”¸é‡‡ç”¨å¤šé˜¶æ®µè´¨é‡æ§åˆ¶ï¼šå¤šæ¨¡å‹æŠ•ç¥¨æå–schemaã€äººå·¥æŠ½æ ·éªŒè¯ï¼ˆå‡†ç¡®ç‡è¶…94%ï¼‰ã€å›°éš¾åº¦åˆ†å±‚ï¼ˆEasy/Medium/Hardï¼‰è¦†ç›–ç»Ÿè®¡èšåˆã€å¤šæ¡ä»¶è®¡ç®—ä¸å¤šæ­¥ä¸šåŠ¡é€»è¾‘ã€‚  \nğŸ”¸ç³»ç»Ÿæ€§è¯„æµ‹ä¸‰ç±»æ–¹æ³•ï¼šåŸºç¡€é•¿ä¸Šä¸‹æ–‡LLMã€æ ‡å‡†RAGç³»ç»Ÿã€å†…å­˜å¢å¼ºå‹æ™ºèƒ½ä½“ï¼ˆMemory Agentï¼‰ï¼Œè¦†ç›–128Kè‡³10Må››æ¡£è§„æ¨¡ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸æ‰€æœ‰å‰æ²¿LLMåœ¨128Kè¯­å¢ƒä¸‹è¡¨ç°å°šå¯ï¼ˆæœ€é«˜82%ï¼‰ï¼Œä½†æ‰©å±•è‡³1Mæ—¶æ€§èƒ½æ–­å´–å¼ä¸‹é™ï¼ˆå¦‚Gemini-2.5-Proä»80%â†’51%ï¼‰ï¼Œè¯å®å•çº¯æ‰©å¤§ä¸Šä¸‹æ–‡çª—å£æ— æ³•æ”¯æ’‘è¯­æ–™çº§æ¨ç†ã€‚  \nğŸ”¸RAGç³»ç»Ÿåœ¨4M/10Mè§„æ¨¡ä¸‹è¿‘ä¹å®Œå…¨å¤±æ•ˆï¼ˆæœ€ä½ä»…0.00%ï¼‰ï¼Œå› å…¶æ£€ç´¢æœºåˆ¶æ— æ³•è¦†ç›–é«˜åº¦åˆ†æ•£çš„è¯æ®ï¼ŒéªŒè¯äº†â€œç¨€ç–æ£€ç´¢â€å‡è®¾åœ¨è¯­æ–™çº§ä»»åŠ¡ä¸­çš„æ ¹æœ¬æ€§å¤±æ•ˆã€‚  \nğŸ”¸Memory Agentè™½æ€§èƒ½éšè§„æ¨¡é€’å‡ï¼Œä½†åœ¨10Mä¸‹ä»ä¿æŒ11%å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºRAGï¼ˆ1.2%ï¼‰ï¼Œè¯æ˜å†…å­˜èšåˆæœºåˆ¶æ˜¯æ›´å¯è¡Œçš„ç³»ç»Ÿçº§æ¼”è¿›æ–¹å‘ã€‚  \nğŸ”¸åŸºäºåˆæˆæ•°æ®å¾®è°ƒQwen3-4Båï¼Œåœ¨CorpusQAåŠå¤–éƒ¨åŸºå‡†ï¼ˆLongBenchV2ã€FRAMESï¼‰ä¸Šå‡æå‡è¶…7%ï¼Œè¯´æ˜è¯¥æ¡†æ¶ç”Ÿæˆæ•°æ®å…·å¤‡å¼ºæ³›åŒ–è®­ç»ƒä»·å€¼ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡æœ€å¤§åˆ›æ–°åœ¨äºç›´å‡»å½“å‰é•¿ä¸Šä¸‹æ–‡ç ”ç©¶çš„ç›²åŒºâ€”â€”å°†â€œå•æ–‡æ¡£é•¿æ–‡æœ¬ç†è§£â€ä¸â€œå¤šæ–‡æ¡£è¯­æ–™çº§åˆ†æâ€æ˜ç¡®åŒºåˆ†ï¼Œå¹¶ä»¥å¯ç¼–ç¨‹çœŸå€¼ã€é«˜è¯æ®åˆ†æ•£ã€ç™¾ä¸‡çº§tokenè§„æ¨¡ä¸‰å¤§ç¡¬æŒ‡æ ‡å®šä¹‰æ–°èŒƒå¼ï¼›å…¶åˆæˆæ¡†æ¶ä¸ä»…è§£å†³è¯„ä¼°éš¾é¢˜ï¼Œæ›´å¼€è¾Ÿäº†æ— éœ€äººå·¥æ ‡æ³¨çš„é«˜è´¨é‡æ¨ç†æ•°æ®å·¥ä¸šåŒ–ç”Ÿäº§è·¯å¾„ï¼Œå¯¹æ¨åŠ¨å…¨å±€ä¿¡æ¯åˆæˆæ¶æ„å‘å±•å…·æœ‰å¥ åŸºæ„ä¹‰ã€‚\n    "
    },
    {
        "title": "Multi-Behavior Sequential Modeling with Transition-Aware Graph Attention Network for E-Commerce Recommendation",
        "authors": [
            "Hanqi Jin",
            "Gaoming Yang",
            "Zhangming Chan",
            "Yapeng Yuan",
            "Longbin Li",
            "Fei Sun",
            "Yeqiu Yang",
            "Jian Wu",
            "Yuning Jiang",
            "Bo Zheng"
        ],
        "categories": [
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14955v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14955v1",
        "summary": "User interactions on e-commerce platforms are inherently diverse, involving behaviors such as clicking, favoriting, adding to cart, and purchasing. The transitions between these behaviors offer valuable insights into user-item interactions, serving as a key signal for understanding evolving preferences. Consequently, there is growing interest in leveraging multi-behavior data to better capture user intent. Recent studies have explored sequential modeling of multi-behavior data, many relying on transformer-based architectures with polynomial time complexity. While effective, these approaches often incur high computational costs, limiting their applicability in large-scale industrial systems with long user sequences. To address this challenge, we propose the Transition-Aware Graph Attention Network (TGA), a linear-complexity approach for modeling multi-behavior transitions. Unlike traditional transformers that treat all behavior pairs equally, TGA constructs a structured sparse graph by identifying informative transitions from three perspectives: (a) item-level transitions, (b) category-level transitions, and (c) neighbor-level transitions. Built upon the structured graph, TGA employs a transition-aware graph Attention mechanism that jointly models user-item interactions and behavior transition types, enabling more accurate capture of sequential patterns while maintaining computational efficiency. Experiments show that TGA outperforms all state-of-the-art models while significantly reducing computational cost. Notably, TGA has been deployed in a large-scale industrial production environment, where it leads to impressive improvements in key business metrics.",
        "category": "æ¨¡å‹æ¶æ„",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14955v1ã€æ¨¡å‹æ¶æ„ã€‘Multi-Behavior Sequential Modeling with Transition-Aware Graph Attention Network for E-Commerce Recommendation.pdf",
        "institution": "é˜¿é‡Œã€ä¸­å›½ç§‘å­¦é™¢å¤§å­¦",
        "note": "ğŸ“–æ ‡é¢˜ï¼šMulti-Behavior Sequential Modeling with Transition-Aware Graph Attention Network for E-Commerce Recommendation\nğŸŒæ¥æºï¼šarXiv, 2601.14955v1\n\nç¬”è®°æ ‡é¢˜ï¼šé«˜æ•ˆå»ºæ¨¡å¤šè¡Œä¸ºè½¬åŒ–\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨å¤§è§„æ¨¡ç”µå•†åœºæ™¯ä¸­é«˜æ•ˆå»ºæ¨¡ç”¨æˆ·å¤šè¡Œä¸ºåºåˆ—å¹¶æ•æ‰è¡Œä¸ºé—´è½¬åŒ–æ¨¡å¼ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºTGAæ¨¡å‹ï¼Œé€šè¿‡ç»“æ„åŒ–ç¨€ç–å›¾ä¸è½¬åŒ–æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨çº¿æ€§å¤æ‚åº¦ä¸‹å®ç°é«˜ç²¾åº¦å¤šè¡Œä¸ºåºåˆ—æ¨èã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡ä¸‰ç±»æœ‰å‘è¾¹æ„å»ºç»“æ„åŒ–è¡Œä¸ºå›¾ï¼šitem-levelè¾¹åˆ»ç”»åŒä¸€ç‰©å“ä¸Šè¡Œä¸ºæ¼”è¿›ï¼ˆå¦‚ç‚¹å‡»â†’è´­ä¹°ï¼‰ï¼Œcategory-levelè¾¹å»ºæ¨¡åŒç±»ç‰©å“é—´æ¯”è¾ƒè¡Œä¸ºï¼Œneighbor-levelè¾¹ä¿ç•™æ—¶é—´é‚»æ¥å…³ç³»ã€‚  \nğŸ”¸å¼•å…¥è½¬åŒ–æ„ŸçŸ¥å›¾æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯¹ä¸åŒè¡Œä¸ºç±»å‹è½¬æ¢ï¼ˆå¦‚æ”¶è—â†’åŠ è´­ï¼‰é‡‡ç”¨å¯å­¦ä¹ çš„æƒé‡çŸ©é˜µå’Œåç½®è¿›è¡Œç‰¹å¾å˜æ¢ï¼Œå¹¶èåˆæ—¶é—´é—´éš”ä¸ä½ç½®å·®ä¿¡æ¯ã€‚  \nğŸ”¸é‡‡ç”¨å¤šå±‚TGAå †å ç»“æ„ï¼Œé€å±‚èšåˆé«˜é˜¶ä¾èµ–ï¼Œä½¿è¿œè·ç¦»è¡Œä¸ºäº¤äº’å¯é€šè¿‡å¤šè·³ä¼ æ’­æ•è·ã€‚  \nğŸ”¸æ•´ä½“æ¶æ„ä¿æŒçº¿æ€§å¤æ‚åº¦O(NÂ·LÂ·dÂ²)ï¼Œæ˜¾è‘—ä½äºTransformerçš„å¹³æ–¹å¤æ‚åº¦ï¼Œé€‚åˆé•¿åºåˆ—å·¥ä¸šéƒ¨ç½²ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨å…¬å¼€ä¸å·¥ä¸šæ•°æ®é›†ä¸Šï¼ŒTGAå‡å–å¾—æœ€ä¼˜AUCè¡¨ç°ï¼Œä¸”è®­ç»ƒæ¨ç†é€Ÿåº¦æ¯”Transformerå¿«5.8å€ä»¥ä¸Šã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¡¨æ˜ä¸‰ç±»è½¬ç§»è¾¹å‡æœ‰æ•ˆï¼Œç§»é™¤item-levelæˆ–category-levelè¾¹å¯¼è‡´æœ€å¤§æ€§èƒ½ä¸‹é™ï¼ˆ-0.0017/-0.0021ï¼‰ã€‚  \nğŸ”¸å±‚æ•°å¢åŠ æŒç»­æå‡æ€§èƒ½ï¼ŒéªŒè¯æ·±å±‚ç»“æ„å¯¹é«˜é˜¶è¡Œä¸ºä¾èµ–å»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€‚  \nğŸ”¸çº¿ä¸ŠA/Bæµ‹è¯•æ˜¾ç¤ºCVRæå‡1.29%ï¼ŒGMVå¢é•¿1.79%ï¼Œå·²æˆåŠŸè½åœ°äºæ·˜å®å¤§è§„æ¨¡ç³»ç»Ÿã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nåˆ›æ–°ç‚¹åœ¨äºå°†å¤šè¡Œä¸ºè½¬åŒ–è·¯å¾„æ˜¾å¼ç»“æ„åŒ–ä¸ºç¨€ç–å›¾ï¼Œå¹¶è®¾è®¡è¡Œä¸ºæ„ŸçŸ¥çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¼é¡¾å»ºæ¨¡ç²¾ç»†åº¦ä¸è®¡ç®—æ•ˆç‡ã€‚ç›¸æ¯”é»‘ç®±å¼å…¨è¿æ¥æ³¨æ„åŠ›ï¼ŒTGAæ›´å…·å¯è§£é‡Šæ€§ä¸”æ›´é€‚åˆçœŸå®ä¸šåŠ¡åœºæ™¯ï¼Œæ˜¯å·¥ä¸šçº§åºåˆ—æ¨èçš„é‡è¦è¿›å±•ã€‚\n    "
    },
    {
        "title": "HiNS: Hierarchical Negative Sampling for More Comprehensive Memory Retrieval Embedding Model",
        "authors": [
            "Motong Tian",
            "Allen P. Wong",
            "Mingjun Mao",
            "Wangchunshu Zhou"
        ],
        "categories": [
            "cs.CL"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14857v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14857v1",
        "summary": "Memory-augmented language agents rely on embedding models for effective memory retrieval. However, existing training data construction overlooks a critical limitation: the hierarchical difficulty of negative samples and their natural distribution in human-agent interactions. In practice, some negatives are semantically close distractors while others are trivially irrelevant, and natural dialogue exhibits structured proportions of these types. Current approaches using synthetic or uniformly sampled negatives fail to reflect this diversity, limiting embedding models' ability to learn nuanced discrimination essential for robust memory retrieval. In this work, we propose a principled data construction framework HiNS that explicitly models negative sample difficulty tiers and incorporates empirically grounded negative ratios derived from conversational data, enabling the training of embedding models with substantially improved retrieval fidelity and generalization in memory-intensive tasks. Experiments show significant improvements: on LoCoMo, F1/BLEU-1 gains of 3.27%/3.30%(MemoryOS) and 1.95%/1.78% (Mem0); on PERSONAMEM, total score improvements of 1.19% (MemoryOS) and 2.55% (Mem0).",
        "category": "Agent",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14857v1ã€Agentã€‘HiNS_ Hierarchical Negative Sampling for More Comprehensive Memory Retrieval Embedding Model.pdf",
        "institution": "OPPOã€æµ™æ±Ÿå¤§å­¦",
        "note": "ğŸ“–æ ‡é¢˜ï¼šHiNS: Hierarchical Negative Sampling for More Comprehensive Memory Retrieval Embedding Model\nğŸŒæ¥æºï¼šarXiv, 2601.14857v1\n\nç¬”è®°æ ‡é¢˜ï¼šåˆ†å±‚è´Ÿé‡‡æ ·æå‡è®°å¿†æ£€ç´¢\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•æ„å»ºæ›´ç¬¦åˆçœŸå®å¯¹è¯ä¸­è´Ÿæ ·æœ¬éš¾åº¦åˆ†å¸ƒä¸æ¯”ä¾‹çš„è®­ç»ƒæ•°æ®ï¼Œä»¥æå‡è®°å¿†å¢å¼ºå‹è¯­è¨€ä»£ç†çš„åµŒå…¥æ¨¡å‹æ£€ç´¢é²æ£’æ€§ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºHiNSæ¡†æ¶ï¼Œé¦–æ¬¡å°†è´Ÿæ ·æœ¬æŒ‰æ˜“/ä¸­/éš¾ä¸‰çº§åˆ†å±‚å»ºæ¨¡ï¼Œå¹¶åŸºäºçœŸå®å¯¹è¯æ•°æ®æ ¡å‡†å„ç±»è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼Œæ˜¾è‘—æå‡è®°å¿†æ£€ç´¢å‡†ç¡®ç‡ä¸æ³›åŒ–èƒ½åŠ›ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡ persona-grounded å¯¹è¯åˆæˆæµç¨‹ï¼Œä»å¤§è§„æ¨¡äººç‰©ç”»åƒåº“é‡‡æ ·è§’è‰²ï¼Œç”Ÿæˆäº‹ä»¶ã€è‡ªç„¶å¯¹è¯åŠè¯é¢˜èšç±»ï¼Œä¿éšœè®­ç»ƒæ•°æ®çš„çœŸå®æ€§ä¸å¤šæ ·æ€§ã€‚  \nğŸ”¸æå‡ºä¸‰çº§åˆ†å±‚è´Ÿé‡‡æ ·ç­–ç•¥ï¼šç¡¬è´Ÿæ ·æœ¬ï¼ˆåŒå¯¹è¯åŒè¯é¢˜ä¸åŒè¯´è¯äººï¼‰ã€ä¸­è´Ÿæ ·æœ¬ï¼ˆåŒå¯¹è¯ä¸åŒè¯é¢˜ï¼‰ã€æ˜“è´Ÿæ ·æœ¬ï¼ˆè·¨å¯¹è¯ï¼‰ï¼Œå¹¶ä¾æ®å®è¯åˆ†æè®¾å®š30%/30%/40%é‡‡æ ·æ¯”ã€‚  \nğŸ”¸å¼•å…¥è¯­ä¹‰æ„ŸçŸ¥æŸ¥è¯¢ç”Ÿæˆæœºåˆ¶ï¼Œç»“åˆäººç‰©ç®€ä»‹ä¸è¯é¢˜èšç±»ï¼Œç”Ÿæˆå…·èº«ä»½æŒ‡ä»£æ€§ã€è¯é¢˜æ˜ç¡®æ€§ä¸è¯æ®å¯æ ¡å‡†æ€§çš„é«˜è´¨é‡æŸ¥è¯¢-æ­£ä¾‹å¯¹ã€‚  \nğŸ”¸é‡‡ç”¨æ˜¾å¼åˆ†å±‚è´Ÿæ ·æœ¬è®­ç»ƒï¼ˆç¦ç”¨in-batchè´Ÿé‡‡æ ·ï¼‰ï¼Œé…åˆInfoNCEæŸå¤±ï¼Œç¡®ä¿æ‰€æœ‰è´Ÿæ ·æœ¬å‡ä¸ºçœŸå®æ— å…³é¡¹ï¼Œé¿å…é”™è¯¯ç›‘ç£ä¿¡å·ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨LoCoMoå’ŒPERSONAMEMä¸¤ä¸ªåŸºå‡†ä¸Šï¼ŒHiNSå¾®è°ƒæ¨¡å‹åœ¨MemoryOSå’ŒMem0æ¡†æ¶ä¸‹å‡å–å¾—ä¸€è‡´æå‡ï¼ŒF1/BLEU-1æœ€é«˜æå‡3.27%/3.30%ï¼ŒéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¡¨æ˜ï¼šä»…ç”¨ç¡¬è´Ÿæ ·æœ¬æ•ˆæœæœ‰é™ï¼›ç§»é™¤æ˜“è´Ÿæ ·æœ¬å¯¼è‡´Temporalä¸Single-hopä»»åŠ¡æ˜æ˜¾ä¸‹é™ï¼Œè¯´æ˜æ˜“æ ·æœ¬å¯¹æ£€ç´¢å¯¹é½å…·æœ‰åŸºç¡€ç¨³å®šä½œç”¨ã€‚  \nğŸ”¸è·¨æ¡†æ¶å®éªŒæ˜¾ç¤ºï¼ŒHiNSæå‡åœ¨è½»é‡çº§ï¼ˆMem0ï¼‰ä¸å…ˆè¿›ï¼ˆMemoryOSï¼‰è®°å¿†ç³»ç»Ÿä¸­å‡æˆç«‹ï¼Œè¯æ˜å…¶æ¶æ„æ— å…³æ€§ä¸å¼ºé€‚é…æ€§ã€‚  \nğŸ”¸åœ¨é€šç”¨åµŒå…¥åŸºå‡†ï¼ˆå¦‚TwentyNewsgroupsã€ArXivèšç±»ï¼‰ä¸Šäº¦æœ‰æå‡ï¼Œè¯´æ˜åˆ†å±‚è´Ÿé‡‡æ ·å¸¦æ¥çš„è¡¨å¾èƒ½åŠ›å¢å¼ºå…·å¤‡è·¨ä»»åŠ¡è¿ç§»æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºæ‰“ç ´â€œè´Ÿæ ·æœ¬å³å™ªå£°â€çš„ç®€åŒ–å‡è®¾ï¼Œå°†è®¤çŸ¥éš¾åº¦ä¸å¯¹è¯ç»Ÿè®¡è§„å¾‹å¼•å…¥æ•°æ®æ„é€ â€”â€”ä¸ä»…åŒºåˆ†éš¾æ˜“ï¼Œæ›´ç”¨çœŸå®æ¯”ä¾‹çº¦æŸè®­ç»ƒä¿¡å·ï¼Œä½¿å¯¹æ¯”å­¦ä¹ æ›´è´´è¿‘äººç±»è®°å¿†å¹²æ‰°æ¨¡å¼ã€‚è¯¥èŒƒå¼æœ‰æœ›æ¨å¹¿è‡³å…¶ä»–éœ€ç»†ç²’åº¦åˆ¤åˆ«çš„æ£€ç´¢åœºæ™¯ã€‚\n    "
    },
    {
        "title": "Towards Holistic Modeling for Video Frame Interpolation with Auto-regressive Diffusion Transformers",
        "authors": [
            "Xinyu Peng",
            "Han Li",
            "Yuyang Huang",
            "Ziyang Zheng",
            "Yaoming Wang",
            "Xin Chen",
            "Wenrui Dai",
            "Chenglin Li",
            "Junni Zou",
            "Hongkai Xiong"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14959v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14959v1",
        "summary": "Existing video frame interpolation (VFI) methods often adopt a frame-centric approach, processing videos as independent short segments (e.g., triplets), which leads to temporal inconsistencies and motion artifacts. To overcome this, we propose a holistic, video-centric paradigm named \\textbf{L}ocal \\textbf{D}iffusion \\textbf{F}orcing for \\textbf{V}ideo \\textbf{F}rame \\textbf{I}nterpolation (LDF-VFI). Our framework is built upon an auto-regressive diffusion transformer that models the entire video sequence to ensure long-range temporal coherence. To mitigate error accumulation inherent in auto-regressive generation, we introduce a novel skip-concatenate sampling strategy that effectively maintains temporal stability. Furthermore, LDF-VFI incorporates sparse, local attention and tiled VAE encoding, a combination that not only enables efficient processing of long sequences but also allows generalization to arbitrary spatial resolutions (e.g., 4K) at inference without retraining. An enhanced conditional VAE decoder, which leverages multi-scale features from the input video, further improves reconstruction fidelity. Empirically, LDF-VFI achieves state-of-the-art performance on challenging long-sequence benchmarks, demonstrating superior per-frame quality and temporal consistency, especially in scenes with large motion. The source code is available at https://github.com/xypeng9903/LDF-VFI.",
        "category": "æ¨¡å‹æ¶æ„",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14959v1ã€æ¨¡å‹æ¶æ„ã€‘Towards Holistic Modeling for Video Frame Interpolation with Auto-regressive Diffusion Transformers.pdf",
        "institution": "ä¸Šæµ·äº¤é€šå¤§å­¦ã€ç¾å›¢",
        "note": "ğŸ“–æ ‡é¢˜ï¼šTowards Holistic Modeling for Video Frame Interpolation with Auto-regressive Diffusion Transformers\nğŸŒæ¥æºï¼šarXiv, 2601.14959v1\n\nç¬”è®°æ ‡é¢˜ï¼šè§†é¢‘çº§æ‰©æ•£å»ºæ¨¡æ’å¸§  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨é•¿è§†é¢‘å¸§æ’å€¼ä¸­åŒæ—¶ä¿è¯é•¿ç¨‹æ—¶é—´ä¸€è‡´æ€§ä¸é«˜åˆ†è¾¨ç‡å¯æ‰©å±•æ€§ï¼Œé¿å…ä¼ ç»Ÿå¸§å¯¹æ–¹æ³•å¯¼è‡´çš„è¿åŠ¨ä¼ªå½±å’Œè¯¯å·®ç´¯ç§¯ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºé¦–ä¸ªè§†é¢‘ä¸­å¿ƒã€è‡ªå›å½’æ‰©æ•£Transformeræ¡†æ¶LDF-VFIï¼Œé€šè¿‡æ•´ä½“åºåˆ—å»ºæ¨¡ã€è·³æ¥æ‹¼æ¥é‡‡æ ·ä¸ç¨€ç–å±€éƒ¨æ³¨æ„åŠ›ï¼Œå®ç°é«˜è´¨é‡ã€é«˜ä¸€è‡´æ€§ã€4Kæ— ç¼æ³›åŒ–çš„å¸§æ’å€¼ã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸é‡‡ç”¨è§†é¢‘ä¸­å¿ƒèŒƒå¼ï¼Œå°†æ•´æ®µä½å¸§ç‡è§†é¢‘ä½œä¸ºæ¡ä»¶ï¼Œè”åˆå»ºæ¨¡å…¨éƒ¨é«˜å¸§ç‡ç›®æ ‡å¸§çš„éšç©ºé—´åˆ†å¸ƒï¼Œè€Œéé€å¯¹ç”Ÿæˆä¸­é—´å¸§ã€‚  \nğŸ”¸è®¾è®¡è‡ªå›å½’æ‰©æ•£æ¨ç†æµç¨‹ï¼šå°†è§†é¢‘åˆ‡åˆ†ä¸ºå›ºå®šé•¿åº¦æ—¶åºå—ï¼Œä»¥å—ä¸ºå•ä½è¿›è¡Œæ‰©æ•£ç”Ÿæˆï¼Œå¹¶é€šè¿‡â€œè·³å—ï¼ˆskipï¼‰â€”æ‹¼å—ï¼ˆconcatenateï¼‰â€äº¤æ›¿é‡‡æ ·ç­–ç•¥é‡ç½®è¯¯å·®ä¼ æ’­é“¾ã€‚  \nğŸ”¸å¼•å…¥ç¨€ç–æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶â€”â€”ç©ºé—´ä¸Šé‡‡ç”¨åˆ†å—æ»‘åŠ¨çª—å£ï¼ˆå±€éƒ¨ï¼‰ï¼Œæ—¶é—´ä¸Šä¿æŒå…¨è¿æ¥ï¼ˆç¨ å¯†ï¼‰ï¼Œå…¼é¡¾æ•ˆç‡ä¸è¿åŠ¨å»ºæ¨¡èƒ½åŠ›ã€‚  \nğŸ”¸ç»“åˆåˆ†å—VAEç¼–ç ä¸æœ€è¿‘é‚»æ—¶åºä¸Šé‡‡æ ·+äºŒå€¼æ©ç ï¼Œè§£è€¦è®¡ç®—å¤æ‚åº¦ä¸å›¾åƒåˆ†è¾¨ç‡ï¼Œæ”¯æŒè®­ç»ƒäº1080pã€æ¨ç†äº4Kæ— éœ€é‡è®­ã€‚  \nğŸ”¸æ„å»ºæ¡ä»¶åŒ–VAEè§£ç å™¨ï¼Œé€šè¿‡å¤šå°ºåº¦è¾“å…¥è§†é¢‘ç‰¹å¾æ³¨å…¥ï¼Œå¢å¼ºé‡å»ºç»†èŠ‚ä¿çœŸåº¦ä¸æ—¶ç©ºè¿è´¯æ€§ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨SNU-FILM-entireä¸XTest-entireç­‰è§†é¢‘çº§é•¿åºåˆ—åŸºå‡†ä¸Šï¼ŒLDF-VFIæ˜¾è‘—ä¼˜äºRIFEã€AMTç­‰SOTAæ–¹æ³•ï¼ŒFVDä¸‹é™æœ€é«˜è¾¾27.2%ï¼ŒéªŒè¯å…¶æ—¶é—´ä¸€è‡´æ€§ä¼˜åŠ¿ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šè·³æ¥æ‹¼æ¥é‡‡æ ·ä½¿FVDé™ä½24.8%ï¼Œæ˜¯ç¼“è§£è‡ªå›å½’è¯¯å·®ç´¯ç§¯æœ€æœ‰æ•ˆæ‰‹æ®µï¼›æ¡ä»¶VAEè¿›ä¸€æ­¥å°†LPIPSé™ä½17.8%ï¼Œæå‡å•å¸§è´¨é‡ã€‚  \nğŸ”¸æœ€è¿‘é‚»ä¸Šé‡‡æ ·æ¯”é›¶å¡«å……æ›´ä¼˜ï¼ˆPSNRâ†‘2.47ï¼ŒLPIPSâ†“45%ï¼‰ï¼Œå› å…¶ä¿ç•™åŸå§‹å¸§ç»Ÿè®¡ç‰¹æ€§ï¼Œé¿å…VAEç¼–ç å™¨è¢«äººå·¥è¾¹ç•Œå¹²æ‰°ã€‚  \nğŸ”¸ç¨€ç–æ³¨æ„åŠ›ä¸åˆ†å—VAEä½¿æ¨¡å‹å†…å­˜æ’å®šã€æ”¯æŒUlyssesåºåˆ—å¹¶è¡Œï¼Œå®ç°åœ¨åŒå¡80GB GPUä¸Šå®Œæˆ4Kè§†é¢‘æ’å€¼æ¨ç†ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºä¸‰é‡ååŒçªç ´ï¼šé¦–æ¬¡å°†è‡ªå›å½’æ€æƒ³ç³»ç»Ÿå¼•å…¥VFIä»»åŠ¡å¹¶é’ˆå¯¹æ€§è®¾è®¡è·³æ¥æ‹¼æ¥æœºåˆ¶è§£å†³æ›å…‰åå·®ï¼›å°†æ‰©æ•£æ¨¡å‹ä»å¸§å¯¹çº§å‡ç»´è‡³è§†é¢‘å—çº§å»ºæ¨¡ï¼ŒçœŸæ­£å®ç°â€œholisticâ€æ—¶é—´ç†è§£ï¼›é€šè¿‡ç¨€ç–æ³¨æ„åŠ›+åˆ†å—VAE+æ¡ä»¶è§£ç å™¨çš„è½»é‡ç»„åˆï¼Œåœ¨ä¸ç‰ºç‰²4Kæ³›åŒ–èƒ½åŠ›å‰æä¸‹è¾¾æˆè®¡ç®—å¯è¡Œæ€§ï¼Œä¸ºé•¿è§†é¢‘ç”Ÿæˆæä¾›äº†å¯è½åœ°çš„æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "PodBench: A Comprehensive Benchmark for Instruction-Aware Audio-Oriented Podcast Script Generation",
        "authors": [
            "Chenning Xu",
            "Mao Zheng",
            "Mingyu Zheng",
            "Mingyang Song"
        ],
        "categories": [
            "cs.CL"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14903v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14903v1",
        "summary": "Podcast script generation requires LLMs to synthesize structured, context-grounded dialogue from diverse inputs, yet systematic evaluation resources for this task remain limited. To bridge this gap, we introduce PodBench, a benchmark comprising 800 samples with inputs up to 21K tokens and complex multi-speaker instructions. We propose a multifaceted evaluation framework that integrates quantitative constraints with LLM-based quality assessment. Extensive experiments reveal that while proprietary models generally excel, open-source models equipped with explicit reasoning demonstrate superior robustness in handling long contexts and multi-speaker coordination compared to standard baselines. However, our analysis uncovers a persistent divergence where high instruction following does not guarantee high content substance. PodBench offers a reproducible testbed to address these challenges in long-form, audio-centric generation.",
        "category": "PodBenchä»»åŠ¡è¯„æµ‹",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14903v1ã€PodBenchä»»åŠ¡è¯„æµ‹ã€‘PodBench_ A Comprehensive Benchmark for Instruction-Aware Audio-Oriented Podcast Script Generation.pdf",
        "institution": "Tencent",
        "note": "ğŸ“–æ ‡é¢˜ï¼šPodBench: A Comprehensive Benchmark for Instruction-Aware Audio-Oriented Podcast Script Generation\nğŸŒæ¥æºï¼šarXiv, 2601.14903v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºæ’­å®¢è„šæœ¬ç”Ÿæˆæ–°åŸºå‡†  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ç³»ç»Ÿè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨æŒ‡ä»¤æ„ŸçŸ¥ã€ä¸Šä¸‹æ–‡æ‰æ ¹çš„éŸ³é¢‘å¯¼å‘å‹æ’­å®¢è„šæœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„çœŸå®èƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºäº†é¦–ä¸ªé¢å‘æ’­å®¢åœºæ™¯çš„ç»¼åˆæ€§åŸºå‡†PodBenchï¼ŒåŒ…å«800ä¸ªé«˜å¤æ‚åº¦æ ·æœ¬ä¸åŒé˜¶æ®µç»†ç²’åº¦è¯„ä¼°æ¡†æ¶ï¼Œå¡«è¡¥äº†éŸ³é¢‘ä¼˜å…ˆç”Ÿæˆä»»åŠ¡çš„è¯„æµ‹ç©ºç™½ã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºä¸Šä¸‹æ–‡æ‰æ ¹çš„æ•°æ®é›†ï¼šä»12ä¸ªé¢†åŸŸé‡‡é›†å•/å¤šæ–‡æ¡£è¾“å…¥ææ–™ï¼ˆæœ€é•¿21K tokensï¼‰ï¼Œè¦†ç›–ç¤¾ä¼šæ–‡åŒ–ã€é‡‘èã€æ•™è‚²ç­‰çœŸå®AI-Podcaståº”ç”¨åœºæ™¯ã€‚  \nğŸ”¸è®¾è®¡æŒ‡ä»¤æ„ŸçŸ¥çš„åˆæˆèŒƒå¼ï¼šåŸºäº8ç±»ç”¨æˆ·éœ€æ±‚ç»´åº¦ï¼ˆå¦‚èšç„¦å†…å®¹ã€ speakeræ¡£æ¡ˆã€ç»“æ„é€»è¾‘ã€è¯­è¨€é£æ ¼ç­‰ï¼‰ï¼Œåˆ©ç”¨Gemini-2.5-proç”Ÿæˆå¤šæ ·åŒ–ã€å¤šçº¦æŸçš„è‡ªç„¶æŒ‡ä»¤ã€‚  \nğŸ”¸æå‡ºè§£è€¦å¼åŒé˜¶æ®µè¯„ä¼°æ¡†æ¶ï¼šç¬¬ä¸€é˜¶æ®µåŠ¨æ€æ£€æŸ¥æŒ‡ä»¤éµå¾ªï¼ˆæ˜¾æ€§+éšæ€§è¦æ±‚ï¼‰ï¼Œç¬¬äºŒé˜¶æ®µé‡‡ç”¨éŸ³é¢‘ä¼˜å…ˆçš„ä¸‰ç»´åº¦äººå·¥å®šä¹‰é‡è§„ï¼ˆå†…å®¹æ·±åº¦45åˆ†ã€å™äº‹å¼ åŠ›30åˆ†ã€å£è¯­è‡ªç„¶åº¦25åˆ†ï¼‰ã€‚  \nğŸ”¸å¼•å…¥LLM-as-a-judgeè‡ªåŠ¨åŒ–è¯„åˆ†ï¼šä½¿ç”¨Claude-4.5-Opusæ‰§è¡Œè¯æ®é©±åŠ¨çš„é€é¡¹å®¡è®¡ä¸ç»†ç²’åº¦æ‰“åˆ†ï¼Œå…¼é¡¾å¯æ‰©å±•æ€§ä¸ä¸“ä¸šæ€§ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸æŒ‡ä»¤éµå¾ªé«˜ä¸ç­‰äºå†…å®¹è´¨é‡é«˜ï¼šå®éªŒå‘ç°å¤šæ•°æ¨¡å‹åœ¨Stage 1å¾—åˆ†ä¼˜å¼‚ï¼Œä½†Stage 2ä¸­â€œå†…å®¹æ·±åº¦â€ç»´åº¦æ™®éè–„å¼±ï¼Œæš´éœ²èƒ½åŠ›æ–­å±‚ã€‚  \nğŸ”¸é•¿ä¸Šä¸‹æ–‡ä¸å¤šè¯´è¯äººæ„æˆåŒé‡æŒ‘æˆ˜ï¼šè¾“å…¥è¶…16K tokensæ—¶å¼€æºæ¨¡å‹æŒ‡ä»¤éµå¾ªç‡æ˜¾è‘—ä¸‹é™ï¼›3â€“4 speakeråœºæ™¯ä¸‹è§’è‰²åè°ƒèƒ½åŠ›æ˜æ˜¾åŠ£äºå•/åŒspeakerï¼Œå½¢æˆâ€œåè°ƒç¨â€ã€‚  \nğŸ”¸æ˜¾å¼æ¨ç†æ¨¡å¼æå‡é²æ£’æ€§ï¼šQwen3ç³»åˆ—å¯ç”¨thinking modeåï¼Œä¸­ç­‰è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚8B/14Bï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡ä¸å¤šspeakerä»»åŠ¡ä¸­æå‡æœ€æ˜¾è‘—ï¼ŒéªŒè¯æ¨ç†å¯¹ç»“æ„æ§åˆ¶çš„ä»·å€¼ã€‚  \nğŸ”¸é¢†åŸŸçŸ¥è¯†å¢å¼ºè¯„ä¼°ä¿¡åº¦ï¼šå¯¹æ¯”å®éªŒè¯æ˜ï¼ŒPodBenchçš„é¢†åŸŸå®šåˆ¶åŒ–é‡è§„+åŠ æƒæœºåˆ¶æ¯”é€šç”¨æ¸…å•å¼è¯„ä¼°ä¸ç­‰æƒé‡é‡è§„æ›´è´´è¿‘äººç±»ä¸“å®¶åˆ¤æ–­ï¼ˆå‡†ç¡®ç‡86.8% vs. 79.6%/83.2%ï¼‰ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºå°†æ’­å®¢è¿™ä¸€éŸ³é¢‘åŸç”Ÿä»»åŠ¡ä»æ³›å†™ä½œè¯„æµ‹ä¸­å‰¥ç¦»ï¼Œä»¥â€œæŒ‡ä»¤â€”ä¸Šä¸‹æ–‡â€”éŸ³é¢‘é€‚é…â€ä¸‰ç»´é”šå®šä»»åŠ¡æœ¬è´¨ï¼›å…¶åŒé˜¶æ®µè¯„ä¼°è®¾è®¡é¦–æ¬¡åˆ†ç¦»â€œåˆè§„æ€§â€ä¸â€œå¬ä¼—ä»·å€¼â€ï¼Œç›´å‡»å½“å‰LLMç”Ÿæˆé‡å½¢å¼è½»å®è´¨çš„ç—›ç‚¹ï¼›åŒæ—¶é€šè¿‡é•¿æ–‡æœ¬ã€å¤šè§’è‰²ã€è·¨è¯­è¨€ç­‰ç¡¬æ€§æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥éŸ³é¢‘å¯¹è¯AIæä¾›äº†å¯å¤ç°ã€å¯å½’å› çš„è¯Šæ–­æ€§æµ‹è¯•åºŠã€‚\n    "
    },
    {
        "title": "What Should I Cite? A RAG Benchmark for Academic Citation Prediction",
        "authors": [
            "Leqi Zheng",
            "Jiajun Zhang",
            "Canzhi Chen",
            "Chaokun Wang",
            "Hongwei Li",
            "Yuying Li",
            "Yaoxin Mao",
            "Shannan Yan",
            "Zixin Song",
            "Zhiyuan Feng",
            "Zhaolu Kang",
            "Zirong Chen",
            "Hang Zhang",
            "Qiang Liu",
            "Liang Wang",
            "Ziyang Liu"
        ],
        "categories": [
            "cs.IR"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14949v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14949v1",
        "summary": "With the rapid growth of Web-based academic publications, more and more papers are being published annually, making it increasingly difficult to find relevant prior work. Citation prediction aims to automatically suggest appropriate references, helping scholars navigate the expanding scientific literature. Here we present \\textbf{CiteRAG}, the first comprehensive retrieval-augmented generation (RAG)-integrated benchmark for evaluating large language models on academic citation prediction, featuring a multi-level retrieval strategy, specialized retrievers, and generators. Our benchmark makes four core contributions: (1) We establish two instances of the citation prediction task with different granularity. Task 1 focuses on coarse-grained list-specific citation prediction, while Task 2 targets fine-grained position-specific citation prediction. To enhance these two tasks, we build a dataset containing 7,267 instances for Task 1 and 8,541 instances for Task 2, enabling comprehensive evaluation of both retrieval and generation. (2) We construct a three-level large-scale corpus with 554k papers spanning many major subfields, using an incremental pipeline. (3) We propose a multi-level hybrid RAG approach for citation prediction, fine-tuning embedding models with contrastive learning to capture complex citation relationships, paired with specialized generation models. (4) We conduct extensive experiments across state-of-the-art language models, including closed-source APIs, open-source models, and our fine-tuned generators, demonstrating the effectiveness of our framework. Our open-source toolkit enables reproducible evaluation and focuses on academic literature, providing the first comprehensive evaluation framework for citation prediction and serving as a methodological template for other scientific domains. Our source code and data are released at https://github.com/LQgdwind/CiteRAG.",
        "category": "RAGä»»åŠ¡è¯„æµ‹",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14949v1ã€RAGä»»åŠ¡è¯„æµ‹ã€‘What Should I Cite_ A RAG Benchmark for Academic Citation Prediction.pdf",
        "institution": "æ¸…åå¤§å­¦ã€ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦ã€åŒ—äº¬ç†å·¥å¤§å­¦ã€åŒ—äº¬å¤§å­¦ã€ä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€",
        "note": "ğŸ“–æ ‡é¢˜ï¼šWhat Should I Cite? A RAG Benchmark for Academic Citation Prediction\nğŸŒæ¥æºï¼šarXiv, 2601.14949v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºé¦–ä¸ªå­¦æœ¯å¼•ç”¨RAGåŸºå‡†  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ç³»ç»Ÿè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨å­¦æœ¯å¼•ç”¨é¢„æµ‹ä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºäº†é¦–ä¸ªé¢å‘å­¦æœ¯å¼•ç”¨é¢„æµ‹çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç»¼åˆåŸºå‡†CiteRAGï¼Œæ¶µç›–åŒç²’åº¦ä»»åŠ¡ã€å¤šå±‚çº§è¯­æ–™åº“ã€ä¸“ç”¨RAGæ¡†æ¶ä¸å¼€æºè¯„æµ‹å·¥å…·é“¾ã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å®šä¹‰ä¸¤ä¸ªäº’è¡¥ä»»åŠ¡ï¼šTask 1ä¸ºç²—ç²’åº¦â€œåˆ—è¡¨çº§å¼•ç”¨é¢„æµ‹â€ï¼ˆç”Ÿæˆæ•´ç¯‡è®ºæ–‡å‚è€ƒæ–‡çŒ®åˆ—è¡¨ï¼‰ï¼ŒTask 2ä¸ºç»†ç²’åº¦â€œä½ç½®çº§å¼•ç”¨é¢„æµ‹â€ï¼ˆé’ˆå¯¹æ–‡ä¸­æ¯ä¸ª[ref]å ä½ç¬¦ç²¾å‡†æ¨èå¯¹åº”æ–‡çŒ®ï¼‰ã€‚  \nğŸ”¸æ„å»º55.4ä¸‡ç¯‡è·¨å­¦ç§‘ã€ä¸‰å±‚çº§ï¼ˆæ ‡é¢˜/æ‘˜è¦â†’å¼•è¨€â†’å»å¼•å…¨æ–‡+ç»“è®ºï¼‰æ¸è¿›å¼å­¦æœ¯è¯­æ–™åº“ï¼Œå¹¶ä¸¥æ ¼å‰”é™¤æµ‹è¯•æ ·æœ¬ä»¥é˜²æ•°æ®æ±¡æŸ“ã€‚  \nğŸ”¸è®¾è®¡å¤šå±‚çº§æ··åˆRAGæ¡†æ¶ï¼šé‡‡ç”¨å¯¹æ¯”å­¦ä¹ å¾®è°ƒQwen3-Embedding-8Bä¸ºCitationRetriever-8Bï¼Œæ”¯æŒä¸‰çº§å¹¶è¡Œæ£€ç´¢ä¸å€’æ’ç§©èåˆï¼›æ­é…ä»»åŠ¡å®šåˆ¶åŒ–ç”Ÿæˆå™¨ï¼ˆå¦‚CitationGenerator-30Bï¼‰ã€‚  \nğŸ”¸å»ºç«‹æ ‡å‡†åŒ–è¯„æµ‹ä½“ç³»ï¼šå¼•å…¥æ–°æŒ‡æ ‡PACA@kè¡¡é‡ä½ç½®é¢„æµ‹è´¨é‡ï¼Œè”åˆCitation Diversity Entropyä¸Hallucination Rateè¯„ä¼°ç”Ÿæˆå¤šæ ·æ€§ä¸äº‹å®æ€§ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸RAGæ˜¾è‘—æå‡æ‰€æœ‰æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶Task 2ä¸­PACA@20å¹³å‡æå‡è¶…70%ï¼ŒGemini-2.0-Flashè¾¾282%ï¼›æ£€ç´¢æ·±åº¦ä»R=5å¢è‡³R=10å¸¦æ¥æŒç»­å¢ç›Šï¼Œä½†è¿‡æ·±ï¼ˆ>R=15ï¼‰å¼•å…¥å™ªå£°å¯¼è‡´ä¸‹é™ã€‚  \nğŸ”¸å¯¹æ¯”å­¦ä¹ å¾®è°ƒä½¿CitationRetriever-8Båœ¨MRR@50ä¸Šè¾ƒåŸºçº¿æå‡47.9%ï¼ŒéªŒè¯å…¶å¯¹æ–¹æ³•å…±äº«ã€é¢†åŸŸæƒ¯ä¾‹ç­‰å¤æ‚å¼•ç”¨å…³ç³»çš„å»ºæ¨¡èƒ½åŠ›ã€‚  \nğŸ”¸å¤šå±‚çº§èåˆæ£€ç´¢å§‹ç»ˆä¼˜äºå•å±‚æ£€ç´¢ï¼Œä¸”CitationRetriever-8Bè·æœ€å¤§å¢ç›Šï¼ˆ+3.7% MRR@50ï¼‰ï¼Œè¯´æ˜å±‚çº§ä¿¡æ¯äº’è¡¥ï¼šLevel 1æ•è·ä¸»é¢˜åŒ¹é…ï¼ŒLevel 2èå…¥æ–¹æ³•ä¸Šä¸‹æ–‡ï¼ŒLevel 3è¦†ç›–å†…å®¹ç»†èŠ‚ã€‚  \nğŸ”¸ç›‘ç£å¾®è°ƒæ˜¯æ€§èƒ½è·ƒå‡ä¸»å› ï¼ŒCitationGenerator-30Bæ— RAGæ—¶å·²åª²ç¾é—­æºæ¨¡å‹ï¼›å¾®è°ƒ+RAGç»„åˆå®ç°æœ€ä¼˜æ•ˆæœï¼ˆTask 2 PACA@20è¾¾0.303ï¼‰ï¼Œä¸”å¤§å¹…é™ä½å¹»è§‰ç‡ï¼ˆ17.4%â†’4.9%ï¼‰ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ¨äºå°†RAGèŒƒå¼æ·±åº¦é€‚é…å­¦æœ¯å¼•ç”¨åœºæ™¯ï¼šé¦–æ¬¡è§£è€¦â€œåˆ—è¡¨â€ä¸â€œä½ç½®â€åŒä»»åŠ¡ä»¥è´´åˆçœŸå®å†™ä½œæµç¨‹ï¼›æå‡ºå±‚çº§åŒ–è¯­æ–™ç»„ç»‡ä¸æ£€ç´¢èåˆæœºåˆ¶ï¼Œçªç ´ä¼ ç»Ÿæ‰å¹³åŒ–æ–‡æœ¬å¤„ç†å±€é™ï¼›æ„å»ºå…¼é¡¾å‡†ç¡®æ€§ã€å¤šæ ·æ€§ä¸å¯é æ€§çš„å¤šç»´è¯„æµ‹ä½“ç³»ï¼Œä¸ºç§‘å­¦çŸ¥è¯†å›¾è°±æ„å»ºä¸AIè¾…åŠ©ç§‘ç ”æä¾›äº†å¯å¤ç°ã€å¯æ‰©å±•çš„æ–¹æ³•è®ºæ¨¡æ¿ã€‚\n    "
    },
    {
        "title": "Improving Regret Approximation for Unsupervised Dynamic Environment Generation",
        "authors": [
            "Harry Mead",
            "Bruno Lacerda",
            "Jakob Foerster",
            "Nick Hawes"
        ],
        "categories": [
            "cs.LG"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14957v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14957v1",
        "summary": "Unsupervised Environment Design (UED) seeks to automatically generate training curricula for reinforcement learning (RL) agents, with the goal of improving generalisation and zero-shot performance. However, designing effective curricula remains a difficult problem, particularly in settings where small subsets of environment parameterisations result in significant increases in the complexity of the required policy. Current methods struggle with a difficult credit assignment problem and rely on regret approximations that fail to identify challenging levels, both of which are compounded as the size of the environment grows. We propose Dynamic Environment Generation for UED (DEGen) to enable a denser level generator reward signal, reducing the difficulty of credit assignment and allowing for UED to scale to larger environment sizes. We also introduce a new regret approximation, Maximised Negative Advantage (MNA), as a significantly improved metric to optimise for, that better identifies more challenging levels. We show empirically that MNA outperforms current regret approximations and when combined with DEGen, consistently outperforms existing methods, especially as the size of the environment grows. We have made all our code available here: https://github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED.",
        "category": "å¼ºåŒ–å­¦ä¹ ",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14957v1ã€å¼ºåŒ–å­¦ä¹ ã€‘Improving Regret Approximation for Unsupervised Dynamic Environment Generation.pdf",
        "institution": "University of Oxford",
        "note": "ğŸ“–æ ‡é¢˜ï¼šImproving Regret Approximation for Unsupervised Dynamic Environment Generation\nğŸŒæ¥æºï¼šarXiv, 2601.14957v1\n\nç¬”è®°æ ‡é¢˜ï¼šåŠ¨æ€ç”Ÿæˆæå‡åæ‚”è¿‘ä¼¼  \n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨æ— ç›‘ç£ç¯å¢ƒè®¾è®¡ä¸­æ›´å‡†ç¡®åœ°è¿‘ä¼¼å­¦ç”Ÿç­–ç•¥çš„åæ‚”å€¼ï¼Œå¹¶æœ‰æ•ˆæ‰©å±•è‡³å¤§è§„æ¨¡ã€éå¹³æ»‘éš¾åº¦å˜åŒ–çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºåŠ¨æ€ç¯å¢ƒç”Ÿæˆï¼ˆDEGenï¼‰æ–¹æ³•ä¸æœ€å¤§åŒ–è´Ÿä¼˜åŠ¿ï¼ˆMNAï¼‰åæ‚”è¿‘ä¼¼æŒ‡æ ‡ï¼Œæ˜¾è‘—æå‡å¤§è§„æ¨¡å¤æ‚ç¯å¢ƒä¸‹çš„é›¶æ ·æœ¬æ³›åŒ–æ€§èƒ½ã€‚  \n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸DEGené‡‡ç”¨å­¦ç”Ÿæ¢ç´¢é©±åŠ¨çš„æ¸è¿›å¼ç¯å¢ƒç”Ÿæˆæœºåˆ¶ï¼Œä»…åœ¨å­¦ç”Ÿè§‚æµ‹åŒºåŸŸå†…å®æ—¶ç”Ÿæˆå…³å¡ç‰‡æ®µï¼Œå°†ç¨€ç–çš„å…¨å±€å¥–åŠ±è½¬åŒ–ä¸ºå¯†é›†çš„å±€éƒ¨æ•™å¸ˆå¥–åŠ±ä¿¡å·ã€‚  \nğŸ”¸é€šè¿‡æ—¶é—´æ­¥æ˜ å°„å‡½æ•°å°†å­¦ç”Ÿè½¨è¿¹ä¸Šçš„åæ‚”è¿‘ä¼¼å€¼åˆ†é…è‡³å¯¹åº”ç”Ÿæˆæ­¥ï¼Œç¼“è§£é•¿æ—¶ç¨‹ä¿¡ç”¨åˆ†é…éš¾é¢˜ï¼Œå¹¶é¿å…æœªè§‚æµ‹åŒºåŸŸå¸¦æ¥çš„å™ªå£°å¹²æ‰°ã€‚  \nğŸ”¸MNAåŸºäºå¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡æ¡†æ¶ï¼Œæ„é€ ä¸‹ç•ŒåŒ–çš„åæ‚”è¿‘ä¼¼ï¼šç”¨è´Ÿnæ­¥ä¼˜åŠ¿åŠ æƒå¹³å‡æ›¿ä»£ä¼ ç»Ÿæ­£å‘ä¼˜åŠ¿æˆ–æœ€å¤§å›æŠ¥ï¼Œæ›´ç²¾å‡†è¯†åˆ«â€œæ¯”é¢„æœŸæ›´éš¾â€çš„æŒ‘æˆ˜æ€§å…³å¡ã€‚  \nğŸ”¸å¼•å…¥æ˜¾å¼ä¸å¯è§£æ€§æƒ©ç½šæœºåˆ¶â€”â€”è‹¥å…³å¡åœ¨å¤šæ¬¡å°è¯•ä¸­ä»æœªè¢«è§£å†³ï¼Œåˆ™ç½®é›¶å…¶å¾—åˆ†ï¼Œé˜²æ­¢MNAå› ä»·å€¼å‡½æ•°è¿‡ä¼°è®¡è€Œè¯¯é€‰ä¸å¯è§£å…³å¡ã€‚  \nğŸ”¸åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ³¨å…¥å­¦ç”Ÿä½ç½®éšæœºæ€§ä¸é«˜ç†µæ­£åˆ™åŒ–ï¼Œå¢å¼ºå…³å¡å¤šæ ·æ€§ï¼Œé¿å…ç”Ÿæˆå™¨é™·å…¥é‡å¤æˆ–é€€åŒ–ç»“æ„ã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸MNAåœ¨æ‰€æœ‰å®éªŒä¸­å‡è¶…è¶ŠPVLå’ŒMaxMCï¼šåœ¨Key-MiniGridä¸Šé›¶æ ·æœ¬æ±‚è§£ç‡æå‡15â€“30%ï¼Œå°¤å…¶åœ¨17Ã—17/21Ã—21å¤§å°ºå¯¸ç¯å¢ƒä¸­ä¼˜åŠ¿æ‰©å¤§ã€‚  \nğŸ”¸DEGenåœ¨å°ç¯å¢ƒï¼ˆ13Ã—13ï¼‰ä¸­æ€§èƒ½åª²ç¾é‡æ”¾ç¼“å†²åŸºçº¿ï¼ˆPLR/ACCELï¼‰ï¼Œè€Œåœ¨å¤§ç¯å¢ƒï¼ˆ21Ã—21ï¼‰ä¸­æ±‚è§£ç‡è¾¾80%ï¼Œè¿œè¶…PLR-MNAï¼ˆ43%ï¼‰å’ŒACCEL-MNAï¼ˆ31%ï¼‰ã€‚  \nğŸ”¸ç°æœ‰å…¨å›¾ç”Ÿæˆå™¨ï¼ˆå¦‚Initial Genï¼‰å› ä¿¡ç”¨åˆ†é…å¤±è´¥ä¸å¤šæ ·æ€§ä¸è¶³ï¼Œåœ¨Key-MiniGridä¸­å¹³å‡æ±‚è§£ç‡ä»…8%ï¼Œè€ŒDEGenè¾¾93%ã€‚  \nğŸ”¸MNAå¯¹é‡æ”¾ç¼“å†²ç±»æ–¹æ³•åŒæ ·æœ‰æ•ˆï¼šPLR-MNAåœ¨Key-MiniGridä¸­æ€§èƒ½åè¶…PLR-PVLè¾¾80ä¸ªç™¾åˆ†ç‚¹ï¼ŒéªŒè¯å…¶æ™®é€‚æ€§ã€‚  \nğŸ”¸åœ¨Sokobanç­‰é«˜ä¸å¯è§£ç‡ç¯å¢ƒä¸­ï¼ŒMNAè¡¨ç°å¼±äºMaxMCï¼Œè¯´æ˜å…¶ä¼˜åŠ¿ä¾èµ–äºâ€œå›°éš¾ä½†å¯è§£â€å…³å¡å æ¯”è¾ƒé«˜çš„åœºæ™¯å‡è®¾ã€‚  \n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºåŒè½¨çªç ´ï¼šä¸€æ˜¯å°†ç¯å¢ƒç”Ÿæˆä»â€œç¦»çº¿å…¨é‡æ„å»ºâ€è½¬å‘â€œåœ¨çº¿äº¤äº’ç”Ÿæˆâ€ï¼Œä»æ ¹æœ¬ä¸Šé‡æ„æ•™å¸ˆ-å­¦ç”Ÿè€¦åˆæœºåˆ¶ï¼›äºŒæ˜¯é‡æ–°å®šä¹‰åæ‚”ä¼˜åŒ–ç›®æ ‡â€”â€”ä»è¿½æ±‚â€œå­¦ç”Ÿè¡¨ç°å·®â€è½¬å‘è¯†åˆ«â€œæœ€ä¼˜ç­–ç•¥èƒ½åŠ›è¢«ä¸¥é‡ä½ä¼°â€çš„çŠ¶æ€ï¼Œä½¿æŒ‡æ ‡ä¸çœŸå®å­¦ä¹ ç“¶é¢ˆå¯¹é½ã€‚äºŒè€…ç»“åˆï¼Œä¸ºæ— ç›‘ç£è¯¾ç¨‹å­¦ä¹ å‘çœŸå®å¤æ‚ä¸–ç•Œè¿ç§»æä¾›äº†å¯æ‰©å±•çš„æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study",
        "authors": [
            "Keyu Lv",
            "Manyi Zhang",
            "Xiaobo Xia",
            "Jingchen Ni",
            "Shannan Yan",
            "Xianzhi Yu",
            "Lu Hou",
            "Chun Yuan",
            "Haoli Bai"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14888v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14888v1",
        "summary": "Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.",
        "category": "é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14888v1ã€é‡åŒ–æ„ŸçŸ¥è®­ç»ƒã€‘What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs_ A Systematic Study.pdf",
        "institution": "æ¸…åå¤§å­¦ã€åä¸ºæŠ€æœ¯ã€æ–°åŠ å¡å›½ç«‹å¤§å­¦",
        "note": "ğŸ“–æ ‡é¢˜ï¼šWhat Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study\nğŸŒæ¥æºï¼šarXiv, 2601.14888v1\n\nç¬”è®°æ ‡é¢˜ï¼šä½æ¯”ç‰¹QATä¼˜åŒ–æ¨ç†\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹\nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ä½¿ä½æ¯”ç‰¹é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰åœ¨æ¨ç†å¤§æ¨¡å‹ä¸­æœ‰æ•ˆå·¥ä½œï¼Ÿ\nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºReasoning-QATæµç¨‹ï¼Œç³»ç»Ÿæ­ç¤ºçŸ¥è¯†è’¸é¦ã€PTQåˆå§‹åŒ–ã€å¼ºåŒ–å­¦ä¹ å†·å¯åŠ¨å’Œæ•°æ®åŸŸå¯¹é½æ˜¯æå‡ä½æ¯”ç‰¹æ¨ç†æ¨¡å‹æ€§èƒ½çš„å…³é”®å› ç´ ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯\nğŸ”¸é‡‡ç”¨çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰ä½œä¸ºQATçš„ä¸»è¦è®­ç»ƒç›®æ ‡ï¼Œç›¸æ¯”ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰èƒ½æ›´ç¨³å®šæ¢å¤æ¨ç†èƒ½åŠ›ã€‚\nğŸ”¸åˆ©ç”¨åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰ç»“æœï¼ˆå¦‚GPTQï¼‰ä¸ºQATæä¾›åˆå§‹åŒ–æƒé‡ï¼Œæ˜¾è‘—æå‡æ”¶æ•›é€Ÿåº¦ä¸æœ€ç»ˆç²¾åº¦ã€‚\nğŸ”¸åœ¨KDå®Œæˆå†·å¯åŠ¨åå¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œè¿›ä¸€æ­¥æå‡é‡åŒ–æ¨¡å‹çš„æ¨ç†è¡¨ç°ã€‚\nğŸ”¸ç¡®ä¿PTQæ ¡å‡†æ•°æ®ä¸QATè®­ç»ƒæ•°æ®çš„é¢†åŸŸä¸€è‡´ï¼ŒåŠ é€Ÿæ”¶æ•›å¹¶æé«˜æœ€ç»ˆæ€§èƒ½ã€‚\n\nğŸ”åˆ†ææ€»ç»“\nğŸ”¸KDåœ¨SFTå’ŒRLè®­ç»ƒçš„æ¨¡å‹ä¸Šå‡ä¼˜äºSFTï¼Œå°¤å…¶åœ¨3/2-bitè®¾ç½®ä¸‹å‡å°‘æ›´å¤šå‡†ç¡®ç‡ä¸‹é™ã€‚\nğŸ”¸ä»¥GPTQåˆå§‹åŒ–çš„QATæ¯”éšæœºåˆå§‹åŒ–ï¼ˆRTNï¼‰èµ·å§‹æ€§èƒ½æ›´é«˜ï¼Œæ”¶æ•›æ›´å¿«ï¼Œè®­ç»ƒæ›´ç¨³å®šã€‚\nğŸ”¸ç›´æ¥åœ¨ä¸¥é‡é‡åŒ–çš„æ¨¡å‹ä¸Šåº”ç”¨RLä¼šå¤±è´¥ï¼Œä½†ç»è¿‡KDæ¢å¤åå¯æˆåŠŸæå‡æ€§èƒ½ã€‚\nğŸ”¸å½“PTQæ ¡å‡†ä½¿ç”¨æ•°å­¦æ•°æ®ï¼ˆNuminaMathï¼‰ã€QATè®­ç»ƒä¹Ÿç”¨æ•°å­¦æ•°æ®ï¼ˆOpenR1-Mathï¼‰æ—¶ï¼Œæ”¶æ•›æœ€å¿«ä¸”æœ€ç»ˆæ•ˆæœæœ€å¥½ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹\nè®ºæ–‡åˆ›æ–°æ€§åœ°å°†QATåº”ç”¨äºæ¨ç†å‹å¤§æ¨¡å‹ï¼Œå¹¶é€šè¿‡ç³»ç»Ÿå®éªŒæç‚¼å‡ºå››ä¸ªå…³é”®è®¾è®¡åŸåˆ™ã€‚å…¶æå‡ºçš„ä¸‰é˜¶æ®µæµç¨‹ï¼ˆPTQåˆå§‹åŒ– â†’ KDæ¢å¤ â†’ RLç²¾è°ƒï¼‰ä¸ä»…æœ‰æ•ˆï¼Œè¿˜å…·å¼ºå¯å¤ç°æ€§ã€‚ç‰¹åˆ«æŒ‡å‡ºKDå¯¹è¾“å‡ºåˆ†å¸ƒçš„å¹³æ»‘å¯¹é½ä½œç”¨ä»¥åŠRLéœ€ä¾èµ–è¶³å¤Ÿåˆå§‹èƒ½åŠ›çš„è§‚ç‚¹ï¼Œå¯¹åç»­ä½æ¯”ç‰¹è®­ç»ƒæ–¹æ³•è®¾è®¡å…·æœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ã€‚\n    "
    },
    {
        "title": "CodeDelegator: Mitigating Context Pollution via Role Separation in Code-as-Action Agents",
        "authors": [
            "Tianxiang Fei",
            "Cheng Chen",
            "Yue Pan",
            "Mao Zheng",
            "Mingyang Song"
        ],
        "categories": [
            "cs.CL"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14914v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14914v1",
        "summary": "Recent advances in large language models (LLMs) allow agents to represent actions as executable code, offering greater expressivity than traditional tool-calling. However, real-world tasks often demand both strategic planning and detailed implementation. Using a single agent for both leads to context pollution from debugging traces and intermediate failures, impairing long-horizon performance. We propose CodeDelegator, a multi-agent framework that separates planning from implementation via role specialization. A persistent Delegator maintains strategic oversight by decomposing tasks, writing specifications, and monitoring progress without executing code. For each sub-task, a new Coder agent is instantiated with a clean context containing only its specification, shielding it from prior failures. To coordinate between agents, we introduce Ephemeral-Persistent State Separation (EPSS), which isolates each Coder's execution state while preserving global coherence, preventing debugging traces from polluting the Delegator's context. Experiments on various benchmarks demonstrate the effectiveness of CodeDelegator across diverse scenarios.",
        "category": "Agent",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14914v1ã€Agentã€‘CodeDelegator_ Mitigating Context Pollution via Role Separation in Code-as-Action Agents.pdf",
        "institution": "Tencent",
        "note": "ğŸ“–æ ‡é¢˜ï¼šCodeDelegator: Mitigating Context Pollution via Role Separation in Code-as-Action Agents\nğŸŒæ¥æºï¼šarXiv, 2601.14914v1\n\nç¬”è®°æ ‡é¢˜ï¼šè§’è‰²åˆ†ç¦»é˜²æ±¡æŸ“  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è§£å†³ä»£ç ä½œä¸ºåŠ¨ä½œçš„æ™ºèƒ½ä½“åœ¨é•¿å‘¨æœŸä»»åŠ¡ä¸­å› è°ƒè¯•ç—•è¿¹ç´¯ç§¯å¯¼è‡´çš„ä¸Šä¸‹æ–‡æ±¡æŸ“é—®é¢˜ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºCodeDelegatoræ¡†æ¶ï¼Œé€šè¿‡è§„åˆ’ä¸å®ç°çš„è§’è‰²åˆ†ç¦»å’ŒçŠ¶æ€éš”ç¦»æœºåˆ¶ç¼“è§£ä¸Šä¸‹æ–‡æ±¡æŸ“ï¼Œæå‡å¤æ‚ä»»åŠ¡æˆåŠŸç‡ã€‚  \n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å¼•å…¥åŒè§’è‰²æ¶æ„ï¼šç”±æŒä¹…æ€§Delegatorè´Ÿè´£ä»»åŠ¡åˆ†è§£ã€è¿›åº¦ç›‘æ§ä¸åŠ¨æ€è°ƒåº¦ï¼Œä¸å‚ä¸ä»£ç æ‰§è¡Œï¼›æ¯ä¸ªå­ä»»åŠ¡ç”±ä¸´æ—¶Coderç‹¬ç«‹å®Œæˆï¼Œäº«æœ‰å¹²å‡€ä¸Šä¸‹æ–‡ã€‚  \nğŸ”¸è®¾è®¡Ephemeral-Persistent State Separationï¼ˆEPSSï¼‰ï¼šæ„å»ºåŒå±‚å·¥ä½œç©ºé—´ï¼ŒOrchestration Layerä¿å­˜å…¨å±€çŠ¶æ€ä¸æˆæœï¼ŒExecution Layerä¸ºæ¯ä¸ªCoderæä¾›éš”ç¦»æ²™ç®±ï¼Œé˜²æ­¢å˜é‡å†²çªä¸ç—•è¿¹æ‰©æ•£ã€‚  \nğŸ”¸é‡‡ç”¨ç»“æ„åŒ–é€šä¿¡åè®®ï¼šä¸Šä¸‹è¡Œä¿¡æ¯å‡åŸºäºç±»å‹åŒ–schemaï¼Œå‘ä¸‹ä¼ é€’è§„èŒƒï¼ˆæŒ‡ä»¤ã€è¾“å…¥ç»‘å®šã€è¿”å›æ ¼å¼ï¼‰ï¼Œå‘ä¸Šä»…åé¦ˆç»“æœçŠ¶æ€ã€äº§å‡ºç‰©ä¸è¯Šæ–­æ‘˜è¦ï¼Œé¿å…è‡ªç„¶è¯­è¨€ä¼ é€’çš„ä¿¡æ¯æŸè€—æˆ–å†—ä½™ã€‚  \nğŸ”¸å®ç°äº¤äº’å¼è¿­ä»£æ‰§è¡Œï¼šCoderåœ¨æ²™ç®±ä¸­å¾ªç¯ç”Ÿæˆä»£ç ã€è§‚å¯Ÿè¾“å‡ºã€è°ƒè¯•ä¿®æ­£ï¼Œç›´è‡³æˆåŠŸæˆ–è€—å°½é¢„ç®—ï¼Œå¤±è´¥ç»†èŠ‚æœ¬åœ°ä¸¢å¼ƒï¼Œä»…ç»“æ„åŒ–ç»“æœå›ä¼ ã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸å®éªŒè¡¨æ˜ï¼Œéšç€ä»»åŠ¡å¤æ‚åº¦ä¸Šå‡ï¼Œä¼ ç»Ÿå•æ™ºèƒ½ä½“æ¨¡å¼æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè€ŒCodeDelegatoråœ¨é«˜å¤æ‚åº¦ä»»åŠ¡ä¸­ä¼˜åŠ¿æ›´æ˜æ˜¾ï¼ŒéªŒè¯äº†è§’è‰²åˆ†ç¦»çš„æœ‰æ•ˆæ€§ã€‚  \nğŸ”¸åœ¨Ï„2-benchå’ŒMCPMarkå¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCodeDelegatorå‡ä¼˜äºReActä¸CodeActï¼Œå°¤å…¶åœ¨éœ€å¤šæ­¥APIäº¤äº’çš„é¢†åŸŸï¼ˆå¦‚GitHubã€Notionï¼‰æå‡è¶…15%ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼Œç§»é™¤EPSSæˆ–è§’è‰²åˆ†ç¦»åˆ†åˆ«å¯¼è‡´4.7%å’Œ10.5%æ€§èƒ½ä¸‹é™ï¼Œè¯´æ˜ä¸¤ç§æœºåˆ¶å¯¹æ•´ä½“æ•ˆæœå‡æœ‰é‡è¦è´¡çŒ®ã€‚  \nğŸ”¸PostgreSQLåœºæ™¯ä¸‹ReActè¡¨ç°æ›´å¥½ï¼Œå½’å› äºå…¶å•æ­¥è°ƒç”¨æ›´å¥‘åˆäº‹åŠ¡åŸå­æ€§ï¼Œåæ˜ å½“å‰æ¡†æ¶åœ¨äº‹åŠ¡ç®¡ç†ä¸Šçš„å±€é™ã€‚  \n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°åœ°å°†â€œèŒè´£åˆ†ç¦»â€æ€æƒ³å¼•å…¥LLMæ™ºèƒ½ä½“è®¾è®¡ï¼Œæ˜ç¡®æå‡ºå¹¶é‡åŒ–äº†ä¸Šä¸‹æ–‡æ±¡æŸ“é—®é¢˜ã€‚EPSSæœºåˆ¶ä¸ä»…å®ç°é€»è¾‘éš”ç¦»ï¼Œæ›´é€šè¿‡å¯¹è±¡å¼•ç”¨ä¼ é€’ä¿éšœæ•°æ®ä¿çœŸï¼Œç»“æ„åŒ–é€šä¿¡æå‡äº†åä½œå¯é æ€§ã€‚è¯¥æ¡†æ¶ä¸ºæ„å»ºå¯æ‰©å±•ã€é«˜é²æ£’æ€§çš„ä»£ç†ç³»ç»Ÿæä¾›äº†æ¸…æ™°è·¯å¾„ï¼Œæœªæ¥è‹¥æ”¯æŒDAGå¼å¹¶è¡Œä»»åŠ¡è°ƒåº¦ï¼Œæ½œåŠ›å°†è¿›ä¸€æ­¥é‡Šæ”¾ã€‚\n    "
    },
    {
        "title": "RECAP: Resistance Capture in Text-based Mental Health Counseling with Large Language Models",
        "authors": [
            "Anqi Li",
            "Yuqian Chen",
            "Yu Lu",
            "Zhaoming Chen",
            "Yuan Xie",
            "Zhenzhong Lan"
        ],
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14780v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14780v1",
        "summary": "Recognizing and navigating client resistance is critical for effective mental health counseling, yet detecting such behaviors is particularly challenging in text-based interactions. Existing NLP approaches oversimplify resistance categories, ignore the sequential dynamics of therapeutic interventions, and offer limited interpretability.\n  To address these limitations, we propose PsyFIRE, a theoretically grounded framework capturing 13 fine-grained resistance behaviors alongside collaborative interactions. Based on PsyFIRE, we construct the ClientResistance corpus with 23,930 annotated utterances from real-world Chinese text-based counseling, each supported by context-specific rationales. Leveraging this dataset, we develop RECAP, a two-stage framework that detects resistance and fine-grained resistance types with explanations.\n  RECAP achieves 91.25% F1 for distinguishing collaboration and resistance and 66.58% macro-F1 for fine-grained resistance categories classification, outperforming leading prompt-based LLM baselines by over 20 points. Applied to a separate counseling dataset and a pilot study with 62 counselors, RECAP reveals the prevalence of resistance, its negative impact on therapeutic relationships and demonstrates its potential to improve counselors' understanding and intervention strategies.",
        "category": "RECAPæ¡†æ¶",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14780v1ã€RECAPæ¡†æ¶ã€‘RECAP_ Resistance Capture in Text-based Mental Health Counseling with Large Language Models.pdf",
        "institution": "æµ™æ±Ÿå¤§å­¦ã€è¥¿æ¹–å¤§å­¦",
        "note": "ğŸ“–æ ‡é¢˜ï¼šRECAP: Resistance Capture in Text-based Mental Health Counseling with Large Language Models\nğŸŒæ¥æºï¼šarXiv, 2601.14780v1\n\nç¬”è®°æ ‡é¢˜ï¼šç»†ç²’åº¦æ•æ‰å’¨è¯¢é˜»æŠ—  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨åŸºäºæ–‡æœ¬çš„å¿ƒç†å¥åº·å’¨è¯¢ä¸­å‡†ç¡®è¯†åˆ«å¹¶è§£é‡Šå®¢æˆ·å¤šæ ·çš„é˜»æŠ—è¡Œä¸ºï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºPsyFIREåˆ†ç±»æ¡†æ¶ä¸RECAPæ¨¡å‹ï¼Œå®ç°å¯¹13ç±»ç»†ç²’åº¦é˜»æŠ—è¡Œä¸ºçš„é«˜ç²¾åº¦æ£€æµ‹ä¸å¯è§£é‡Šæ€§åˆ†æã€‚  \n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºPsyFIREç†è®ºé©±åŠ¨çš„é˜»æŠ—åˆ†ç±»ä½“ç³»ï¼Œæ¶µç›–4å¤§ç±»13ç§ç»†ç²’åº¦å®¢æˆ·é˜»æŠ—è¡Œä¸ºï¼Œå¹¶å¼ºè°ƒå…¶ä½œä¸ºå¯¹å’¨è¯¢å¸ˆå¹²é¢„çš„äº’åŠ¨æ€§ååº”ã€‚  \nğŸ”¸åŸºäºçœŸå®ä¸­æ–‡æ–‡æœ¬å’¨è¯¢æ•°æ®æ„å»ºClientResistanceè¯­æ–™åº“ï¼ŒåŒ…å«23,930æ¡æ ‡æ³¨è¯­å¥ï¼Œæ¯æ¡å‡é™„æœ‰ä¸Šä¸‹æ–‡ç›¸å…³çš„ä¸“å®¶ç†ç”±è¯´æ˜ã€‚  \nğŸ”¸è®¾è®¡RECAPä¸¤é˜¶æ®µæ¡†æ¶ï¼šç¬¬ä¸€é˜¶æ®µåŒºåˆ†åˆä½œä¸é˜»æŠ—ï¼Œç¬¬äºŒé˜¶æ®µè¯†åˆ«å…·ä½“é˜»æŠ—ç±»å‹å¹¶ç”Ÿæˆè§£é‡Šï¼Œé‡‡ç”¨Llama-3.1-8B-Instructè¿›è¡Œå…¨å‚æ•°å¾®è°ƒã€‚  \nğŸ”¸é€šè¿‡5æŠ˜äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå¹¶ä¸å¤šç§ä¸»æµå¤§æ¨¡å‹çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºæ–¹æ³•å¯¹æ¯”ï¼ŒéªŒè¯å…¶ä¼˜è¶Šæ€§ã€‚  \nğŸ”¸å°†RECAPåº”ç”¨äºç‹¬ç«‹æ•°æ®é›†CounselingWAIï¼Œåˆ†æé˜»æŠ—å‘ç”Ÿé¢‘ç‡åŠå…¶ä¸æ²»ç–—è”ç›Ÿçš„å…³ç³»ï¼›å¹¶é€šè¿‡62åå’¨è¯¢å¸ˆå‚ä¸çš„å¯¹ç…§å®éªŒéªŒè¯åé¦ˆç³»ç»Ÿçš„å®ç”¨æ€§ã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸RECAPåœ¨äºŒå…ƒåˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°91.25% F1å€¼ï¼Œåœ¨ç»†ç²’åº¦åˆ†ç±»ä»»åŠ¡ä¸Šå®ç°66.58% macro-F1ï¼Œæ˜¾è‘—ä¼˜äºGPT-4oç­‰åŸºçº¿æ¨¡å‹ï¼ˆé¢†å…ˆè¶…20ä¸ªç™¾åˆ†ç‚¹ï¼‰ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼Œä½¿ç”¨ä»»åŠ¡ç‰¹å®šæ•°æ®å¾®è°ƒæ¯”LoRAæ›´æœ‰æ•ˆï¼ŒåŠ å…¥è§£é‡Šä¿¡æ¯è¿›ä¸€æ­¥æå‡æ¨¡å‹è¡¨ç°ï¼Œå°¤å…¶åœ¨å¤æ‚ç±»åˆ«ä¸­æ•ˆæœæ˜æ˜¾ã€‚  \nğŸ”¸å®é™…åº”ç”¨å‘ç°ï¼šè¶…è¿‡90%çš„å’¨è¯¢ä¼šè¯ä¸­å‡ºç°é˜»æŠ—ï¼Œå¹³å‡æ¯ä¼šè¯æ¶‰åŠä¸¤ç§ä»¥ä¸Šé˜»æŠ—ç±»å‹ï¼›å…¶ä¸­â€œè´¨ç–‘-è´¬ä½â€è¡Œä¸ºå¯¹æ²»ç–—å…³ç³»ç ´åæœ€å¼ºã€‚  \nğŸ”¸è¯•ç‚¹å®éªŒæ˜¾ç¤ºï¼Œè·å¾—RECAPåé¦ˆçš„å®éªŒç»„å’¨è¯¢å¸ˆåœ¨åº”å¯¹é˜»æŠ—ç­–ç•¥ä¸Šçš„æ”¹è¿›ç¨‹åº¦æ˜¾è‘—é«˜äºå¯¹ç…§ç»„ï¼ˆCohenâ€™s d = 1.17ï¼‰ï¼Œä¸”æ™®éè®¤ä¸ºåé¦ˆæœ‰åŠ©äºå¿«é€Ÿè¯†åˆ«å’Œè°ƒæ•´å¹²é¢„æ–¹å¼ã€‚  \n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥ç ”ç©¶åˆ›æ–°æ€§åœ°å°†å¿ƒç†æ²»ç–—ç†è®ºä¸NLPæŠ€æœ¯æ·±åº¦èåˆï¼Œä¸ä»…æå‡ºäº†é¦–ä¸ªé¢å‘æ–‡æœ¬å’¨è¯¢çš„ç»†ç²’åº¦é˜»æŠ—åˆ†ç±»ä½“ç³»PsyFIREï¼Œè¿˜æ„å»ºäº†é«˜è´¨é‡æ ‡æ³¨æ•°æ®é›†å¹¶å¼€å‘å‡ºå…·å¤‡è§£é‡Šèƒ½åŠ›çš„å®ç”¨å·¥å…·RECAPã€‚å…¶æœ€å¤§äº®ç‚¹åœ¨äºå¼ºè°ƒâ€œäº¤äº’æ€§â€ä¸â€œå¯è§£é‡Šæ€§â€ï¼Œä½¿AIè¾“å‡ºä¸ä»…èƒ½è¢«ç†è§£ï¼Œæ›´èƒ½æŒ‡å¯¼å®è·µã€‚æ­¤å¤–ï¼Œé€šè¿‡çœŸå®åœºæ™¯éªŒè¯æ¨¡å‹ä»·å€¼ï¼Œå±•ç°äº†äººå·¥æ™ºèƒ½è¾…åŠ©å¿ƒç†å’¨è¯¢çš„å·¨å¤§æ½œåŠ›ã€‚\n    "
    },
    {
        "title": "FunCineForge: A Unified Dataset Toolkit and Model for Zero-Shot Movie Dubbing in Diverse Cinematic Scenes",
        "authors": [
            "Jiaxuan Liu",
            "Yang Xiang",
            "Han Zhao",
            "Xiangang Li",
            "Zhenhua Ling"
        ],
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14777v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14777v1",
        "summary": "Movie dubbing is the task of synthesizing speech from scripts conditioned on video scenes, requiring accurate lip sync, faithful timbre transfer, and proper modeling of character identity and emotion. However, existing methods face two major limitations: (1) high-quality multimodal dubbing datasets are limited in scale, suffer from high word error rates, contain sparse annotations, rely on costly manual labeling, and are restricted to monologue scenes, all of which hinder effective model training; (2) existing dubbing models rely solely on the lip region to learn audio-visual alignment, which limits their applicability to complex live-action cinematic scenes, and exhibit suboptimal performance in lip sync, speech quality, and emotional expressiveness. To address these issues, we propose FunCineForge, which comprises an end-to-end production pipeline for large-scale dubbing datasets and an MLLM-based dubbing model designed for diverse cinematic scenes. Using the pipeline, we construct the first Chinese television dubbing dataset with rich annotations, and demonstrate the high quality of these data. Experiments across monologue, narration, dialogue, and multi-speaker scenes show that our dubbing model consistently outperforms SOTA methods in audio quality, lip sync, timbre transfer, and instruction following. Code and demos are available at https://anonymous.4open.science/w/FunCineForge.",
        "category": "MLLM-based dubbing model",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14777v1ã€MLLM-based dubbing modelã€‘FunCineForge_ A Unified Dataset Toolkit and Model for Zero-Shot Movie Dubbing in Diverse Cinematic Scenes.pdf",
        "institution": "Alibaba Groupã€University of Science and Technology of China",
        "note": "ğŸ“–æ ‡é¢˜ï¼šFunCineForge: A Unified Dataset Toolkit and Model for Zero-Shot Movie Dubbing in Diverse Cinematic Scenes\nğŸŒæ¥æºï¼šarXiv, 2601.14777v1\n\nç¬”è®°æ ‡é¢˜ï¼šç»Ÿä¸€æ•°æ®ä¸æ¨¡å‹çš„é›¶æ ·æœ¬ç”µå½±é…éŸ³  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨å¤šæ ·åŒ–çœŸå®å½±è§†åœºæ™¯ä¸­å®ç°é«˜è´¨é‡ã€é›¶æ ·æœ¬çš„ç”µå½±é…éŸ³ï¼ŒåŒæ—¶è§£å†³ç°æœ‰æ•°æ®é›†è§„æ¨¡å°ã€æ ‡æ³¨ç¨€ç–ã€ä¾èµ–äººå·¥ï¼Œä»¥åŠæ¨¡å‹éš¾ä»¥åº”å¯¹å¤šè¯´è¯äººã€é•œå¤´åˆ‡æ¢ã€é¢éƒ¨é®æŒ¡ç­‰å¤æ‚åœºæ™¯çš„é—®é¢˜ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºäº†FunCineForgeâ€”â€”é¦–ä¸ªç«¯åˆ°ç«¯çš„å½±è§†é…éŸ³æ•°æ®æ„å»ºå·¥å…·é“¾ä¸MLLMé©±åŠ¨çš„é›¶æ ·æœ¬é…éŸ³æ¨¡å‹ï¼Œæ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨ã€å¯Œæ ‡æ³¨çš„ä¸­æ–‡ç”µè§†å‰§é…éŸ³æ•°æ®é›†CineDub-CNï¼Œå¹¶åœ¨å•/å¤šè¯´è¯äººã€å¯¹è¯ã€æ—ç™½ç­‰å…¨åœºæ™¯ä¸‹æ˜¾è‘—è¶…è¶ŠSOTAã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æå‡ºâ€œè½»é‡ä¸“ç”¨æ¨¡å‹é¢„æµ‹+å¤šæ¨¡æ€æ€ç»´é“¾ï¼ˆCoTï¼‰ä¿®æ­£â€æ•°æ®æµæ°´çº¿ï¼ŒèåˆASRã€å£°æºåˆ†ç¦»ã€è§†è§‰å¢å¼ºè¯´è¯äººæ—¥å¿—ï¼ˆdiarizationï¼‰ã€LLMè¯­ä¹‰çº é”™ä¸å±æ€§æ¨ç†ï¼Œå…¨è‡ªåŠ¨äº§å‡ºå¸¦å¸§çº§æ—¶é—´æˆ³ã€è¯´è¯äººIDã€æ€§åˆ«å¹´é¾„ã€éŸ³è‰²å…³é”®è¯ã€æƒ…ç»ªçº¿ç´¢åŠåœºæ™¯ç±»åˆ«çš„ç»“æ„åŒ–å¤šæ¨¡æ€æ•°æ®ã€‚  \nğŸ”¸è®¾è®¡æ–°å‹å¤šæ¨¡æ€å¯¹é½æœºåˆ¶ï¼šä»¥æ—¶é—´æˆ³-è¯´è¯äººå…ƒç»„ï¼ˆTNï¼‰æä¾›å¼ºæ—¶åºç›‘ç£ï¼Œè”åˆå»ºæ¨¡â€œè¯­éŸ³å‘ç”Ÿä½ç½®â€â€œè¯­éŸ³å†…å®¹â€å’Œâ€œç»†ç²’åº¦å”‡éŸ³å¯¹é½â€ï¼Œå¼•å…¥å¸§çº§è¯­éŸ³æ´»åŠ¨æŸå¤±ï¼ˆLVAï¼‰ä¸å”‡åµŒå…¥å¯¹æ¯”æŸå¤±ï¼ˆLLipï¼‰ã€‚  \nğŸ”¸æ”¹è¿›æµåŒ¹é…æ¨¡å—ï¼šæå‡ºè¯´è¯äººåˆ‡æ¢æ‹¼æ¥ç­–ç•¥ï¼ˆSSCï¼‰ï¼Œä¾æ®TNåœ¨é™éŸ³tokenååŠ¨æ€æ³¨å…¥å¯¹åº”å‚è€ƒè¯´è¯äººåµŒå…¥ï¼Œæ”¯æŒå¯¹è¯ä¸å¤šè¯´è¯äººåœºæ™¯ä¸‹çš„ç²¾å‡†éŸ³è‰²åˆ‡æ¢ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸CineDub-CNæ•°æ®é›†ç»å¤šæ¨¡æ€CoTä¿®æ­£åï¼ŒCERä¸‹é™67%ï¼ŒSPK-TLé™ä½59%ï¼ŒéªŒè¯äº†åŒå‘æ ¡éªŒæœºåˆ¶å¯æ˜¾è‘—å‡å°‘è½¬å½•é”™è¯¯ä¸è¯´è¯äººIDé”™æ¼ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šç§»é™¤TNå¯¼è‡´å¯¹è¯åœºæ™¯SPK-TLé£™å‡3.5å€ï¼Œè¯å®æ˜¾å¼æ—¶åºç›‘ç£å¯¹å¤æ‚åœºæ™¯ä¸å¯æˆ–ç¼ºï¼›ç§»é™¤LLipä½¿LSE-Dä¸Šå‡54%ï¼Œè¯´æ˜å”‡éƒ¨ç»†ç²’åº¦å¯¹é½éœ€è§†è§‰å¯¹æ¯”å­¦ä¹ è€Œéä»…é é¢éƒ¨ç‰¹å¾ã€‚  \nğŸ”¸åœ¨å•è¯´è¯äººåœºæ™¯ä¸‹ï¼ŒFunCineForgeè¾ƒInstructDubberæå‡UTMOS 0.13ã€LSE-C 0.17ï¼›åœ¨å¤šè¯´è¯äººåœºæ™¯ä¸‹ï¼ŒSPK-SIMæå‡10.45ä¸ªç™¾åˆ†ç‚¹ï¼ŒES-MOSè¾¾4.03ï¼ˆæ»¡åˆ†5ï¼‰ï¼Œè¯æ˜å…¶è·¨è¯´è¯äººä¸€è‡´æ€§ä¸æƒ…ç»ªå¯æ§æ€§ä¼˜åŠ¿ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºæ‰“ç ´â€œæ•°æ®â€”æ¨¡å‹â€å‰²è£‚èŒƒå¼ï¼šä¸€æ–¹é¢ç”¨å·¥ç¨‹åŒ–æµæ°´çº¿è§£å†³å½±è§†é…éŸ³é¢†åŸŸé•¿æœŸå­˜åœ¨çš„æ•°æ®è’ä¸æ ‡æ³¨ä¸å¯é é—®é¢˜ï¼Œå¦ä¸€æ–¹é¢å°†æ—¶é—´ç»“æ„ï¼ˆTNï¼‰ã€è§†è§‰ç»†ç²’åº¦ï¼ˆå”‡åŠ¨ï¼‰ã€è¯´è¯äººåŠ¨æ€ï¼ˆSSCï¼‰ä¸‰è€…æ·±åº¦è€¦åˆè¿›MLLMæ¶æ„ï¼Œä½¿æ¨¡å‹çœŸæ­£å…·å¤‡å¤„ç†çœŸå®å½±è§†å¤æ‚æ€§çš„èƒ½åŠ›ï¼Œä¸ºå¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡æä¾›äº†å¯å¤ç”¨çš„æ–¹æ³•è®ºæ¡†æ¶ã€‚\n    "
    },
    {
        "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
        "authors": [
            "Yifan Wang",
            "Shiyu Li",
            "Peiming Li",
            "Xiaochen Yang",
            "Yang Tang",
            "Zheng Wei"
        ],
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14750v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14750v1",
        "summary": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT",
        "category": "æ¨¡å‹æ¶æ„",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14750v1ã€æ¨¡å‹æ¶æ„ã€‘Render-of-Thought_ Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning.pdf",
        "institution": "Tencent BACã€Tsinghua Shenzhen International Graduate Schoolã€Tsinghua Universityã€Peking Universityã€University of Glasgow",
        "note": "ğŸ“–æ ‡é¢˜ï¼šRender-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning\nğŸŒæ¥æºï¼šarXiv, 2601.14750v1\n\nç¬”è®°æ ‡é¢˜ï¼šç”¨å›¾åƒæ˜¾åŒ–æ€ç»´é“¾  \n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨å‹ç¼©å¤§æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„åŒæ—¶ï¼Œä¿æŒä¸­é—´æ¨ç†æ­¥éª¤çš„å¯è§£é‡Šæ€§å’Œå¯è¿½æº¯æ€§ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºRender-of-Thoughtï¼ˆRoTï¼‰æ¡†æ¶ï¼Œé¦–æ¬¡å°†æ–‡æœ¬æ€ç»´é“¾æ¸²æŸ“ä¸ºå›¾åƒï¼Œåˆ©ç”¨è§†è§‰æ¨¡æ€å®ç°é«˜å¯†åº¦ã€æ˜¾å¼ã€å¯åˆ†æçš„æ½œæ„è¯†æ¨ç†ã€‚  \n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡å•è¡ŒåŠ¨æ€å®½åº¦æ–‡æœ¬å›¾åƒæ¸²æŸ“æ¨¡å—ï¼Œç¡®ä¿è§†è§‰åºåˆ—ä¸¥æ ¼å¯¹åº”æ–‡æœ¬é€»è¾‘é¡ºåºï¼Œé¿å…ç©ºé—´æ­§ä¹‰ã€‚  \nğŸ”¸é‡‡ç”¨å†»ç»“çš„ç°æˆè§†è§‰ç¼–ç å™¨ä½œä¸ºè¯­ä¹‰é”šç‚¹ï¼Œå°†LLMéšçŠ¶æ€å¯¹é½è‡³è§†è§‰åµŒå…¥ç©ºé—´ï¼Œæ— éœ€é¢å¤–é¢„è®­ç»ƒã€‚  \nğŸ”¸æ„å»ºä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼šç¬¬ä¸€é˜¶æ®µç”¨MSEæŸå¤±å¯¹é½æ–‡æœ¬CoTå›¾åƒä¸LLMéšçŠ¶æ€ï¼›ç¬¬äºŒé˜¶æ®µå†»ç»“æŠ•å½±å¤´ï¼Œå¾®è°ƒLLMè‡ªå›å½’ç”Ÿæˆè§†è§‰æ½œå˜é‡åŠç­”æ¡ˆã€‚  \nğŸ”¸å¼•å…¥<|img_begin|>/<|img_end|>ç‰¹æ®Šæ ‡è®°æ§åˆ¶æ¨¡æ€åˆ‡æ¢ï¼Œå¹¶å®è¯éªŒè¯å›ºå®šé•¿åº¦æ½œå˜é‡é¢„ç®—æ¯”åŠ¨æ€ç»ˆæ­¢æ›´ç¨³å®šå¯é ã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸RoTåœ¨GSM8kç­‰æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°3â€“4å€tokenå‹ç¼©ï¼ˆå¦‚108.4â†’32ï¼‰ï¼Œæ¨ç†é€Ÿåº¦æå‡æ˜¾è‘—ï¼ˆGSM-Hardå»¶è¿Ÿä»8.55sé™è‡³1.84sï¼‰ã€‚  \nğŸ”¸ç›¸æ¯”çº¯æ–‡æœ¬éšå¼CoTæ–¹æ³•ï¼ˆå¦‚CoLaRã€Coconutï¼‰ï¼ŒRoTåœ¨è·¨åŸŸæ³›åŒ–ï¼ˆå¦‚GSM-Hardã€MATHï¼‰ä¸­è¡¨ç°æ›´é²æ£’ï¼Œå½’å› äºé¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨æä¾›çš„ä¸°å¯Œè¯­ä¹‰ç›‘ç£ä¿¡å·ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ä¸¤é˜¶æ®µç¼ºä¸€ä¸å¯ï¼šç§»é™¤Stage Iå¯¼è‡´GSM8k-Augå‡†ç¡®ç‡ä¸‹é™13ä¸ªç™¾åˆ†ç‚¹ï¼›ç§»é™¤Stage IIåˆ™ä½¿MATHæ€§èƒ½ä»33.2%è·Œè‡³26.2%ã€‚  \nğŸ”¸æ½œå˜é‡å¯è§†åŒ–åˆ†æå‘ç°ï¼šå‰æ®µæ½œåµŒå…¥æ‰¿è½½æ ¸å¿ƒæ¨ç†é€»è¾‘ï¼Œåæ®µè¶‹äºé¥±å’Œï¼Œè¡¨æ˜æ¨¡å‹è‡ªåŠ¨å­¦ä¹ â€œå…³é”®æ¨ç†+ä¸Šä¸‹æ–‡ç»´æŒâ€çš„é«˜æ•ˆç»“æ„ã€‚  \n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†â€œæ–‡æœ¬è½¬å›¾åƒâ€æŠ€æœ¯ä»è¾“å…¥å‹ç¼©æ‹“å±•è‡³æ¨ç†è¿‡ç¨‹å»ºæ¨¡ï¼Œçªç ´äº†ä¼ ç»Ÿéšå¼æ½œç©ºé—´é»‘ç®±å±€é™ï¼›å…¶æ’ä»¶å¼è®¾è®¡ï¼ˆå¤ç”¨ç°æœ‰VLMï¼‰ã€æ˜¾å¼å¯è§†åŒ–èƒ½åŠ›ä¸è®¡ç®—æ•ˆç‡ä¼˜åŠ¿ï¼Œä¸ºå¯ä¿¡ã€é«˜æ•ˆã€å¯è°ƒè¯•çš„å¤§æ¨¡å‹æ¨ç†æä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
        "authors": [
            "Haowei Zhang",
            "Shudong Yang",
            "Jinlan Fu",
            "See-Kiong Ng",
            "Xipeng Qiu"
        ],
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14724v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14724v1",
        "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10$\\times$ faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.",
        "category": "æ¨¡å‹æ¶æ„",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14724v1ã€æ¨¡å‹æ¶æ„ã€‘HERMES_ KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding.pdf",
        "institution": "å¤æ—¦å¤§å­¦ã€ä¸Šæµ·åˆ›æ–°ç ”ç©¶é™¢ã€æ–°åŠ å¡å›½ç«‹å¤§å­¦",
        "note": "ğŸ“–æ ‡é¢˜ï¼šHERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding\nğŸŒæ¥æºï¼šarXiv, 2601.14724v1\n\nç¬”è®°æ ‡é¢˜ï¼šKVç¼“å­˜åˆ†å±‚è®°å¿†åŒ–  \n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨æµå¼è§†é¢‘ç†è§£ä¸­å®ç°å®æ—¶å“åº”ã€ä½æ˜¾å­˜å¼€é”€ä¸é«˜å‡†ç¡®ç‡çš„ä¸‰é‡å¹³è¡¡ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºæ— éœ€è®­ç»ƒçš„HERMESæ¡†æ¶ï¼Œå°†KVç¼“å­˜å»ºæ¨¡ä¸ºåˆ†å±‚è®°å¿†ç³»ç»Ÿï¼Œé¦–æ¬¡å®ç°åŸºäºæœºåˆ¶åŒ–æ³¨æ„åŠ›åˆ†æçš„æµå¼è§†é¢‘é«˜æ•ˆç†è§£ã€‚  \n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸åŸºäºå¯¹LLaVA-OVç­‰æ¨¡å‹å„è§£ç å±‚æ³¨æ„åŠ›æ¨¡å¼çš„å®è¯åˆ†æï¼Œå‘ç°æµ…å±‚å…·å¼ºæ—¶æ•ˆæ€§ï¼ˆæ„ŸçŸ¥è®°å¿†ï¼‰ã€ä¸­å±‚å‘ˆè¿‡æ¸¡æ€§ï¼ˆå·¥ä½œè®°å¿†ï¼‰ã€æ·±å±‚å…·å¸§çº§ç¨€ç–é”šç‚¹ï¼ˆé•¿æ—¶è®°å¿†ï¼‰ï¼Œæ®æ­¤å°†KVç¼“å­˜åˆ’åˆ†ä¸ºä¸‰å±‚ç®¡ç†ã€‚  \nğŸ”¸è®¾è®¡åˆ†å±‚é‡è¦æ€§è¯„åˆ†ï¼šæµ…å±‚é‡‡ç”¨æŒ‡æ•°è¡°å‡æ¨¡å‹åˆ»ç”»æ—¶æ•ˆæ€§ï¼›æ·±å±‚ç›´æ¥åˆ©ç”¨æ³¨æ„åŠ›æƒé‡è¡¡é‡å¸§çº§è¯­ä¹‰é‡è¦æ€§ï¼›ä¸­å±‚é€šè¿‡å¯è°ƒæƒé‡æ’å€¼äºŒè€…ã€‚  \nğŸ”¸å¼•å…¥è·¨å±‚è®°å¿†å¹³æ»‘æœºåˆ¶ï¼Œè‡ªæ·±å‘æµ…ä¼ æ’­é‡è¦æ€§ä¿¡å·ï¼Œç¼“è§£å±‚é—´ä¸ä¸€è‡´ï¼›å¹¶é‡‡ç”¨æ‡’æƒ°å¼ä½ç½®é‡ç´¢å¼•ï¼Œåœ¨ä¿æŒRoPEè¯­ä¹‰è¿ç»­æ€§çš„åŒæ—¶é¿å…é¢‘ç¹è®¡ç®—å¼€é”€ã€‚  \nğŸ”¸å¯¹æ·±å±‚è¢«è£å‰ªçš„è§†è§‰tokenè¿›è¡Œç›¸ä½å¯¹é½åçš„å‡å€¼èšåˆï¼Œç”Ÿæˆæ‘˜è¦tokenä¿ç•™åœ¨KVç¼“å­˜ä¸­ï¼Œä»¥å‹ç¼©æ–¹å¼ä¿ç•™é•¿æ—¶ä¿¡æ¯ã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨StreamingBenchç­‰æµå¼åŸºå‡†ä¸Šï¼ŒHERMESåœ¨ä»…ç”¨4Kè§†é¢‘tokenï¼ˆè¾ƒå‡åŒ€é‡‡æ ·å‡å°‘68%ï¼‰æ—¶ï¼Œå‡†ç¡®ç‡åè¶…åŸºçº¿æœ€é«˜11.4%ï¼Œä¸”TTFTé™ä½è‡³27msï¼Œè¾¾SOTAæ–¹æ³•10å€åŠ é€Ÿã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¡¨æ˜ï¼šè·¨å±‚å¹³æ»‘ä¸æ‘˜è¦tokenæ˜¾è‘—æå‡é•¿è§†é¢‘ç†è§£æ€§èƒ½ï¼›æ‡’æƒ°é‡ç´¢å¼•æ›´é€‚åˆæµå¼åœºæ™¯ï¼Œè€Œæ¿€è¿›é‡ç´¢å¼•æ›´é€‚ç¦»çº¿ä»»åŠ¡ï¼›4Kå†…å­˜é¢„ç®—å³è¾¾æ€§èƒ½é¥±å’Œã€‚  \nğŸ”¸HERMESåœ¨OVO-Benchã€RVSç­‰å¤šç±»å‹æµå¼ä»»åŠ¡ï¼ˆå®æ—¶æ„ŸçŸ¥/å›æº¯æ¨ç†/å¼€æ”¾ç”Ÿæˆï¼‰åŠMVBenchã€VideoMMEç­‰ç¦»çº¿é•¿è§†é¢‘åŸºå‡†ä¸Šå‡ä¿æŒç«äº‰åŠ›ï¼ŒéªŒè¯å…¶æ³›åŒ–æ€§ã€‚  \nğŸ”¸GPUæ˜¾å­˜å ç”¨æ’å®šï¼ˆå¦‚17.66GBï¼‰ï¼Œä¸éšè§†é¢‘é•¿åº¦å¢é•¿ï¼Œæ— OOMé£é™©ï¼Œä¸”æŸ¥è¯¢æ—¶æ— éœ€é¢å¤–æ£€ç´¢æˆ–å¤–éƒ¨è®¡ç®—ï¼ŒçœŸæ­£å®ç°ç«¯åˆ°ç«¯å®æ—¶å“åº”ã€‚  \n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°æ€§åœ¨äºå°†ä¼ ç»Ÿè§†ä¸ºâ€œä¸´æ—¶å­˜å‚¨â€çš„KVç¼“å­˜å‡ç»´ä¸ºå…·æœ‰è®¤çŸ¥å¿ƒç†å­¦åŸºç¡€ï¼ˆæ„Ÿè§‰/å·¥ä½œ/é•¿æ—¶è®°å¿†ï¼‰çš„åˆ†å±‚è®°å¿†æ¶æ„ï¼Œå¹¶é€šè¿‡å¯è§£é‡Šçš„æ³¨æ„åŠ›æœºåˆ¶é©±åŠ¨å‹ç¼©ç­–ç•¥ï¼Œå…¼é¡¾æ•ˆç‡ã€ç²¾åº¦ä¸éƒ¨ç½²å‹å¥½æ€§ï¼Œä¸ºæµå¼å¤šæ¨¡æ€ç†è§£æä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "DeepMoLM: Leveraging Visual and Geometric Structural Information for Molecule-Text Modeling",
        "authors": [
            "Jing Lan",
            "Hexiao Ding",
            "Hongzhao Chen",
            "Yufeng Jiang",
            "Nga-Chun Ng",
            "Gwing Kei Yip",
            "Gerald W. Y. Cheng",
            "Yunlin Mao",
            "Jing Cai",
            "Liang-ting Lin",
            "Jung Sun Yoo"
        ],
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.MM"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14732v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14732v1",
        "summary": "AI models for drug discovery and chemical literature mining must interpret molecular images and generate outputs consistent with 3D geometry and stereochemistry. Most molecular language models rely on strings or graphs, while vision-language models often miss stereochemical details and struggle to map continuous 3D structures into discrete tokens. We propose DeepMoLM: Deep Molecular Language M odeling, a dual-view framework that grounds high-resolution molecular images in geometric invariants derived from molecular conformations. DeepMoLM preserves high-frequency evidence from 1024 $\\times$ 1024 inputs, encodes conformer neighborhoods as discrete Extended 3-Dimensional Fingerprints, and fuses visual and geometric streams with cross-attention, enabling physically grounded generation without atom coordinates. DeepMoLM improves PubChem captioning with a 12.3% relative METEOR gain over the strongest generalist baseline while staying competitive with specialist methods. It produces valid numeric outputs for all property queries and attains MAE 13.64 g/mol on Molecular Weight and 37.89 on Complexity in the specialist setting. On ChEBI-20 description generation from images, it exceeds generalist baselines and matches state-of-the-art vision-language models. Code is available at https://github.com/1anj/DeepMoLM.",
        "category": "æ¨¡å‹æ¶æ„",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14732v1ã€æ¨¡å‹æ¶æ„ã€‘DeepMoLM_ Leveraging Visual and Geometric Structural Information for Molecule-Text Modeling.pdf",
        "institution": "é¦™æ¸¯ç†å·¥å¤§å­¦ã€é¦™æ¸¯åœ£çº¦ç¿°åŒ»é™¢ã€ä¼Šåˆ©æ²™ä¼¯åŒ»é™¢",
        "note": "ğŸ“–æ ‡é¢˜ï¼šDeepMoLM: Leveraging Visual and Geometric Structural Information for Molecule-Text Modeling\nğŸŒæ¥æºï¼šarXiv, 2601.14732v1\n\nç¬”è®°æ ‡é¢˜ï¼šè§†è§‰-å‡ ä½•åŒæ¨¡æ€åˆ†å­å»ºæ¨¡  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è®©åˆ†å­å›¾æ–‡æ¨¡å‹åŒæ—¶å‡†ç¡®ç†è§£é«˜åˆ†è¾¨ç‡åˆ†å­å›¾åƒçš„ç«‹ä½“åŒ–å­¦ç»†èŠ‚ä¸3Då‡ ä½•ç»“æ„ï¼Œè€Œä¸ä¾èµ–åŸå­åæ ‡ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºDeepMoLMæ¡†æ¶ï¼Œé¦–æ¬¡å°†é«˜åˆ†è¾¨ç‡åˆ†å­å›¾åƒä¸åŸºäºæ„è±¡çš„3Då‡ ä½•æŒ‡çº¹ï¼ˆE3FPï¼‰é€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›æ·±åº¦èåˆï¼Œå®ç°ç‰©ç†å¯è§£é‡Šã€ç«‹ä½“åŒ–å­¦ä¿çœŸçš„ç«¯åˆ°ç«¯åˆ†å­-æ–‡æœ¬ç”Ÿæˆã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡åŒè·¯å¾„DeepEncoderï¼šå±€éƒ¨çª—å£ViTæ•è·é«˜é¢‘ç‡ç«‹ä½“åŒ–å­¦çº¿ç´¢ï¼ˆå¦‚æ¥”å½¢é”®ã€ç¯é—­åˆï¼‰ï¼Œå·ç§¯tokenå‹ç¼©+å…¨å±€CLIP-Large ViTä¿éšœ1024Ã—1024å›¾åƒé«˜æ•ˆå¤„ç†ä¸é•¿ç¨‹ç»“æ„å»ºæ¨¡ã€‚  \nğŸ”¸æ„å»ºç»“æ„æ„ŸçŸ¥çš„1D+3Dè”åˆtokenåŒ–ï¼šä»¥canonical SELFIESä¸ºæ‹“æ‰‘éª¨æ¶ï¼Œå°†E3FPç”Ÿæˆçš„åŸå­çº§3DæŒ‡çº¹ï¼ˆK+1å±‚åŠå¾„å“ˆå¸Œæ ‡è¯†ï¼‰æŒ‰é‡åŸå­ä½ç½®å¯¹é½åµŒå…¥ï¼Œå½¢æˆå‡ ä½•æ„ŸçŸ¥çš„ç»“æ„åºåˆ—ã€‚  \nğŸ”¸å¼•å…¥äº¤å‰æ³¨æ„åŠ›èåˆæŠ•å½±å™¨ï¼šä»¥è§†è§‰tokenä¸ºqueryã€3Dç»“æ„tokenä¸ºkey/valueï¼Œåœ¨ç»Ÿä¸€åµŒå…¥ç©ºé—´ä¸­æ˜¾å¼å»ºç«‹å›¾åƒåƒç´ ä¸åˆ†å­å‡ ä½•ä¸å˜é‡çš„å¯¹åº”å…³ç³»ï¼Œæ— éœ€åŸå­åæ ‡å³å¯å®ç°ç‰©ç†æ¥åœ°ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨PubChemåˆ†å­æè¿°ä»»åŠ¡ä¸­ï¼ŒDeepMoLMç›¸å¯¹æœ€å¼ºé€šç”¨åŸºçº¿æå‡12.3% METEORï¼Œä¸”æ‰€æœ‰å±æ€§é¢„æµ‹è¾“å‡º100%æœ‰æ•ˆï¼ˆæ— æ ¼å¼é”™è¯¯ï¼‰ï¼ŒMAEè¾¾13.64 g/molï¼ˆåˆ†å­é‡ï¼‰å’Œ37.89ï¼ˆå¤æ‚åº¦ï¼‰ã€‚  \nğŸ”¸åœ¨ChEBI-20å›¾åƒåˆ°æè¿°ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½è¶…è¶Šæ‰€æœ‰é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸SOTA specialistæ¨¡å‹ï¼ˆå¦‚BioT5+ï¼‰æ¥è¿‘ï¼Œè¯æ˜ä»…ç”¨2Då›¾åƒå³å¯å­¦ä¹ ç­‰æ•ˆäºSMILESçš„ç»“æ„è¡¨å¾ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯å®ï¼šç§»é™¤é¢„è®­ç»ƒå¯¼è‡´å…¨é¢æ€§èƒ½ä¸‹é™ï¼›æ›¿æ¢äº¤å‰æ³¨æ„åŠ›ä¸ºç®€å•æ‹¼æ¥ä½¿METEORä¸‹é™è¶…12åˆ†ï¼›å‰”é™¤E3FPåˆ†æ”¯æ˜¾è‘—å‰Šå¼±ROUGE-Lï¼ŒéªŒè¯3Då‡ ä½•çº¿ç´¢ä¸ä¸“ç”¨èåˆæœºåˆ¶ç¼ºä¸€ä¸å¯ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°æ€§ä½“ç°åœ¨ä¸‰æ–¹é¢ï¼šä¸€æ˜¯å°†å…‰å­¦åŒ–å­¦è¯†åˆ«ï¼ˆOCSRï¼‰ä»»åŠ¡å‡çº§ä¸ºå‡ ä½•æ„ŸçŸ¥çš„ç«¯åˆ°ç«¯å›¾æ–‡å»ºæ¨¡ï¼Œçªç ´2Då›¾â†’å­—ç¬¦ä¸²çš„ä¼ ç»ŸèŒƒå¼ï¼›äºŒæ˜¯æå‡ºâ€œè§†è§‰tokenæŸ¥è¯¢å‡ ä½•æŒ‡çº¹â€çš„æ–°èåˆèŒƒå¼ï¼Œé¿å…å‡ ä½•ä¿¡æ¯åœ¨tokenåŒ–ä¸­åç¼©ï¼›ä¸‰æ˜¯ä»¥E3FPä½œä¸ºè½»é‡ã€ç¦»æ•£ã€æ—‹è½¬/å¹³ç§»ä¸å˜çš„3Då…ˆéªŒï¼Œå·§å¦™ç»•è¿‡è¿ç»­åæ ‡å»ºæ¨¡éš¾é¢˜ï¼Œå…¼å…·ç‰©ç†ä¸¥è°¨æ€§ä¸å·¥ç¨‹å¯è¡Œæ€§ã€‚\n    "
    },
    {
        "title": "DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs",
        "authors": [
            "Mingxuan Song",
            "Yusen Huo",
            "Bohan Zhou",
            "Shenglin Yin",
            "Zhen Xiao",
            "Jieyi Long",
            "Zhilin Zhang",
            "Chuan Yu"
        ],
        "categories": [
            "cs.AI",
            "cs.LG"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14711v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14711v1",
        "summary": "Optimizing the advertiser's cumulative value of winning impressions under budget constraints poses a complex challenge in online advertising, under the paradigm of AI-Generated Bidding (AIGB). Advertisers often have personalized objectives but limited historical interaction data, resulting in few-shot scenarios where traditional reinforcement learning (RL) methods struggle to perform effectively. Large Language Models (LLMs) offer a promising alternative for AIGB by leveraging their in-context learning capabilities to generalize from limited data. However, they lack the numerical precision required for fine-grained optimization. To address this limitation, we introduce GRPO-Adaptive, an efficient LLM post-training strategy that enhances both reasoning and numerical precision by dynamically updating the reference policy during training. Built upon this foundation, we further propose DARA, a novel dual-phase framework that decomposes the decision-making process into two stages: a few-shot reasoner that generates initial plans via in-context prompting, and a fine-grained optimizer that refines these plans using feedback-driven reasoning. This separation allows DARA to combine LLMs' in-context learning strengths with precise adaptability required by AIGB tasks. Extensive experiments on both real-world and synthetic data environments demonstrate that our approach consistently outperforms existing baselines in terms of cumulative advertiser value under budget constraints.",
        "category": "å¼ºåŒ–å­¦ä¹ ä¸å¤§æ¨¡å‹ç»“åˆ",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14711v1ã€å¼ºåŒ–å­¦ä¹ ä¸å¤§æ¨¡å‹ç»“åˆã€‘DARA_ Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs.pdf",
        "institution": "åŒ—äº¬å¤§å­¦ã€é˜¿é‡Œå·´å·´é›†å›¢",
        "note": "ğŸ“–æ ‡é¢˜ï¼šDARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs\nğŸŒæ¥æºï¼šarXiv, 2601.14711v1\n\nç¬”è®°æ ‡é¢˜ï¼šåŒé˜¶æ®µé¢„ç®—åˆ†é…æ–°æ¡†æ¶  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨æ•°æ®ç¨€å°‘çš„æƒ…å†µä¸‹å®ç°åœ¨çº¿å¹¿å‘Šä¸­é«˜æ•ˆçš„é¢„ç®—åˆ†é…ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºDARAæ¡†æ¶ï¼Œç»“åˆå¤§æ¨¡å‹çš„å°‘æ ·æœ¬æ¨ç†ä¸å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œæå‡é¢„ç®—åˆ†é…æ•ˆæœã€‚  \n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å°†é¢„ç®—åˆ†é…ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå°‘æ ·æœ¬æ¨ç†ç”Ÿæˆåˆå§‹è®¡åˆ’ï¼Œç»†ç²’åº¦ä¼˜åŒ–å™¨åŸºäºåé¦ˆè°ƒæ•´ã€‚  \nğŸ”¸è®¾è®¡GRPO-Adaptiveå¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•ï¼ŒåŠ¨æ€æ›´æ–°å‚è€ƒç­–ç•¥ä»¥å¢å¼ºæ•°å€¼ç²¾åº¦å’Œæ¨ç†èƒ½åŠ›ã€‚  \nğŸ”¸æ„å»ºçœŸå®ä¸åˆæˆåŒç¯å¢ƒè®­ç»ƒæœºåˆ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿå¤šæ ·åŒ–åœºæ™¯æå‡ç­–ç•¥æ³›åŒ–æ€§ã€‚  \nğŸ”¸é‡‡ç”¨æ»‘åŠ¨çª—å£æœºåˆ¶ä½¿ä¼˜åŒ–å™¨èƒ½åˆ©ç”¨è¿‘æœŸåé¦ˆè¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸å®éªŒè¡¨æ˜DARAåœ¨é™ä½è¾¹é™…ROIæ–¹å·®ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶åœ¨åæœŸåˆ†é…ä¸­è¡¨ç°æ›´ä¼˜ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜åŒé˜¶æ®µæ¶æ„æ¯”å•é˜¶æ®µæ›´æœ‰æ•ˆï¼Œåˆ†ç¦»æ¨ç†ä¸ä¼˜åŒ–å¯é¿å…èƒ½åŠ›ç“¶é¢ˆã€‚  \nğŸ”¸å¼•å…¥RLå¾®è°ƒåæ€§èƒ½æ˜æ˜¾æå‡ï¼Œä¸”åœ¨åŒé˜¶æ®µç»“æ„ä¸­å¢ç›Šæ›´å¤§ï¼Œè¯´æ˜æ¶æ„ä¸å­¦ä¹ æ–¹æ³•ååŒä½œç”¨å¼ºã€‚  \nğŸ”¸æ•æ„Ÿæ€§åˆ†ææ˜¾ç¤ºæ–¹æ³•å¯¹æ—¶é—´å‘¨æœŸåˆ’åˆ†ä¸æ•æ„Ÿï¼Œåœ¨ä¸åŒç²’åº¦ä¸‹å‡ä¿æŒç¨³å®šä¼˜åŠ¿ã€‚  \nğŸ”¸GRPO-Adaptiveä¸­æ¯60è½®æ›´æ–°å‚è€ƒç­–ç•¥æ•ˆæœæœ€ä½³ï¼Œè¿‡é¢‘æˆ–è¿‡æ…¢æ›´æ–°å‡å½±å“æ”¶æ•›ç¨³å®šæ€§ã€‚  \n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡åˆ›æ–°åœ°å°†å¤§è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆï¼Œé€šè¿‡ä»»åŠ¡åˆ†è§£åŒ¹é…æ¨¡å‹ä¸“é•¿ã€‚åŒé˜¶æ®µè®¾è®¡åˆç†åˆ†å·¥ï¼šç¬¬ä¸€é˜¶æ®µé‡æ¨¡å¼è¿ç§»ï¼Œç¬¬äºŒé˜¶æ®µé‡ç²¾ç»†è°ƒæ§ã€‚GRPO-Adaptiveé€šè¿‡åŠ¨æ€é”šå®šå‚è€ƒç­–ç•¥ï¼Œè§£å†³äº†ä¼ ç»ŸKLæ­£åˆ™åŒ–ä¸­å› å›ºå®šåŸºå‡†å¯¼è‡´çš„å­¦ä¹ é€€åŒ–é—®é¢˜ã€‚æ•´ä½“æ¡†æ¶å…¼é¡¾å¯è§£é‡Šæ€§ä¸å®ç”¨æ€§ï¼Œä¸ºå°‘æ ·æœ¬å†³ç­–ä»»åŠ¡æä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning",
        "authors": [
            "Yao Lu",
            "Dengdong Fan",
            "Jianzheng Nie",
            "Fan Xu",
            "Jie Chen",
            "Bin Zhou",
            "Yonghong Tian"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14716v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14716v1",
        "summary": "We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.",
        "category": "å¼ºåŒ–å­¦ä¹ ",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14716v1ã€å¼ºåŒ–å­¦ä¹ ã€‘PCL-Reasoner-V1.5_ Advancing Math Reasoning with Offline Reinforcement Learning.pdf",
        "institution": "Peng Cheng Laboratoryã€Peking University",
        "note": "ğŸ“–æ ‡é¢˜ï¼šPCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning\nğŸŒæ¥æºï¼šarXiv, 2601.14716v1\n\nç¬”è®°æ ‡é¢˜ï¼šç¦»çº¿å¼ºåŒ–å­¦ä¹ æå‡æ•°å­¦æ¨ç†\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹\nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸ä¾èµ–åœ¨çº¿äº¤äº’çš„æƒ…å†µä¸‹ï¼Œé«˜æ•ˆç¨³å®šåœ°æå‡å¤§æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Ÿ\nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºä¸€ç§åŸºäºç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è®­ç»ƒæ–¹æ³•ï¼Œæ˜¾è‘—æå‡Qwen2.5-32Båœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä¸”è®­ç»ƒæ›´ç¨³å®šã€æ•ˆç‡æ›´é«˜ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯\nğŸ”¸ä»¥Qwen2.5-32Bä¸ºåŸºåº§æ¨¡å‹ï¼Œå…ˆé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¼•å…¥æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®ï¼Œæ„å»ºé«˜æ€§èƒ½åˆå§‹æ¨¡å‹PCL-Reasoner-V1ã€‚\nğŸ”¸è®¾è®¡ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼šSFTåæ¥ç¦»çº¿RLï¼Œé¿å…åœ¨çº¿RLä¸­æ¨ç†ä¸è®­ç»ƒè€¦åˆå¸¦æ¥çš„ä¸ç¨³å®šæ€§ã€‚\nğŸ”¸åœ¨ç¦»çº¿RLé˜¶æ®µï¼Œä½¿ç”¨SFTæ¨¡å‹å¯¹å›ºå®šé—®é¢˜é›†æ‰¹é‡ç”Ÿæˆå€™é€‰ç­”æ¡ˆï¼Œå¹¶åˆ©ç”¨éªŒè¯æ¨¡å‹æ‰“åˆ†å½¢æˆé™æ€å¥–åŠ±æ•°æ®é›†ã€‚\nğŸ”¸é‡‡ç”¨ç®€åŒ–çš„RLæŸå¤±å‡½æ•°ï¼ŒåŸºäºtokençº§æ¦‚ç‡çš„å‡ ä½•å¹³å‡è¿›è¡Œä¼˜åŒ–ï¼Œä¸ä½¿ç”¨é‡è¦æ€§é‡‡æ ·ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ã€‚\nğŸ”¸è®­ç»ƒè¿‡ç¨‹ä¸­è§£è€¦æ¨ç†ä¸è®­ç»ƒé˜¶æ®µï¼Œæ”¯æŒé«˜ååæ¨ç†æ¡†æ¶ï¼ˆå¦‚vLLMï¼‰å’Œé«˜æ•ˆè®­ç»ƒæ¡†æ¶ç‹¬ç«‹è¿è¡Œï¼Œæå‡å·¥ç¨‹å¯æ‰©å±•æ€§ã€‚\n\nğŸ”åˆ†ææ€»ç»“\nğŸ”¸PCL-Reasoner-V1.5åœ¨AIME 2024å’Œ2025ä¸Šåˆ†åˆ«è¾¾åˆ°90.9%å’Œ85.6%å‡†ç¡®ç‡ï¼Œè¶…è¶ŠåŒè§„æ¨¡æ¨¡å‹ï¼Œæˆä¸ºQwen2.5-32Bè¡ç”Ÿæ¨¡å‹ä¸­çš„SOTAã€‚\nğŸ”¸ç›¸æ¯”SFTæ¨¡å‹ï¼Œç¦»çº¿RLæ˜¾è‘—å¢åŠ äº†æ¨¡å‹å›ç­”çš„å¹³å‡é•¿åº¦ï¼ˆä»çº¦2.5ä¸‡tokenå¢è‡³è¿‘4ä¸‡ï¼‰ï¼Œè¡¨æ˜å…¶æ¨åŠ¨äº†æ›´æ·±å…¥çš„æ¨ç†è¿‡ç¨‹ã€‚\nğŸ”¸é•¿æ€ç»´é“¾ï¼ˆ>32K tokenï¼‰é—®é¢˜ä¸Šæ€§èƒ½å¤§å¹…æå‡ï¼Œè¯´æ˜ç¦»çº¿RLæœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹å¤„ç†å¤æ‚ã€å¤šæ­¥æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚\nğŸ”¸å®éªŒè¯æ˜ï¼Œå°½ç®¡ç¦»çº¿RLå—é™äºé™æ€æ•°æ®ï¼Œä½†åœ¨å¼ºåŸºçº¿æ¨¡å‹åŸºç¡€ä¸Šä»èƒ½å®ç°æ˜¾è‘—å¢ç›Šï¼Œä¸”è®­ç»ƒè¿‡ç¨‹æ›´ç¨³å®šã€èµ„æºåˆ©ç”¨ç‡æ›´é«˜ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹\nè¯¥è®ºæ–‡åˆ›æ–°æ€§åœ°å°†ç¦»çº¿RLåº”ç”¨äºå¤§æ¨¡å‹æ•°å­¦æ¨ç†ä¼˜åŒ–ï¼ŒæŒ‘æˆ˜äº†ä¸»æµä¾èµ–åœ¨çº¿RLçš„èŒƒå¼ã€‚å…¶æ ¸å¿ƒä»·å€¼åœ¨äºè¯æ˜ï¼šå³ä½¿æ²¡æœ‰åŠ¨æ€åé¦ˆå¾ªç¯ï¼Œä»…é€šè¿‡é«˜è´¨é‡é™æ€æ•°æ®å’Œç®€å•RLç›®æ ‡ï¼Œä¹Ÿèƒ½æ˜¾è‘—æå‡æ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæ–¹æ³•åœ¨è®­ç»ƒç¨³å®šæ€§ã€è®¡ç®—æ•ˆç‡å’Œå·¥ç¨‹å®ç°ä¸Šå…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œä¸ºå¤§è§„æ¨¡æ¨¡å‹æ¨ç†è®­ç»ƒæä¾›äº†æ›´å®ç”¨çš„æŠ€æœ¯è·¯å¾„ã€‚å°¤å…¶åœ¨å›½äº§NPUå¹³å°å®Œæˆå…¨éƒ¨å®éªŒï¼Œä¹Ÿä½“ç°äº†æŠ€æœ¯è‡ªä¸»æ€§ã€‚æœªæ¥è‹¥ç»“åˆè¿­ä»£å¼æ•°æ®æ›´æ–°ï¼Œæˆ–å¯è¿›ä¸€æ­¥çªç ´æ€§èƒ½è¾¹ç•Œã€‚\n    "
    },
    {
        "title": "CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation",
        "authors": [
            "Yutong Chen",
            "Jiandong Gao",
            "Ji Wu"
        ],
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14695v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14695v1",
        "summary": "Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM's ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM's reasoning ability.",
        "category": "å¼ºåŒ–å­¦ä¹ ",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14695v1ã€å¼ºåŒ–å­¦ä¹ ã€‘CoScale-RL_ Efficient Post-Training by Co-Scaling Data and Computation.pdf",
        "institution": "æ¸…åå¤§å­¦ã€æ¸…åå¤§å­¦ã€åŒ—äº¬ä¿¡æ¯ç§‘å­¦ä¸æŠ€æœ¯å›½å®¶ç ”ç©¶ä¸­å¿ƒ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šCoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation\nğŸŒæ¥æºï¼šarXiv, 2601.14695v1\n\nç¬”è®°æ ‡é¢˜ï¼šååŒæ‰©å±•æ•°æ®ä¸è®¡ç®—  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨å¼±åŸºç¡€æ¨¡å‹å’Œéš¾é—®é¢˜ä¸Šæå‡å¤§æ¨ç†æ¨¡å‹çš„åè®­ç»ƒæ•ˆç‡ä¸ç¨³å®šæ€§ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºCoScale-RLï¼Œé€šè¿‡åŒæ—¶æ‰©å±•æ¯é¢˜è§£æ³•æ•°é‡å’Œ rollout è®¡ç®—é‡ï¼Œæ˜¾è‘—æå‡æ•°æ®ä¸è®¡ç®—æ•ˆç‡ï¼Œçªç ´æ¨¡å‹èƒ½åŠ›è¾¹ç•Œã€‚  \n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸é¦–å…ˆåœ¨SFTé˜¶æ®µä¸ºæ¯ä¸ªé—®é¢˜æ”¶é›†å¤šä¸ªè§£æ³•ï¼Œè€Œéç®€å•å¢åŠ é—®é¢˜æ•°é‡ï¼Œä»¥æå‡å­¦ä¹ æœ‰æ•ˆæ€§ã€‚  \nğŸ”¸åœ¨RLé˜¶æ®µæ‰©å¤§æ¯é¢˜ç”Ÿæˆçš„rolloutæ•°é‡ï¼ˆRollout Nï¼‰ï¼Œé™ä½è®­ç»ƒå™ªå£°ï¼Œå¢å¼ºæ¢¯åº¦ç¨³å®šæ€§ã€‚  \nğŸ”¸å¼•å…¥Re-distillationæŠ€æœ¯åˆå¹¶ä¸åŒRLè¿›ç¨‹çš„ç»“æœï¼Œå®ç°é«˜æ•ˆæ¨¡å‹èåˆå¹¶é¿å…ç¾éš¾æ€§é—å¿˜ã€‚  \nğŸ”¸é‡‡ç”¨åˆ†ç»„è®­ç»ƒç­–ç•¥ï¼Œæ ¹æ®ä¸åŒé—®é¢˜éš¾åº¦åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œä¿æŒæ•´ä½“è®¡ç®—æ•ˆç‡æœ€ä¼˜ã€‚  \nğŸ”¸é€šè¿‡ç†è®ºåˆ†æè¯æ˜è®¡ç®—æ•ˆç‡æ˜¯å­¦ä¹ ç‡ä¸Rollout Næ¯”å€¼çš„äºŒæ¬¡å‡½æ•°ï¼ŒæŒ‡å¯¼è¶…å‚æ•°è°ƒä¼˜ã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸å®éªŒæ˜¾ç¤ºCoScale-RLåœ¨å››ä¸ªæ•°å­¦åŸºå‡†ä¸Šå¹³å‡å‡†ç¡®ç‡æå‡3.76å€ï¼Œæ˜¾è‘—ä¼˜äºå„ç±»åŸºçº¿æ–¹æ³•ã€‚  \nğŸ”¸ä»…æ‰©å±•æ•°æ®æˆ–è®¡ç®—å•ä¸€ç»´åº¦æ•ˆæœæœ‰é™ï¼ŒåŒæ‰©å±•ååŒæ‰èƒ½é‡Šæ”¾æ¨¡å‹æ½œåŠ›ã€‚  \nğŸ”¸å¤šè§£æ³•SFTèƒ½æœ‰æ•ˆå°†åŸæœ¬ä¸å¯è§£çš„é—®é¢˜å˜ä¸ºå¯è§£ï¼Œä¸”ä¸ä¾èµ–å¤§è§„æ¨¡SFTæ•°æ®é›†ã€‚  \nğŸ”¸åˆ†ç»„è°ƒæ•´Rollout Nå¯åœ¨ç›¸åŒç”šè‡³æ›´å°‘è®¡ç®—é‡ä¸‹è·å¾—æ›´é«˜æ€§èƒ½ï¼ŒéªŒè¯äº†è®¡ç®—æ•ˆç‡ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚  \nğŸ”¸è¯¥æ–¹æ³•å¯¹0.5Bå°æ¨¡å‹åŒæ ·æœ‰æ•ˆï¼ŒæˆåŠŸè®­ç»ƒå‡ºå¤„ç†é•¿è¾¾ä¸‡tokenæ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚  \n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°åœ°æå‡ºâ€œæ•°æ®ä¸è®¡ç®—å…±æ‰©å±•â€çš„åè®­ç»ƒèŒƒå¼ï¼Œæ‰“ç ´ä¼ ç»Ÿå•ç‹¬æ‰©æ•°æ®æˆ–ç®—åŠ›çš„æ€è·¯ã€‚å…¶æ ¸å¿ƒæ´å¯Ÿâ€”â€”å•è§£æ³•ä¸è¶³ä»¥æ”¯æ’‘å¤æ‚æ¨ç†å­¦ä¹ â€”â€”å…·æœ‰å¯å‘æ€§ã€‚ç»“åˆRe-distillationå®ç°é«˜æ•ˆæ¨¡å‹èåˆï¼Œä¸ºRLç¨³å®šæ€§é—®é¢˜æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œå…·å¤‡è¾ƒå¼ºå¯è¿ç§»æ€§å’Œå·¥ç¨‹ä»·å€¼ã€‚\n    "
    },
    {
        "title": "AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving",
        "authors": [
            "Zecong Tang",
            "Zixu Wang",
            "Yifei Wang",
            "Weitong Lian",
            "Tianjian Gao",
            "Haoran Li",
            "Tengju Ru",
            "Lingyi Meng",
            "Zhejun Cui",
            "Yichen Zhu",
            "Qi Kang",
            "Kaixuan Wang",
            "Yu Zhang"
        ],
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14702v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14702v1",
        "summary": "Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.",
        "category": "AutoDriDMæ•°æ®é›†",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14702v1ã€AutoDriDMæ•°æ®é›†ã€‘AutoDriDM_ An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving.pdf",
        "institution": "æµ™æ±Ÿå¤§å­¦ã€é¦™æ¸¯å¤§å­¦",
        "note": "ğŸ“–æ ‡é¢˜ï¼šAutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving\nğŸŒæ¥æºï¼šarXiv, 2601.14702v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºå†³ç­–å¯¼å‘çš„è‡ªåŠ¨é©¾é©¶è¯„æµ‹\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•å…¨é¢è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„å†³ç­–èƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºAutoDriDMï¼Œé¦–ä¸ªèšç„¦å†³ç­–èƒ½åŠ›ã€å…·å¤‡å¯è§£é‡Šæ€§åˆ†æçš„å¤šå±‚çº§è¯„æµ‹åŸºå‡†ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸åŸºäºnuScenesã€KITTIå’ŒBDD100Kæ„å»ºåŒ…å«6,650ä¸ªé—®é¢˜çš„åŸºå‡†ï¼Œæ¶µç›–ç‰©ä½“ã€åœºæ™¯ä¸å†³ç­–ä¸‰ä¸ªé€’è¿›å±‚çº§ã€‚  \nğŸ”¸è®¾è®¡å…­é¡¹é€‰æ‹©é¢˜ä»»åŠ¡ï¼Œåˆ†åˆ«è¯„ä¼°å…³é”®ç‰©ä½“è¯†åˆ«ã€çŠ¶æ€åˆ¤æ–­ã€ç¯å¢ƒç†è§£åŠå®‰å…¨å†³ç­–ç­‰èƒ½åŠ›ï¼Œå¹¶æŒ‰é£é™©ç­‰çº§åˆ’åˆ†æ•°æ®é›†ã€‚  \nğŸ”¸å¼•å…¥ç›¸ä¼¼åœºæ™¯å¯¹æµ‹è¯•æ¨¡å‹é²æ£’æ€§ï¼Œæ£€éªŒå…¶æ˜¯å¦ä¾èµ–å› æœæ¨ç†è€Œéè¡¨é¢ç‰¹å¾ã€‚  \nğŸ”¸é€šè¿‡é“¾å¼æ€ç»´æç¤ºè·å–æ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼Œå®šä¹‰ä¹ç±»é”™è¯¯æ¨¡å¼å¹¶äººå·¥æ ‡æ³¨ï¼Œç”¨äºæ·±å…¥åˆ†æå¤±è´¥åŸå› ã€‚  \nğŸ”¸è®­ç»ƒä¸€ä¸ª7Bå‚æ•°çš„è½»é‡çº§åˆ†ææ¨¡å‹ï¼Œè‡ªåŠ¨è¯†åˆ«æ¨ç†é”™è¯¯ç±»å‹ï¼Œå®ç°å¤§è§„æ¨¡å¯è§£é‡Šæ€§è¯„ä¼°ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸ä¸»æµVLMåœ¨æ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å†³ç­–èƒ½åŠ›æå‡æœ‰é™ï¼Œä¸”æ„ŸçŸ¥ä¸å†³ç­–å¾—åˆ†é—´ç›¸å…³æ€§å¼±ï¼Œè¯´æ˜èƒ½åŠ›æœªæœ‰æ•ˆè½¬åŒ–ã€‚  \nğŸ”¸é«˜é£é™©åœºæ™¯ä¸‹å¤§æ¨¡å‹å†³ç­–è¡¨ç°æ›´ä¼˜ï¼Œå°æ¨¡å‹åˆ™æ³¢åŠ¨è¾ƒå¤§ï¼Œæ˜¾ç¤ºè§„æ¨¡å¯¹å¤æ‚æƒ…å¢ƒå¤„ç†çš„é‡è¦æ€§ã€‚  \nğŸ”¸åœ¨è§†è§‰ç›¸ä¼¼ä½†å†³ç­–ä¸åŒçš„åœºæ™¯ä¸­ï¼Œå¤šæ•°æ¨¡å‹è”åˆå‡†ç¡®ç‡æ˜¾è‘—ä½äºé¢„æœŸï¼Œæš´éœ²å…¶ç¼ºä¹å› æœæ¨ç†èƒ½åŠ›ã€‚  \nğŸ”¸InternVLç³»åˆ—ä¸­38Bæ¨¡å‹å‡ºç°æ€§èƒ½æ–­å±‚ï¼Œè¡¨æ˜å‚æ•°å¢é•¿ä¸æ€»å¸¦æ¥æå‡ï¼Œå¯èƒ½å­˜åœ¨æ¨ç†æœºåˆ¶å¤±é…ã€‚  \nğŸ”¸é”™è¯¯åˆ†ææ˜¾ç¤ºé€»è¾‘é”™è¯¯ã€è¯­ä¹‰é—æ¼å’Œå¹»è§‰æ˜¯ä¸»è¦é—®é¢˜ï¼Œå³ä½¿ç­”æ¡ˆæ­£ç¡®ï¼Œæ¨ç†è¿‡ç¨‹ä¹Ÿå¸¸å­˜åœ¨ç¼ºé™·ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†è¯„æµ‹é‡å¿ƒä»æ„ŸçŸ¥è½¬å‘å†³ç­–ï¼Œæ„å»ºäº†ç»“æ„åŒ–ã€å¯è§£é‡Šçš„è¯„ä¼°ä½“ç³»ã€‚é€šè¿‡å¤šå±‚çº§ä»»åŠ¡è®¾è®¡å’Œç»†ç²’åº¦é”™è¯¯åˆ†ç±»ï¼Œæ­ç¤ºäº†å½“å‰VLMåœ¨è‡ªåŠ¨é©¾é©¶åº”ç”¨ä¸­çš„æ ¸å¿ƒçŸ­æ¿â€”â€”æ— æ³•å°†æ„ŸçŸ¥ç»“æœè½¬åŒ–ä¸ºå¯é å†³ç­–ã€‚æå‡ºçš„è‡ªåŠ¨åŒ–åˆ†ææ¨¡å‹ä¸ºåç»­ç ”ç©¶æä¾›äº†é«˜æ•ˆå·¥å…·ï¼Œæ¨åŠ¨å‘æ›´å®‰å…¨ã€å¯ä¿¡èµ–çš„æ™ºèƒ½é©¾é©¶ç³»ç»Ÿå‘å±•ã€‚\n    "
    },
    {
        "title": "ClaimDB: A Fact Verification Benchmark over Large Structured Data",
        "authors": [
            "Michael Theologitis",
            "Preetam Prabhu Srikar Dammu",
            "Chirag Shah",
            "Dan Suciu"
        ],
        "categories": [
            "cs.CL"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14698v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14698v1",
        "summary": "Despite substantial progress in fact-verification benchmarks, claims grounded in large-scale structured data remain underexplored. In this work, we introduce ClaimDB, the first fact-verification benchmark where the evidence for claims is derived from compositions of millions of records and multiple tables. ClaimDB consists of 80 unique real-life databases covering a wide range of domains, from governance and healthcare to media, education and the natural sciences. At this scale, verification approaches that rely on \"reading\" the evidence break down, forcing a timely shift toward reasoning in executable programs. We conduct extensive experiments with 30 state-of-the-art proprietary and open-source (below 70B) LLMs and find that none exceed 83% accuracy, with more than half below 55%. Our analysis also reveals that both closed- and open-source models struggle with abstention -- the ability to admit that there is no evidence to decide -- raising doubts about their reliability in high-stakes data analysis. We release the benchmark, code, and the LLM leaderboard at https://claimdb.github.io .",
        "category": "å¤§æ¨¡å‹è¯„æµ‹åŸºå‡†",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14698v1ã€å¤§æ¨¡å‹è¯„æµ‹åŸºå‡†ã€‘ClaimDB_ A Fact Verification Benchmark over Large Structured Data.pdf",
        "institution": "University of Washington",
        "note": "ğŸ“–æ ‡é¢˜ï¼šClaimDB: A Fact Verification Benchmark over Large Structured Data\nğŸŒæ¥æºï¼šarXiv, 2601.14698v1\n\nç¬”è®°æ ‡é¢˜ï¼šé¦–ä¸ªå¤§è§„æ¨¡ç»“æ„åŒ–æ•°æ®äº‹å®éªŒè¯åŸºå‡†  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•æ„å»ºä¸€ä¸ªèƒ½è¯„ä¼°å¤§æ¨¡å‹åœ¨æµ·é‡ç»“æ„åŒ–æ•°æ®ä¸Šè¿›è¡Œäº‹å®éªŒè¯èƒ½åŠ›çš„åŸºå‡†ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºäº†CLAIMDBï¼Œé¦–ä¸ªä»¥çœŸå®å¤šè¡¨æ•°æ®åº“ï¼ˆå¹³å‡460ä¸‡è®°å½•ï¼‰ä¸ºè¯æ®æºçš„äº‹å®éªŒè¯åŸºå‡†ï¼Œæ¨åŠ¨éªŒè¯èŒƒå¼ä»â€œé˜…è¯»å¼æ¨ç†â€è½¬å‘â€œå¯æ‰§è¡Œç¨‹åºå¼æ¨ç†â€ã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸åŸºäºBIRDæ•°æ®é›†çš„11k NL2SQLæ ·æœ¬ï¼Œç­›é€‰å«èšåˆã€æ’åºã€å¤šè¡¨è¿æ¥ç­‰éœ€ç»„åˆæ¨ç†çš„SQLæŸ¥è¯¢ï¼Œç¡®ä¿è¯æ®è§„æ¨¡è¿œè¶…LLMä¸Šä¸‹æ–‡çª—å£ã€‚  \nğŸ”¸å¯¹æ¯ä¸ªSQLæ‰§è¡Œç»“æœç”Ÿæˆä¸‰ç±»è‡ªç„¶è¯­è¨€å£°æ˜ï¼šè•´å«å‹ï¼ˆentailedï¼‰ã€çŸ›ç›¾å‹ï¼ˆcontradictedï¼‰å’Œä¿¡æ¯ä¸è¶³å‹ï¼ˆNEIï¼‰ï¼Œå…¶ä¸­NEIç»†åˆ†ä¸ºâ€œè¶…æ¨¡å¼â€â€œä¸»è§‚â€â€œåäº‹å®â€ä¸‰ç±»ã€‚  \nğŸ”¸é‡‡ç”¨gpt-5è‡ªåŠ¨ç”Ÿæˆå£°æ˜ï¼Œå¹¶é€šè¿‡ç”±Phi-4ã€grok-3-miniã€mistral-smallç»„æˆçš„LLMæ³•å®˜å°ç»„è¿›è¡Œä¸‰é‡è´¨é‡å®¡æ ¸ï¼Œä¸¥æ§æ ‡ç­¾æ­£ç¡®æ€§ã€è‡ªåŒ…å«æ€§ä¸æ¨¡å¼æ³„éœ²ã€‚  \nğŸ”¸å¯¹NEIå£°æ˜å¼•å…¥è¯­ä¹‰ç›¸ä¼¼åº¦ç­›é€‰ï¼ˆåŸºäºåµŒå…¥ï¼‰ï¼Œä¿ç•™ä¸æ•°æ®åº“æ¦‚å¿µâ€œè´´è¿‘â€çš„æ ·æœ¬ï¼Œé¿å…è¿‡äºæ˜æ˜¾æˆ–è„±ç¦»å®é™…çš„å£°æ˜å¹²æ‰°è¯„ä¼°ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸30ä¸ªä¸»æµLLMåœ¨CLAIMDBä¸Šè¡¨ç°æ™®éå—é™ï¼šæ— ä¸€æ¨¡å‹å‡†ç¡®ç‡è¶…83%ï¼Œè¶…åŠæ•°ä½äº55%ï¼Œå‡¸æ˜¾å½“å‰æ¨¡å‹å¤„ç†å¤§è§„æ¨¡ç»“æ„åŒ–æ•°æ®çš„èƒ½åŠ›ç“¶é¢ˆã€‚  \nğŸ”¸æ‰€æœ‰æ¨¡å‹å‡ä¸¥é‡è¯¯åˆ¤NEIç±»åˆ«ï¼šé—­æºæ¨¡å‹æ˜¾è‘—å›é¿ abstentionï¼ˆNEIé¢„æµ‹ç‡è¿‘0%ï¼‰ï¼Œå¼€æºæ¨¡å‹åˆ™è¿‡åº¦é¢„æµ‹NEIï¼ˆé«˜è¾¾90%ä»¥ä¸Šé”™è¯¯å½’å› ï¼‰ï¼Œåæ˜ å…¶å¯¹ä¸ç¡®å®šæ€§è¯†åˆ«èƒ½åŠ›è–„å¼±ã€‚  \nğŸ”¸æ€§èƒ½ä¸å·¥å…·è°ƒç”¨æ¬¡æ•°å‘ˆå€’Uå‹å…³ç³»ï¼šæœ€ä¼˜æ¨¡å‹å¹³å‡è°ƒç”¨4â€“8æ¬¡SQLï¼Œè¿‡å°‘å¯¼è‡´ç›²çŒœï¼Œè¿‡å¤šå¼•å‘æ³¨æ„åŠ›å´©æºƒä¸å™ªå£°ç´¯ç§¯ï¼ŒéªŒè¯äº†äº¤äº’å¼æ¨ç†çš„è„†å¼±æ€§ã€‚  \nğŸ”¸å¼€æºæ¨¡å‹æ•´ä½“è½åæ˜æ˜¾ï¼šé™¤gpt-oss-20bå¤–ï¼Œå…¶ä½™20ä¸ªå¼€æºæ¨¡å‹å‡†ç¡®ç‡å‡æœªè¾¾68%ï¼Œè¡¨æ˜å…¶åœ¨æ•°æ®åº“æ„ŸçŸ¥ä¸å¯æ‰§è¡Œæ¨ç†æ–¹é¢å­˜åœ¨ç³»ç»Ÿæ€§å·®è·ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†äº‹å®éªŒè¯ä»»åŠ¡é”šå®šåœ¨çœŸå®ã€å¤æ‚ã€å¤§è§„æ¨¡ç»“æ„åŒ–æ•°æ®ä¸Šï¼Œçªç ´äº†ä»¥å¾€ä¾èµ–å°è¡¨æ ¼æˆ–æ–‡æœ¬ç‰‡æ®µçš„ç®€åŒ–èŒƒå¼ï¼›å…¶ä¸¥è°¨çš„æ„é€ æµç¨‹ï¼ˆä»SQLç­›é€‰â†’å£°æ˜ç”Ÿæˆâ†’å¤šæ³•å®˜å®¡æ ¸â†’è¯­ä¹‰è¿‡æ»¤ï¼‰ä¸ºé«˜ä¿çœŸæ•°æ®å¯†é›†å‹åŸºå‡†è®¾è®¡æ ‘ç«‹äº†æ–°æ ‡å‡†ï¼›å°¤å…¶å¯¹NEIç±»åˆ«çš„ç²¾ç»†åŒ–å®šä¹‰ä¸è¯„ä¼°ï¼Œç›´æŒ‡å¤§æ¨¡å‹åœ¨ç°å®å†³ç­–ä¸­â€œçŸ¥ä¹‹ä¸ºçŸ¥ä¹‹ï¼Œä¸çŸ¥ä¸ºä¸çŸ¥â€çš„å…³é”®å¯é æ€§ç¼ºå£ã€‚\n    "
    },
    {
        "title": "LaVR: Scene Latent Conditioned Generative Video Trajectory Re-Rendering using Large 4D Reconstruction Models",
        "authors": [
            "Mingyang Xie",
            "Numair Khan",
            "Tianfu Wang",
            "Naina Dhingra",
            "Seonghyeon Nam",
            "Haitao Yang",
            "Zhuo Hui",
            "Christopher Metzler",
            "Andrea Vedaldi",
            "Hamed Pirsiavash",
            "Lei Luo"
        ],
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14674v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14674v1",
        "summary": "Given a monocular video, the goal of video re-rendering is to generate views of the scene from a novel camera trajectory. Existing methods face two distinct challenges. Geometrically unconditioned models lack spatial awareness, leading to drift and deformation under viewpoint changes. On the other hand, geometrically-conditioned models depend on estimated depth and explicit reconstruction, making them susceptible to depth inaccuracies and calibration errors.\n  We propose to address these challenges by using the implicit geometric knowledge embedded in the latent space of a large 4D reconstruction model to condition the video generation process. These latents capture scene structure in a continuous space without explicit reconstruction. Therefore, they provide a flexible representation that allows the pretrained diffusion prior to regularize errors more effectively. By jointly conditioning on these latents and source camera poses, we demonstrate that our model achieves state-of-the-art results on the video re-rendering task. Project webpage is https://lavr-4d-scene-rerender.github.io/",
        "category": "å¤§æ¨¡å‹æ¶æ„",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14674v1ã€å¤§æ¨¡å‹æ¶æ„ã€‘LaVR_ Scene Latent Conditioned Generative Video Trajectory Re-Rendering using Large 4D Reconstruction Models.pdf",
        "institution": "Meta Reality Labsã€University of Oxford",
        "note": "ğŸ“–æ ‡é¢˜ï¼šLaVR: Scene Latent Conditioned Generative Video Trajectory Re-Rendering using Large 4D Reconstruction Models\nğŸŒæ¥æºï¼šarXiv, 2601.14674v1\n\nç¬”è®°æ ‡é¢˜ï¼šéšå¼4Dæ½œç å¼•å¯¼è§†é¢‘é‡æ¸²æŸ“  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨å•ç›®è§†é¢‘è¾“å…¥ä¸‹ç”Ÿæˆå‡ ä½•ä¸€è‡´ã€è§†è§‰é«˜è´¨é‡çš„æ–°è§†è§’è§†é¢‘è½¨è¿¹ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºåˆ©ç”¨é¢„è®­ç»ƒå¤§4Dé‡å»ºæ¨¡å‹ï¼ˆLRMï¼‰çš„éšå¼åœºæ™¯æ½œç ä½œä¸ºè½¯å‡ ä½•å…ˆéªŒï¼Œæ¡ä»¶åŒ–è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå…¼é¡¾å‡ ä½•ä¿çœŸä¸è§†è§‰è´¨é‡ã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸é‡‡ç”¨CUT3Rç­‰å¤§å‹4Dé‡å»ºæ¨¡å‹æå–æºè§†é¢‘çš„æ—¶åºä¸€è‡´æ½œç ï¼ˆstokensï¼‰ï¼Œç¼–ç å¤–è§‚ã€å‡ ä½•ä¸åŠ¨æ€ä¿¡æ¯ï¼Œé¿å…æ˜¾å¼æ·±åº¦ä¼°è®¡ä¸ç‚¹äº‘é‡å»ºã€‚  \nğŸ”¸è®¾è®¡è½»é‡çº§CUT3Ré€‚é…å™¨ï¼Œé€šè¿‡å¸§é‡‡æ ·ï¼ˆkï¼‰ã€é€šé“åˆ†ç»„ï¼ˆmï¼‰å’ŒTransformerè§£ç ï¼Œå°†é«˜ç»´æ½œç å‹ç¼©ä¸ºä¸è§†é¢‘VAEæ½œç å¯¹é½çš„æ—¶ç©ºç‰¹å¾ï¼Œæ— ç¼æ³¨å…¥æ‰©æ•£Transformerï¼ˆDiTï¼‰ã€‚  \nğŸ”¸è”åˆæ¡ä»¶åŒ–ï¼šä»¥CUT3Ræ½œç ã€æºç›¸æœºä½å§¿ã€æ–‡æœ¬æè¿°ä¸ºè¾“å…¥æ¡ä»¶ï¼Œä»¥ç›®æ ‡è½¨è¿¹ä½å§¿ä¸ºæ§åˆ¶ä¿¡å·ï¼Œå¼•å¯¼æ‰©æ•£è¿‡ç¨‹æ²¿æŒ‡å®šè·¯å¾„ç”Ÿæˆè§†é¢‘ã€‚  \nğŸ”¸ä»…å¾®è°ƒDiTçš„æŠ•å½±å±‚ä¸è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå†»ç»“å…¶ä½™å‚æ•°ï¼ˆå«VAEä¸ä¸»å¹²ï¼‰ï¼Œä¿ç•™å¤§è§„æ¨¡è§†é¢‘å…ˆéªŒï¼Œæå‡é²æ£’æ€§ä¸è®­ç»ƒæ•ˆç‡ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨VBenchå¤šæŒ‡æ ‡è¯„æµ‹ä¸­ï¼Œæœ¬æ–¹æ³•åœ¨å¤šè§†è§’ä¸€è‡´æ€§ã€ä¸»ä½“ä¸€è‡´æ€§ã€èƒŒæ™¯ä¸€è‡´æ€§ä¸Šå‡è¾¾SOTAï¼Œæ˜¾è‘—ä¼˜äºç‚¹äº‘æ¡ä»¶æ³•ï¼ˆTrajectoryCrafter/Gen3Cï¼‰ä¸æ— å‡ ä½•æ¡ä»¶æ³•ï¼ˆReCamMasterï¼‰ã€‚  \nğŸ”¸ç›®æ ‡ä½å§¿é‡å»ºè¯¯å·®æœ€ä½ï¼ˆå¹³ç§»Abs(t)=14.39mmï¼Œæ—‹è½¬Rel(R)=0.411Â°ï¼‰ï¼Œè¯æ˜æ½œç æ¡ä»¶èƒ½æ›´ç²¾å‡†è·Ÿè¸ªç›®æ ‡è½¨è¿¹ï¼Œå…‹æœæ·±åº¦è¯¯å·®ä¼ æ’­é—®é¢˜ã€‚  \nğŸ”¸å¾ªç¯ä¸€è‡´æ€§å®éªŒæ˜¾ç¤ºï¼Œæœ¬æ–¹æ³•åœ¨é™æ€åœºæ™¯é‡è®¿åŒä¸€ä½å§¿æ—¶é‡å»ºè¯¯å·®æœ€å°ï¼ŒéªŒè¯å…¶å‡ ä½•ç»“æ„ç¨³å®šæ€§å¼ºäºåŸºçº¿ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜k=4ã€m=2çš„é€‚é…å™¨é…ç½®æœ€ä¼˜ï¼Œåœ¨è®¡ç®—å¼€é”€å¯æ§å‰æä¸‹å®ç°æœ€ä½³å‡ ä½•-è§†è§‰å¹³è¡¡ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°åœ¨äºè·³å‡ºâ€œæ˜¾å¼é‡å»ºâ†’æ¸²æŸ“â†’ç”Ÿæˆâ€çš„ä¼ ç»ŸèŒƒå¼ï¼Œè½¬è€ŒæŒ–æ˜å¤§4Dé‡å»ºæ¨¡å‹æ½œç©ºé—´ä¸­è•´å«çš„éšå¼ã€è¿ç»­ã€é²æ£’çš„å‡ ä½•çŸ¥è¯†ï¼Œå°†å…¶ä½œä¸ºâ€œè½¯çº¦æŸâ€æ³¨å…¥ç”Ÿæˆè¿‡ç¨‹ï¼›è¯¥è®¾è®¡æ—¢è§„é¿äº†æ·±åº¦ä¼°è®¡å™ªå£°ä¸ç‚¹äº‘å¤±çœŸï¼Œåˆèµ‹äºˆæ‰©æ•£æ¨¡å‹å¯ä¿®æ­£å±€éƒ¨ä¸ä¸€è‡´çš„çµæ´»æ€§ï¼Œæ˜¯å‡ ä½•å¼•å¯¼ç”ŸæˆèŒƒå¼çš„å®è´¨æ€§çªç ´ã€‚\n    "
    },
    {
        "title": "MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks",
        "authors": [
            "Zixuan Ke",
            "Yifei Ming",
            "Austin Xu",
            "Ryan Chin",
            "Xuan-Phi Nguyen",
            "Prathyusha Jwalapuram",
            "Semih Yavuz",
            "Caiming Xiong",
            "Shafiq Joty"
        ],
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.MA"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14652v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14652v1",
        "summary": "While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.",
        "category": "Agent",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14652v1ã€Agentã€‘MAS-Orchestra_ Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks.pdf",
        "institution": "Salesforce Researchã€Massachusetts Institute of Technology",
        "note": "ğŸ“–æ ‡é¢˜ï¼šMAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks\nğŸŒæ¥æºï¼šarXiv, 2601.14652v1\n\nç¬”è®°æ ‡é¢˜ï¼š holistic orchestration for MAS  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è®¾è®¡ä¸€ç§æ—¢èƒ½æå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰æ€§èƒ½ï¼Œåˆèƒ½ç§‘å­¦ç†è§£å…¶ç›¸å¯¹äºå•æ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆSASï¼‰ä¼˜åŠ¿è¾¹ç•Œçš„è‡ªåŠ¨MASæ„å»ºæ–¹æ³•ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºMAS-Orchestraæ¡†æ¶â€”â€”é¦–ä¸ªå°†MASç¼–æ’å»ºæ¨¡ä¸ºå‡½æ•°è°ƒç”¨å¼å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„è®­ç»ƒæ—¶æ•´ä½“åŒ–ç¼–æ’æ–¹æ³•ï¼Œå¹¶é…å¥—å‘å¸ƒå¯æ§äº”ç»´åŸºå‡†MASBENCHï¼Œç³»ç»Ÿæ­ç¤ºMASæ”¶ç›Šçš„æ¡ä»¶æ€§æœ¬è´¨ã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å°†MASç¼–æ’æŠ½è±¡ä¸ºå‡½æ•°è°ƒç”¨RLé—®é¢˜ï¼Œå­æ™ºèƒ½ä½“å°è£…ä¸ºé»‘ç›’å¯è°ƒç”¨å‡½æ•°ï¼ˆcreate_agent/create_flowï¼‰ï¼Œéšè—å†…éƒ¨æ‰§è¡Œç»†èŠ‚ï¼Œä»…æš´éœ²ç›®æ ‡ä¸æ¥å£ã€‚  \nğŸ”¸å¼•å…¥â€œMASç¨‹åº¦ï¼ˆDoMï¼‰â€æ˜¾å¼æ§åˆ¶æ™ºèƒ½ä½“æ•°é‡ä¸æ‹“æ‰‘å¤æ‚åº¦ï¼Œæ”¯æŒlowï¼ˆâ‰¤1 agentï¼‰ä¸highï¼ˆæ— çº¦æŸï¼‰ä¸¤ç§é…ç½®ï¼Œå®ç°ä»»åŠ¡é€‚é…å‹éƒ¨ç½²ã€‚  \nğŸ”¸é‡‡ç”¨æ•´ä½“åŒ–ç¼–æ’ï¼ˆholistic orchestrationï¼‰ï¼šåœ¨å•æ­¥å†³ç­–ä¸­ç”Ÿæˆå®Œæ•´MASç»“æ„ï¼ˆå«æ‰€æœ‰å­æ™ºèƒ½ä½“ã€è¿æ¥å…³ç³»ä¸èšåˆé€»è¾‘ï¼‰ï¼Œè€Œéé€æ­¥æ·»åŠ ï¼Œä½¿ç¼–æ’å™¨å…·å¤‡å…¨å±€ç³»ç»Ÿçº§æ¨ç†èƒ½åŠ›ã€‚  \nğŸ”¸æ„å»ºMASBENCHåŸºå‡†ï¼Œæ²¿Depthã€Horizonã€Breadthã€Parallelã€Robustnessäº”è½´åˆ»ç”»ä»»åŠ¡ç»“æ„ä¸éªŒè¯åè®®ï¼Œå®ç°å¯¹MAS/SASå·®å¼‚çš„å¯æ§å½’å› åˆ†æã€‚  \nğŸ”¸åŸºäºMASBENCHå¼€å±•ä¸‰å‘åˆ†æï¼šä»»åŠ¡ç»“æ„å½±å“ã€ç¼–æ’å™¨èƒ½åŠ›å½±å“ã€å­æ™ºèƒ½ä½“èƒ½åŠ›å½±å“ï¼Œè¯†åˆ«MASå¢ç›Šçš„å…³é”®è¾¹ç•Œæ¡ä»¶ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸MASå¢ç›Šéæ™®é€‚ï¼šä»…åœ¨å­æ™ºèƒ½ä½“èƒ½åŠ›å¤„äºâ€œä¸´ç•ŒåŒºâ€ï¼ˆå¤Ÿå¼ºä½†æœªè¶³å¤Ÿå¼ºï¼‰æ—¶æ˜¾è‘—ï¼›å½“å­æ™ºèƒ½ä½“è¿‡å¼ºæ—¶ï¼Œåè°ƒå¼€é”€ä¸é”™è¯¯ä¼ æ’­åè€ŒæŠµæ¶ˆæ”¶ç›Šã€‚  \nğŸ”¸ä»»åŠ¡ç»“æ„å†³å®šMASé€‚ç”¨æ€§ï¼šParallelä¸Robustnessè½´ä¸ŠMASæŒç»­é¢†å…ˆï¼›Depthè½´ï¼ˆå¼ºåºåˆ—ä¾èµ–ï¼‰ä¸‹SASæ›´ä¼˜ï¼›Horizonè½´å¢ç›Šéšå­æ™ºèƒ½ä½“èƒ½åŠ›å¢å¼ºè€Œè¡°å‡ã€‚  \nğŸ”¸é²æ£’æ€§æ˜¯MASæœ€ç¨³å®šä¼˜åŠ¿ï¼šåœ¨å¯¹æŠ—æ€§å¹²æ‰°ï¼ˆå¦‚é”™è¯¯ä¸­é—´ä¿¡æ¯æ³¨å…¥ï¼‰ä¸‹ï¼ŒMASé€šè¿‡å†—ä½™ã€äº¤å‰éªŒè¯ä¸ç»“æ„åŒ–åˆ†å·¥æ˜¾è‘—ä¼˜äºSASï¼Œä¸”è¯¥ä¼˜åŠ¿ä¸éšå­æ™ºèƒ½ä½“æ¨ç†å¼ºåº¦æå‡è€Œå‡å¼±ã€‚  \nğŸ”¸ç¼–æ’å™¨ç±»å‹è‡³å…³é‡è¦ï¼šæŒ‡ä»¤å¾®è°ƒLLMï¼ˆå¦‚Qwen-7bï¼‰ä½œä¸ºç¼–æ’å™¨æ˜¾è‘—ä¼˜äºåŒè§„æ¨¡æ¨ç†ä¼˜åŒ–LLMï¼ˆRLMï¼‰ï¼Œå› å…¶æ›´æ“…é•¿ä»»åŠ¡åˆ†è§£ä¸å§”æ‰˜ï¼Œè€Œéç›´æ¥æ±‚è§£ã€‚  \nğŸ”¸MASBENCHéªŒè¯äº†â€œç»“æ„å¯¹é½â€ç°è±¡ï¼šç¼–æ’å™¨èƒ½è‡ªä¸»å­¦ä¹ åŒ¹é…ä»»åŠ¡ç»“æ„ï¼ˆå¦‚Parallelå€¼é«˜â†’ç”Ÿæˆæ›´å¤šå¹¶è¡Œå­æ™ºèƒ½ä½“ï¼‰ï¼Œä½†ç»“æ„åŒ¹é…ä¸å¿…ç„¶å¸¦æ¥æ€§èƒ½æå‡ï¼Œéœ€ç»“åˆèƒ½åŠ›åŒ¹é…ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºæ‰“ç ´â€œMASå³æ›´å¥½â€çš„éšå«å‡è®¾ï¼Œä»¥å¯è§£é‡Šã€å¯æ§åˆ¶ã€å¯å¤ç°çš„æ–¹å¼ï¼Œå°†MASç ”ç©¶ä»ç»éªŒé©±åŠ¨è½¬å‘ç§‘å­¦èŒƒå¼ï¼šé€šè¿‡DoMæ˜¾å¼å»ºæ¨¡åè°ƒç²’åº¦ï¼Œé€šè¿‡å‡½æ•°è°ƒç”¨æŠ½è±¡è§£è€¦ç³»ç»Ÿè®¾è®¡ä¸å­æ™ºèƒ½ä½“å®ç°ï¼Œé€šè¿‡äº”ç»´åŸºå‡†å®ç°å¤šå› ç´ å½’å› åˆ†æï¼Œæœ€ç»ˆå½¢æˆâ€œç†è§£â€”å»ºæ¨¡â€”éªŒè¯â€”ä¼˜åŒ–â€çš„é—­ç¯æ–¹æ³•è®ºã€‚\n    "
    },
    {
        "title": "INFA-Guard: Mitigating Malicious Propagation via Infection-Aware Safeguarding in LLM-Based Multi-Agent Systems",
        "authors": [
            "Yijin Zhou",
            "Xiaoya Lu",
            "Dongrui Liu",
            "Junchi Yan",
            "Jing Shao"
        ],
        "categories": [
            "cs.MA",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14667v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14667v1",
        "summary": "The rapid advancement of Large Language Model (LLM)-based Multi-Agent Systems (MAS) has introduced significant security vulnerabilities, where malicious influence can propagate virally through inter-agent communication. Conventional safeguards often rely on a binary paradigm that strictly distinguishes between benign and attack agents, failing to account for infected agents i.e., benign entities converted by attack agents. In this paper, we propose Infection-Aware Guard, INFA-Guard, a novel defense framework that explicitly identifies and addresses infected agents as a distinct threat category. By leveraging infection-aware detection and topological constraints, INFA-Guard accurately localizes attack sources and infected ranges. During remediation, INFA-Guard replaces attackers and rehabilitates infected ones, avoiding malicious propagation while preserving topological integrity. Extensive experiments demonstrate that INFA-Guard achieves state-of-the-art performance, reducing the Attack Success Rate (ASR) by an average of 33%, while exhibiting cross-model robustness, superior topological generalization, and high cost-effectiveness.",
        "category": "æ¨¡å‹å®‰å…¨",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14667v1ã€æ¨¡å‹å®‰å…¨ã€‘INFA-Guard_ Mitigating Malicious Propagation via Infection-Aware Safeguarding in LLM-Based Multi-Agent Systems.pdf",
        "institution": "ä¸Šæµ·äº¤é€šå¤§å­¦ã€ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ã€ä¸Šæµ·åˆ›æ–°ç ”ç©¶é™¢",
        "note": "ğŸ“–æ ‡é¢˜ï¼šINFA-Guard: Mitigating Malicious Propagation via Infection-Aware Safeguarding in LLM-Based Multi-Agent Systems\nğŸŒæ¥æºï¼šarXiv, 2601.14667v1\n\nç¬”è®°æ ‡é¢˜ï¼šæå‡ºæ„ŸæŸ“æ„ŸçŸ¥é˜²å¾¡æ–°èŒƒå¼  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•æœ‰æ•ˆé˜»æ–­æ¶æ„å½±å“åœ¨å¤§è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„ç—…æ¯’å¼ä¼ æ’­ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šé¦–æ¬¡å°†â€œæ„ŸæŸ“ä»£ç†â€ä½œä¸ºç‹¬ç«‹å¨èƒç±»åˆ«å»ºæ¨¡ï¼Œæå‡ºæ„ŸæŸ“æ„ŸçŸ¥é˜²å¾¡æ¡†æ¶INFA-GUARDï¼Œå®ç°æ”»å‡»æºå®šä½ã€æ„ŸæŸ“èŒƒå›´è¯†åˆ«ä¸å·®å¼‚åŒ–ä¿®å¤ã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºæ„ŸæŸ“ä»£ç†æ˜ç¡®å®šä¹‰ï¼šåŸºäºå“åº”æ­£ç¡®æ€§åŠ¨æ€å˜åŒ–ï¼ˆJ(Râ°)=1â†’J(Ráµ)=0ï¼‰ï¼Œä¸¥æ ¼åŒºåˆ†åˆå§‹æ”»å‡»ä»£ç†Vâ‚â‚œâ‚–ä¸ç”±å…¶è¯´æœè½¬åŒ–çš„æ„ŸæŸ“ä»£ç†Váµ¢â‚™fï¼ŒäºŒè€…äº’æ–¥ã€‚  \nğŸ”¸è®¾è®¡æ„ŸæŸ“æ„ŸçŸ¥æ£€æµ‹æœºåˆ¶ï¼šèåˆæ—¶åºç‰¹å¾ï¼ˆä¸‰é€šé“åµŒå…¥ï¼šå½“å‰ã€æ®‹å·®ã€æ»‘åŠ¨å¹³å‡ï¼‰ä¸å›¾ç¥ç»ç½‘ç»œåˆ†æ”¯ç»“æ„ï¼Œæ”¯æŒè½®æ¬¡è‡ªé€‚åº”æ£€æµ‹ï¼Œç²¾å‡†æ•æ‰ä»è‰¯æ€§åˆ°æ„ŸæŸ“çš„çŠ¶æ€è¿ç§»ã€‚  \nğŸ”¸å¼•å…¥æ‹“æ‰‘çº¦æŸå»ºæ¨¡ï¼šåˆ©ç”¨â€œæ„ŸæŸ“ä»£ç†å¿…é‚»è¿‘æ”»å‡»æºâ€çš„ç»“æ„æ€§è§„å¾‹ï¼Œè®¾è®¡æ‹“æ‰‘æŸå¤±Lâ‚œâ‚’â‚šâ‚’æƒ©ç½šå­¤ç«‹æ„ŸæŸ“é¢„æµ‹ï¼Œå¹¶åœ¨åå¤„ç†é˜¶æ®µç»“åˆæ—¶ç©ºè¶‹åŠ¿è¿›è¡Œé‚»åŸŸèº«ä»½é‡æ ¡å‡†ã€‚  \nğŸ”¸å®æ–½å·®å¼‚åŒ–ä¿®å¤ç­–ç•¥ï¼šå¯¹æ”»å‡»ä»£ç†ç›´æ¥æ›¿æ¢ä¸ºè‰¯æ€§ä»£ç†ä»¥åˆ‡æ–­æºå¤´ï¼›å¯¹æ„ŸæŸ“ä»£ç†é‡‡ç”¨LLMé©±åŠ¨çš„å›å¤çº§å‡€åŒ–ï¼ˆRFå‡½æ•°ï¼‰ï¼Œä¿ç•™å…¶æ‹“æ‰‘è¿æ¥å¹¶æ¢å¤è¯­ä¹‰æ­£ç¡®æ€§ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸æ„ŸæŸ“ä»£ç†æ˜¯å…³é”®ä¼ æ’­æ¢çº½ï¼šå®éªŒè¯æ˜ï¼Œä»…é˜²å¾¡æ”»å‡»ä»£ç†æ—¶ASRåœ¨3è½®åä»ä¸Šå‡11%ï¼ˆå†…å­˜æ”»å‡»ï¼‰å’Œ30%ï¼ˆå·¥å…·æ”»å‡»ï¼‰ï¼Œè€Œè”åˆé˜²å¾¡å¯æ˜¾è‘—æŠ‘åˆ¶ä¼ æ’­ã€‚  \nğŸ”¸æ„ŸæŸ“ä»£ç†å…·æœ‰å¼ºæ‹“æ‰‘æŒ‡ç¤ºæ€§ï¼šå…¶é‚»åŸŸå†…å­˜åœ¨æ”»å‡»ä»£ç†çš„æ¦‚ç‡æ˜¾è‘—é«˜äºéšæœºèŠ‚ç‚¹ï¼ŒéªŒè¯äº†â€œè¿ååŸåˆ™â€å¯æå‡å®šä½ç²¾åº¦ã€‚  \nğŸ”¸INFA-GUARDå…¨é¢ä¼˜äºåŸºçº¿ï¼šåœ¨5ç±»æ”»å‡»ã€2ç§LLMä¸»å¹²ã€4ç§æ‹“æ‰‘ä¸‹ï¼Œå¹³å‡é™ä½ASRè¾¾33%ï¼ŒMDSRæå‡è‡³96.7%ï¼Œä¸”è·¨æ¨¡å‹é²æ£’æ€§å¼ºã€‚  \nğŸ”¸å„æ¨¡å—å‡ä¸å¯æˆ–ç¼ºï¼šæ¶ˆèå®éªŒæ˜¾ç¤ºï¼Œç§»é™¤ä¿®å¤ç­–ç•¥ï¼ˆRDï¼‰å¯¼è‡´ASRé£™å‡è‡³12.9%ï¼Œè¯å®å·®å¼‚åŒ–å¤„ç½®å¯¹ç»´æŒæ‹“æ‰‘å®Œæ•´æ€§è‡³å…³é‡è¦ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œçªç ´äº†ä¼ ç»ŸäºŒå…ƒå®‰å…¨èŒƒå¼ï¼Œé¦–æ¬¡å°†ä¼ æ’­åŠ¨åŠ›å­¦å»ºæ¨¡ä¸ºâ€œæ”»å‡»â†’æ„ŸæŸ“â†’å†ä¼ æ’­â€ä¸‰çº§è¿‡ç¨‹ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºï¼šâ‘ ç†è®ºå±‚é¢æ˜ç¡®å®šä¹‰å¹¶éªŒè¯æ„ŸæŸ“ä»£ç†çš„ç»“æ„æ€§è§’è‰²ï¼›â‘¡æ–¹æ³•å±‚é¢è€¦åˆæ—¶åºæ¼”åŒ–å»ºæ¨¡ä¸æ‹“æ‰‘å…ˆéªŒçº¦æŸï¼›â‘¢å·¥ç¨‹å±‚é¢å®ç°è½»é‡é«˜æ•ˆï¼ˆtokenå¼€é”€ä»…7.2%/9.3%ï¼‰ä¸é«˜ä¿çœŸä¿®å¤çš„ç»Ÿä¸€ï¼Œä¸ºå¯ä¿¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†å¯è½åœ°çš„æ–°åŸºå‡†ã€‚\n    "
    },
    {
        "title": "LFS: Learnable Frame Selector for Event-Aware and Temporally Diverse Video Captioning",
        "authors": [
            "Lianying Chao",
            "Linfeng Yin",
            "Peiyu Ren",
            "Yifan Jiang",
            "Qiaoyu Ren",
            "Dingcheng Shan",
            "Jing-cheng Pang",
            "Sijie Wu",
            "Xubin Li",
            "Kai Zhang"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14594v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14594v1",
        "summary": "Video captioning models convert frames into visual tokens and generate descriptions with large language models (LLMs). Since encoding all frames is prohibitively expensive, uniform sampling is the default choice, but it enforces equal temporal coverage while ignoring the uneven events distribution. This motivates a Learnable Frame Selector (LFS) that selects temporally diverse and event-relevant frames. LFS explicitly models temporal importance to balance temporal diversity and event relevance, and employs a stratified strategy to ensure temporal coverage while avoiding clustering. Crucially, LFS leverages caption feedback from frozen video-LLMs to learn frame selection that directly optimizes downstream caption quality. Additionally, we identify the gap between existing benchmark and human's cognition. Thus, we introduce ICH-CC built from carefully designed questions by annotators that reflect human-consistent understanding of video. Experiments indicate that LFS consistently improves detailed video captioning across two representative community benchmarks and ICH-CC, achieving up to 2.0% gains on VDC and over 4% gains on ICH-CC. Moreover, we observe that enhanced captions with LFS leads to improved performance on video question answering. Overall, LFS provides an effective and easy-to-integrate solution for detailed video captioning.",
        "category": "æ¨¡å‹æ¶æ„",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14594v1ã€æ¨¡å‹æ¶æ„ã€‘LFS_ Learnable Frame Selector for Event-Aware and Temporally Diverse Video Captioning.pdf",
        "institution": "åä¸º Technologies Co., Ltd.",
        "note": "ğŸ“–æ ‡é¢˜ï¼šLFS: Learnable Frame Selector for Event-Aware and Temporally Diverse Video Captioning\nğŸŒæ¥æºï¼šarXiv, 2601.14594v1\n\nç¬”è®°æ ‡é¢˜ï¼šå¯å­¦ä¹ å¸§é€‰æ‹©æå‡è§†é¢‘æè¿°è´¨é‡  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨è®¡ç®—å—é™ä¸‹ï¼Œä¸ºè¯¦ç»†è§†é¢‘æè¿°ä»»åŠ¡é€‰æ‹©æ—¢äº‹ä»¶ç›¸å…³åˆæ—¶é—´å¤šæ ·åŒ–çš„å…³é”®å¸§ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºLearnable Frame Selectorï¼ˆLFSï¼‰ï¼Œé¦–ä¸ªé€šè¿‡captionåé¦ˆç›´æ¥ä¼˜åŒ–å¸§é€‰æ‹©ã€å…¼é¡¾äº‹ä»¶æ„ŸçŸ¥ä¸æ—¶é—´å¤šæ ·æ€§çš„è½»é‡å¯å­¦ä¹ æ¨¡å—ã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡è½»é‡æ—¶åºè¯„åˆ†ç½‘ç»œï¼ˆTSNetï¼‰ï¼Œä»¥å†»ç»“çš„Long-CLIPå¸§åµŒå…¥ä¸ºè¾“å…¥ï¼Œå»ºæ¨¡æ¯å¸§è¿ç»­é‡è¦æ€§å¾—åˆ†ï¼Œèåˆå±€éƒ¨æ—¶åºå·ç§¯ä¸å…¨å±€é—¨æ§è°ƒåˆ¶ã€‚  \nğŸ”¸å¼•å…¥åˆ†å±‚Top-Kç­–ç•¥ï¼šå°†è§†é¢‘ç­‰åˆ†ä¸ºKæ®µï¼Œæ¯æ®µé€‰1å¸§ï¼Œå¼ºåˆ¶æ—¶é—´è¦†ç›–ã€é¿å…èšé›†ï¼Œå¹¶å›ºå®šä¿ç•™é¦–å°¾å¸§ã€‚  \nğŸ”¸é‡‡ç”¨captionå¼•å¯¼ç›‘ç£ï¼šå°†å¸§æƒé‡æ³¨å…¥å†»ç»“è§†é¢‘-LLMçš„è§†è§‰æ¨¡å—ï¼Œä»¥ç”Ÿæˆcaptionçš„tokençº§äº¤å‰ç†µä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œä½¿ç”¨ç›¸å¯¹æŸå¤±æ¶ˆé™¤å‡åŒ€åŸºçº¿åå·®ã€‚  \nğŸ”¸æ·»åŠ â„“â‚æ­£åˆ™åŒ–ä¸ç†µæ­£åˆ™åŒ–ï¼šå‰è€…ä¿ƒè¿›å¸§æƒé‡é›†ä¸­äºä»£è¡¨æ€§å¸§ï¼Œåè€…é¼“åŠ±è®­ç»ƒåˆæœŸæ¢ç´¢ï¼Œéšæ¸©åº¦å‚æ•°é€€ç«é€æ­¥å‡å¼±ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸LFSåœ¨ICH-CCï¼ˆæ–°æ„å»ºçš„äººç±»è®¤çŸ¥å¯¹é½åŸºå‡†ï¼‰ä¸Šå¸¦æ¥æœ€é«˜4.47%å‡†ç¡®ç‡æå‡ï¼ŒéªŒè¯å…¶å¯¹ç»†ç²’åº¦ã€æ—¶åºè¿è´¯æè¿°çš„æœ‰æ•ˆæ€§ã€‚  \nğŸ”¸åœ¨VDCç­‰ä¸»æµåŸºå‡†ä¸Šç¨³å®šæå‡å¤šä¸ªè§†é¢‘-LLMï¼ˆå¦‚Qwen3-VLã€AuroraCapï¼‰ï¼Œå°¤å…¶åœ¨â€œDetailedâ€å­é¡¹å¢ç›Šæœ€æ˜¾è‘—ï¼ˆ+2.02%ï¼‰ï¼Œè¯´æ˜æ›´ä¼˜æ•æ‰å…³é”®åŠ¨ä½œã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šåˆ†å±‚æœºåˆ¶å½±å“æœ€å¤§ï¼ˆç§»é™¤åICH-CC-zhä¸‹é™6.2%ï¼‰ï¼Œè¯å®æ—¶é—´è¦†ç›–å¯¹é•¿è§†é¢‘é˜¶æ®µåŒ–äº‹ä»¶è‡³å…³é‡è¦ï¼›äº‹ä»¶å»ºæ¨¡ä¸captionç›‘ç£ç¼ºä¸€ä¸å¯ã€‚  \nğŸ”¸LFSå¢å¼ºçš„æè¿°æ˜¾è‘—æå‡é›¶æ ·æœ¬è§†é¢‘é—®ç­”æ€§èƒ½ï¼ˆMVBench +2.1%ï¼ŒVideoMMMU +1.5%ï¼‰ï¼Œè¡¨æ˜ç”Ÿæˆå†…å®¹æ›´å…·ä¸‹æ¸¸æ¨ç†æ”¯æ’‘åŠ›ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°åœ¨äºè·³å‡ºä¼ ç»Ÿä»£ç†ç›®æ ‡ï¼ˆå¦‚é‡å»ºè¯¯å·®æˆ–æŸ¥è¯¢åŒ¹é…ï¼‰ï¼Œé¦–æ¬¡å°†å¸§é€‰æ‹©ç›´æ¥ç»‘å®šæœ€ç»ˆcaptionè´¨é‡ï¼Œå®ç°ç«¯åˆ°ç«¯è¯­ä¹‰å¯¹é½ï¼›åŒæ—¶æå‡ºçš„åˆ†å±‚+äº‹ä»¶æ„ŸçŸ¥åŒçº¦æŸæ¡†æ¶ï¼Œå·§å¦™å¹³è¡¡äº†æ—¶é—´å†—ä½™ä¸ä¿¡æ¯ç¼ºå¤±è¿™ä¸€é•¿æœŸçŸ›ç›¾ï¼Œä¸”å³æ’å³ç”¨ã€æ— éœ€ä¿®æ”¹å¤§æ¨¡å‹å‚æ•°ï¼Œå·¥ç¨‹è½åœ°æ€§å¼ºã€‚\n    "
    },
    {
        "title": "Rethinking Reinforcement fine-tuning of LLMs: A Multi-armed Bandit Learning Perspective",
        "authors": [
            "Xiao Hu",
            "Hong Xie",
            "Tao Tan",
            "Defu Lian",
            "Jianyu Han"
        ],
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14599v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14599v1",
        "summary": "A large number of heuristics have been proposed to optimize the reinforcement fine-tuning of LLMs. However, inconsistent claims are made from time to time, making this area elusive. Reflecting on this situation, two fundamental questions still lack a clear understanding: 1) what is the role of each optimizing choice? 2) which ones are the bottlenecks? This paper aims to shed light on them, and it faces the challenge of several entangled confounding factors in the fine-tuning process. To tackle this challenge, we propose a bottom-up experiment pipeline. The bottom layer is composed of a minimalist configuration: one training data, one rollout per round and the reward directly serve as the learning signal without advantage function design. This minimalist configuration connects to multi-armed bandit learning with extremely large discrete action space, which offers theories to corroborate the experiment findings. The up procedure of the experiment pipeline expanding the minimalist configuration layer by layer, examining the role of each design choice. Experimental results on three LLMs and two reasoning datasets not only reveal new understanding of the design choice but also yield essential insights to shape the area.",
        "category": "å¼ºåŒ–å­¦ä¹ ",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14599v1ã€å¼ºåŒ–å­¦ä¹ ã€‘Rethinking Reinforcement fine-tuning of LLMs_ A Multi-armed Bandit Learning Perspective.pdf",
        "institution": "ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦ã€IFlyTek (ä¸­å›½)",
        "note": "ğŸ“–æ ‡é¢˜ï¼šRethinking Reinforcement fine-tuning of LLMs: A Multi-armed Bandit Learning Perspective\nğŸŒæ¥æºï¼šarXiv, 2601.14599v1\n\nç¬”è®°æ ‡é¢˜ï¼šå¤šè‡‚è€è™æœºè§†è§’çœ‹RLå¾®è°ƒ  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šåœ¨å¤§è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸­ï¼Œå“ªäº›è®¾è®¡é€‰æ‹©çœŸæ­£å…³é”®ï¼Œå“ªäº›æ˜¯æ€§èƒ½ç“¶é¢ˆï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºä¸€ç§è‡ªä¸‹è€Œä¸Šçš„å®éªŒæ¡†æ¶ï¼Œä»å¤šè‡‚è€è™æœºè§†è§’é‡æ–°ç†è§£å¼ºåŒ–å¾®è°ƒä¸­çš„å„å› ç´ ä½œç”¨ã€‚  \n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºæç®€é…ç½®ä½œä¸ºåŸºçº¿ï¼šä»…ä½¿ç”¨ä¸€æ¡è®­ç»ƒæ•°æ®ã€æ¯è½®ä¸€æ¬¡rolloutï¼Œç›´æ¥ç”¨å¥–åŠ±ä¿¡å·ï¼ˆæ— éœ€ä¼˜åŠ¿å‡½æ•°ï¼‰ã€‚  \nğŸ”¸å°†è¯¥è®¾ç½®ç±»æ¯”ä¸ºè¶…å¤§ç¦»æ•£åŠ¨ä½œç©ºé—´çš„å¤šè‡‚è€è™æœºé—®é¢˜ï¼Œå€ŸåŠ©å…¶ç†è®ºè§£é‡Šå­¦ä¹ è¡Œä¸ºã€‚  \nğŸ”¸é€å±‚æ‰©å±•é…ç½®ï¼Œåˆ†åˆ«æµ‹è¯•ä¼˜åŠ¿å‡½æ•°ã€rolloutæ•°é‡ã€å¥–åŠ±è®¾è®¡ã€æ•°æ®éš¾åº¦å’ŒåŸºç¡€æ¨¡å‹çš„å½±å“ã€‚  \nğŸ”¸åœ¨ä¸‰ä¸ªLLMï¼ˆLLaMAã€Qwenã€OLMoï¼‰å’Œä¸¤ä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šè¿›è¡Œç³»ç»Ÿå®éªŒã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸æç®€é…ç½®ä¸‹ï¼Œæ‰€æœ‰æ¨¡å‹å‡èƒ½åœ¨è®­ç»ƒé›†è¾¾åˆ°Pass@1=1ï¼Œä¸”æµ‹è¯•é›†å‡†ç¡®ç‡æœ€é«˜æå‡0.5ï¼Œè¯´æ˜ç®€å•æ–¹æ³•å·²è¶³å¤Ÿæœ‰æ•ˆã€‚  \nğŸ”¸å¢åŠ rolloutæ•°é‡å¯åŠ å¿«å­¦ä¹ é€Ÿåº¦ï¼Œä½†ä¸æå‡æœ€ç»ˆæ³›åŒ–æ€§èƒ½ï¼Œä¸”é™ä½æ ·æœ¬æ•ˆç‡ã€‚  \nğŸ”¸GRPOå¼ä¼˜åŠ¿å‡½æ•°åœ¨æ™®é€šæ•°æ®ä¸Šæ— æ˜æ˜¾ç›Šå¤„ï¼Œåè€Œå¢åŠ è®­ç»ƒæ³¢åŠ¨ï¼›ä»…åœ¨æéš¾æ•°æ®æ—¶ç•¥å¾®ç¼“è§£ç¾éš¾æ€§é—å¿˜ã€‚  \nğŸ”¸è´Ÿå¥–åŠ±ï¼ˆ{-1,0}ï¼‰å¯¼è‡´æ¨¡å‹æ— æ³•æ”¶æ•›ï¼Œè®­ç»ƒå‘æ•£ï¼Œä¸¥é‡æŸå®³æ³›åŒ–èƒ½åŠ›ã€‚  \nğŸ”¸å½“è®­ç»ƒæ•°æ®æéš¾ï¼ˆPass@1<0.05ï¼‰æ—¶ï¼Œæ¨¡å‹ä»èƒ½å­¦ä¼šæœ€ä¼˜ç­–ç•¥ä½†éœ€æ›´å¤šè½®æ¬¡ï¼Œä¸”æ˜“å‡ºç°æ³›åŒ–å´©æºƒã€‚  \nğŸ”¸OLMoè™½èƒ½æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œä½†å®Œå…¨æ— æ³•æ³›åŒ–ï¼Œæ­ç¤ºåŸºç¡€æ¨¡å‹è‡ªèº«ç‰¹æ€§å¯¹å¾®è°ƒç»“æœæœ‰å†³å®šæ€§å½±å“ã€‚  \n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°åœ°å°†å¼ºåŒ–å¾®è°ƒç®€åŒ–ä¸ºå¤šè‡‚è€è™æœºé—®é¢˜ï¼Œå‰¥ç¦»å¤æ‚è®¾è®¡å¹²æ‰°ï¼Œæ­ç¤ºäº†è®¸å¤šâ€œä¸»æµä¼˜åŒ–â€å¦‚ä¼˜åŠ¿å‡½æ•°ã€å¤šrolloutç­‰åœ¨å¸¸è§„æƒ…å†µä¸‹å®é™…æ”¶ç›Šæœ‰é™ã€‚çœŸæ­£ç“¶é¢ˆåœ¨äºæç«¯å›°éš¾æ ·æœ¬ä¸‹çš„æ³›åŒ–å¤±è´¥ä¸æ¨¡å‹æœ¬èº«çš„é€‚é…æ€§ï¼Œè€Œéç²¾ç»†ç®—æ³•è®¾è®¡ã€‚è¿™ä¸€è§†è§’æœ‰åŠ©äºç¤¾åŒºå›å½’æœ¬è´¨é—®é¢˜ï¼šä½•æ—¶éœ€è¦å¤æ‚æ–¹æ³•ï¼Œä»¥åŠå¦‚ä½•æå‡æ¨¡å‹æ³›åŒ–é²æ£’æ€§ã€‚\n    "
    },
    {
        "title": "SearchGym: Bootstrapping Real-World Search Agents via Cost-Effective and High-Fidelity Environment Simulation",
        "authors": [
            "Xichen Zhang",
            "Ziyi He",
            "Yinghao Zhu",
            "Sitong Wu",
            "Shaozuo Yu",
            "Meng Chu",
            "Wenhu Zhang",
            "Haoru Tan",
            "Jiaya Jia"
        ],
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14615v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14615v1",
        "summary": "Search agents have emerged as a pivotal paradigm for solving open-ended, knowledge-intensive reasoning tasks. However, training these agents via Reinforcement Learning (RL) faces a critical dilemma: interacting with live commercial Web APIs is prohibitively expensive, while relying on static data snapshots often introduces noise due to data misalignment. This misalignment generates corrupted reward signals that destabilize training by penalizing correct reasoning or rewarding hallucination. To address this, we propose SearchGym, a simulation environment designed to bootstrap robust search agents. SearchGym employs a rigorous generative pipeline to construct a verifiable knowledge graph and an aligned document corpus, ensuring that every reasoning task is factually grounded and strictly solvable. Building on this controllable environment, we introduce SearchGym-RL, a curriculum learning methodology that progressively optimizes agent policies through purified feedback, evolving from basic interactions to complex, long-horizon planning. Extensive experiments across the Llama and Qwen families demonstrate strong Sim-to-Real generalization. Notably, our Qwen2.5-7B-Base model trained within SearchGym surpasses the web-enhanced ASearcher baseline across nine diverse benchmarks by an average relative margin of 10.6%. Our results validate that high-fidelity simulation serves as a scalable and highly cost-effective methodology for developing capable search agents.",
        "category": "Agent",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14615v1ã€Agentã€‘SearchGym_ Bootstrapping Real-World Search Agents via Cost-Effective and High-Fidelity Environment Simulation.pdf",
        "institution": "é¦™æ¸¯ç§‘æŠ€å¤§å­¦ã€é¦™æ¸¯å¤§å­¦ã€é¦™æ¸¯ä¸­æ–‡å¤§å­¦",
        "note": "ğŸ“–æ ‡é¢˜ï¼šSearchGym: Bootstrapping Real-World Search Agents via Cost-Effective and High-Fidelity Environment Simulation\nğŸŒæ¥æºï¼šarXiv, 2601.14615v1\n\né«˜ä¿çœŸæ¨¡æ‹Ÿè®­ç»ƒæœç´¢æ™ºèƒ½ä½“  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä½æˆæœ¬ä¸”é«˜ä¿çœŸçš„ç¯å¢ƒä¸‹è®­ç»ƒå…·å¤‡çœŸå®ä¸–ç•Œæ³›åŒ–èƒ½åŠ›çš„æœç´¢æ™ºèƒ½ä½“ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºSearchGymï¼Œä¸€ç§åŸºäºé—­ç¯ç”Ÿæˆçš„çŸ¥è¯†å›¾è°±ä¸å¯¹é½è¯­æ–™åº“çš„é«˜ä¿çœŸæ¨¡æ‹Ÿç¯å¢ƒï¼Œå®ç°ç¨³å®šé«˜æ•ˆçš„æœç´¢æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ã€‚  \nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºéªŒè¯å‹çŸ¥è¯†å›¾è°±ä¸æ–‡æ¡£è¯­æ–™åº“ï¼Œç¡®ä¿æ¯ä¸ªæ¨ç†ä»»åŠ¡äº‹å®å¯è¿½æº¯ä¸”ä¸¥æ ¼å¯è§£ã€‚  \nğŸ”¸è®¾è®¡ä¸‰ç±»å¤æ‚é—®ç­”ç»“æ„ï¼ˆç®€å•ã€å¹¶è¡Œã€ç»„åˆå‹QAï¼‰ï¼Œæ”¯æŒå¤šè·³ã€é•¿è§†é‡ä¸å¤åˆé€»è¾‘æ¨ç†ã€‚  \nğŸ”¸å¼•å…¥ä¸¤é˜¶æ®µè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œå…ˆæŒæ¡åŸºç¡€äº¤äº’å†è¿›é˜¶è‡³å¤æ‚æ¨ç†ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ã€‚  \nğŸ”¸å®šä¹‰â€œæ£€ç´¢-è®¿é—®â€åŒåŠ¨ä½œç©ºé—´ï¼Œæ¨¡æ‹ŸçœŸå®ç½‘é¡µæµè§ˆè¡Œä¸ºï¼Œå¼ºåˆ¶æ¨¡å‹è¿›è¡Œç‰‡æ®µè¯„ä¼°åå†æ·±å…¥é˜…è¯»ã€‚  \nğŸ”¸é€šè¿‡è¾¹ç¼˜å¯æ£€ç´¢æ€§è¿‡æ»¤æœºåˆ¶ï¼Œç¡®ä¿æ¯æ¡çŸ¥è¯†è·¯å¾„å¯é€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢è¢«å‘ç°ï¼Œä¿éšœåé¦ˆä¿¡å·çº¯å‡€ã€‚  \nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸ç›¸æ¯”ä¾èµ–é™æ€ç»´åŸºæ•°æ®çš„æ–¹æ³•ï¼ŒSearchGymæ˜¾è‘—å‡å°‘å› æ•°æ®é”™ä½å¯¼è‡´çš„é”™è¯¯å¥–åŠ±ï¼Œè®­ç»ƒè¿‡ç¨‹æ›´ç¨³å®šï¼Œæ— ç­–ç•¥å´©æºƒç°è±¡ã€‚  \nğŸ”¸åœ¨Qwenå’ŒLlamaç³»åˆ—æ¨¡å‹ä¸Šå‡å®ç°è¶…è¶Šç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œå¹³å‡ç›¸å¯¹æå‡10.6%ï¼Œå°¤å…¶åœ¨GAIAç­‰å¼€æ”¾ç ”ç©¶ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚  \nğŸ”¸è®­ç»ƒå‡ºçš„æ™ºèƒ½ä½“åœ¨çœŸå®Web APIç¯å¢ƒä¸­ä»ä¿æŒä¼˜å¼‚æ€§èƒ½ï¼ŒéªŒè¯äº†å¼ºSim-to-Realè¿ç§»èƒ½åŠ›ã€‚  \nğŸ”¸ä»…ç”¨é›¶APIæˆæœ¬å³è¶…è¶Šéœ€è¶…500ç¾å…ƒæˆæœ¬çš„åœ¨çº¿è®­ç»ƒåŸºçº¿ï¼Œè¯æ˜å…¶æé«˜æ€§ä»·æ¯”ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¡¨æ˜ï¼Œè¯¾ç¨‹å­¦ä¹ ä¸åŒåŠ¨ä½œç©ºé—´å¯¹å¤æ‚ä»»åŠ¡æ€§èƒ½è‡³å…³é‡è¦ï¼Œç§»é™¤ä»»ä¸€æ¨¡å—å‡å¯¼è‡´æ˜¾è‘—ä¸‹é™ã€‚  \nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡åˆ›æ–°æ€§åœ°æ„å»ºäº†ä¸€ä¸ªå¯æ§ã€å¯éªŒè¯ã€é«˜ä¿çœŸçš„åˆæˆè®­ç»ƒç¯å¢ƒï¼Œä»æ ¹æœ¬ä¸Šè§£å†³äº†ç¦»çº¿è®­ç»ƒä¸­çš„å¥–åŠ±æ±¡æŸ“é—®é¢˜ã€‚å…¶æ ¸å¿ƒæ€æƒ³â€”â€”é€šè¿‡é—­ç¯ç”Ÿæˆç¡®ä¿ä»»åŠ¡å¯è§£æ€§å’Œåé¦ˆä¸€è‡´æ€§â€”â€”ä¸ºå·¥å…·å¢å¼ºå‹æ™ºèƒ½ä½“çš„è®­ç»ƒæä¾›äº†æ–°èŒƒå¼ã€‚åŒæ—¶ï¼Œè¯¾ç¨‹å­¦ä¹ ä¸ç²¾ç»†åŒ–åŠ¨ä½œè®¾è®¡æœ‰æ•ˆæå‡äº†å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œå±•ç°å‡ºå¼ºå¤§çš„å®é™…åº”ç”¨æ½œåŠ›ã€‚\n    "
    },
    {
        "title": "Social Caption: Evaluating Social Understanding in Multimodal Models",
        "authors": [
            "Bhaavanaa Thumu",
            "Leena Mathur",
            "Youssouf Kebe",
            "Louis-Philippe Morency"
        ],
        "categories": [
            "cs.CL",
            "cs.LG"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14569v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14569v1",
        "summary": "Social understanding abilities are crucial for multimodal large language models (MLLMs) to interpret human social interactions. We introduce Social Caption, a framework grounded in interaction theory to evaluate social understanding abilities of MLLMs along three dimensions: Social Inference (SI), the ability to make accurate inferences about interactions; Holistic Social Analysis (HSA), the ability to generate comprehensive descriptions of interactions; Directed Social Analysis (DSA), the ability to extract relevant social information from interactions. We analyze factors influencing model performance in social understanding, such as scale, architectural design, and spoken context. Experiments with MLLM judges contribute insights about scaling automated evaluation of multimodal social understanding.",
        "category": "Social Captionä»»åŠ¡è¯„æµ‹",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14569v1ã€Social Captionä»»åŠ¡è¯„æµ‹ã€‘Social Caption_ Evaluating Social Understanding in Multimodal Models.pdf",
        "institution": "å¡å†…åŸºæ¢…éš†å¤§å­¦",
        "note": "ğŸ“–æ ‡é¢˜ï¼šSocial Caption: Evaluating Social Understanding in Multimodal Models\nğŸŒæ¥æºï¼šarXiv, 2601.14569v1\n\nç¬”è®°æ ‡é¢˜ï¼šè¯„ä¼°å¤šæ¨¡æ€ç¤¾ä¼šç†è§£  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•å…¨é¢è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨çœŸå®ç¤¾äº¤äº’åŠ¨ä¸­çš„ç¤¾ä¼šç†è§£èƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºSOCIALCAPTIONæ¡†æ¶ï¼Œä»ç¤¾ä¼šæ¨ç†ã€æ•´ä½“åˆ†æå’Œå®šå‘åˆ†æä¸‰ä¸ªç»´åº¦ç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„ç¤¾ä¼šç†è§£èƒ½åŠ›ã€‚  \n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºä¸‰ç»´åº¦è¯„ä¼°ä½“ç³»ï¼šç¤¾ä¼šæ¨ç†ï¼ˆSIï¼‰é€šè¿‡é€‰æ‹©é¢˜å‡†ç¡®ç‡è¡¡é‡ï¼›æ•´ä½“ç¤¾ä¼šåˆ†æï¼ˆHSAï¼‰è¦æ±‚æ¨¡å‹ç”Ÿæˆæ¶µç›–åœºæ™¯ã€ä¸ªä½“ã€æƒ…ç»ªç­‰å…­æ–¹é¢çš„ç»¼åˆæè¿°ï¼›å®šå‘ç¤¾ä¼šåˆ†æï¼ˆDSAï¼‰è¦æ±‚æ¨¡å‹æ ¹æ®é—®é¢˜æå–ç›¸å…³ä¿¡æ¯å¹¶ç»“æ„åŒ–è¾“å‡ºã€‚  \nğŸ”¸é‡‡ç”¨äººç±»ä¸MLLMåŒè½¨è¯„åˆ†æœºåˆ¶ï¼šç”±äººå·¥æ ‡æ³¨å‘˜å’Œé«˜æ€§èƒ½MLLMå…±åŒå¯¹HSAå’ŒDSAç”Ÿæˆç»“æœæ‰“åˆ†ï¼ŒéªŒè¯è‡ªåŠ¨è¯„ä¼°çš„å¯è¡Œæ€§ã€‚  \nğŸ”¸æ§åˆ¶å˜é‡åˆ†æå½±å“å› ç´ ï¼šæ¯”è¾ƒä¸åŒæ¨¡å‹è§„æ¨¡ã€æ¶æ„è®¾è®¡ï¼ˆå¦‚è§†é¢‘ç¼–ç æ–¹å¼ï¼‰ã€æ˜¯å¦ä½¿ç”¨è¯­éŸ³è½¬å½•æ–‡æœ¬å¯¹ç¤¾ä¼šç†è§£æ€§èƒ½çš„å½±å“ã€‚  \nğŸ”¸å¼•å…¥åå·®æ£€æµ‹å®éªŒï¼šé€šè¿‡é”™é…è§†é¢‘-å›ç­”å¯¹æµ‹è¯•MLLMè¯„å§”æ˜¯å¦å­˜åœ¨ç›²ç›®é«˜åˆ†å€¾å‘ï¼Œç¡®ä¿è¯„åˆ†å¯é æ€§ã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åŠ å…¥è¯­éŸ³è½¬å½•æ˜¾è‘—æå‡SIè¡¨ç°ï¼Œå¹³å‡å¢ç›Šè¾¾13%ï¼Œè¯´æ˜è¯­è¨€çº¿ç´¢å¯¹ç¤¾ä¼šæ¨æ–­è‡³å…³é‡è¦ã€‚  \nğŸ”¸æ¨¡å‹è§„æ¨¡å¹¶éå†³å®šæ€§å› ç´ ï¼šå°å‹å¼€æºæ¨¡å‹ï¼ˆå¦‚InternVL3-8Bï¼‰åœ¨HSAå’ŒDSAä¸Šå¯åª²ç¾ç”šè‡³è¶…è¶Šå¤§å‹é—­æºæ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰ã€‚  \nğŸ”¸å¼ºSIèƒ½åŠ›ä¸ä¿è¯å¼ºç”Ÿæˆèƒ½åŠ›ï¼šQwen2.5-Omniåœ¨SIä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨HSA/DSAä¸­ä¸¥é‡è½åï¼Œæ˜¾ç¤ºåˆ¤åˆ«ä¸ç”Ÿæˆèƒ½åŠ›è§£è€¦ã€‚  \nğŸ”¸MLLMè¯„å§”ä¸äººç±»è¯„åˆ†é«˜åº¦ä¸€è‡´ï¼šInternVL3ç³»åˆ—ä½œä¸ºè¯„å§”æ—¶ï¼ŒäºŒå…ƒF1å¾—åˆ†è¶…92%ï¼Œä¼˜äºGemini-2.5-Proï¼Œè¡¨æ˜å¼€æºæ¨¡å‹å¯ä½œå¯é è‡ªåŠ¨è¯„ä¼°å·¥å…·ã€‚  \n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥ç ”ç©¶åˆ›æ–°æ€§åœ°å°†ç¤¾ä¼šå­¦ç†è®ºï¼ˆAPRACEåˆ†ç±»æ³•ï¼‰èå…¥è¯„æµ‹æ¡†æ¶ï¼Œçªç ´ä¼ ç»ŸQAèŒƒå¼ï¼Œå®ç°å¯¹ç¤¾ä¼šç†è§£çš„å¤šç»´é‡åŒ–ã€‚å…¶æœ€å¤§ä»·å€¼åœ¨äºæ­ç¤ºï¼šç¤¾ä¼šæ™ºèƒ½ä¸ä»…ä¾èµ–æ¨¡å‹è§„æ¨¡ï¼Œæ›´å—æ¶æ„è®¾è®¡ä¸è®­ç»ƒç­–ç•¥å½±å“ï¼›åŒæ—¶è¯æ˜é«˜è´¨é‡å¼€æºæ¨¡å‹å¯ç”¨äºè‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œä¸ºæœªæ¥å¤§è§„æ¨¡ç¤¾ä¼šæ™ºèƒ½ç ”ç©¶æä¾›æ–¹æ³•è®ºåŸºç¡€ã€‚\n    "
    },
    {
        "title": "QMC: Efficient SLM Edge Inference via Outlier-Aware Quantization and Emergent Memories Co-Design",
        "authors": [
            "Nilesh Prasad Pandey",
            "Jangseon Park",
            "Onat Gungor",
            "Flavio Ponzina",
            "Tajana Rosing"
        ],
        "categories": [
            "cs.LG"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14549v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14549v1",
        "summary": "Deploying Small Language Models (SLMs) on edge platforms is critical for real-time, privacy-sensitive generative AI, yet constrained by memory, latency, and energy budgets. Quantization reduces model size and cost but suffers from device noise in emerging non-volatile memories, while conventional memory hierarchies further limit efficiency. SRAM provides fast access but has low density, DRAM must simultaneously accommodate static weights and dynamic KV caches, which creates bandwidth contention, and Flash, although dense, is primarily used for initialization and remains inactive during inference. These limitations highlight the need for hybrid memory organizations tailored to LLM inference. We propose Outlier-aware Quantization with Memory Co-design (QMC), a retraining-free quantization with a novel heterogeneous memory architecture. QMC identifies inlier and outlier weights in SLMs, storing inlier weights in compact multi-level Resistive-RAM (ReRAM) while preserving critical outliers in high-precision on-chip Magnetoresistive-RAM (MRAM), mitigating noise-induced degradation. On language modeling and reasoning benchmarks, QMC outperforms and matches state-of-the-art quantization methods using advanced algorithms and hybrid data formats, while achieving greater compression under both algorithm-only evaluation and realistic deployment settings. Specifically, compared against SoTA quantization methods on the latest edge AI platform, QMC reduces memory usage by 6.3x-7.3x, external data transfers by 7.6x, energy by 11.7x, and latency by 12.5x when compared to FP16, establishing QMC as a scalable, deployment-ready co-design for efficient on-device inference.",
        "category": "æ¨¡å‹æ¶æ„",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14549v1ã€æ¨¡å‹æ¶æ„ã€‘QMC_ Efficient SLM Edge Inference via Outlier-Aware Quantization and Emergent Memories Co-Design.pdf",
        "institution": "University of California, San Diegoã€San Diego State University",
        "note": "ğŸ“–æ ‡é¢˜ï¼šQMC: Efficient SLM Edge Inference via Outlier-Aware Quantization and Emergent Memories Co-Design\nğŸŒæ¥æºï¼šarXiv, 2601.14549v1\n\nç¬”è®°æ ‡é¢˜ï¼šQMCå®ç°é«˜æ•ˆè¾¹ç¼˜æ¨ç†  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šé«˜æ•ˆéƒ¨ç½²å°å‹è¯­è¨€æ¨¡å‹å¹¶ä¿æŒç²¾åº¦ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºQMCï¼Œä¸€ç§æ— éœ€é‡è®­ç»ƒçš„é‡åŒ–ä¸å¼‚æ„å†…å­˜ååŒè®¾è®¡æ–¹æ³•ï¼Œæ˜¾è‘—æå‡SLMè¾¹ç¼˜æ¨ç†æ•ˆç‡ã€‚  \n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸é€šè¿‡è¯†åˆ«æ¨¡å‹æƒé‡ä¸­çš„â€œå¼‚å¸¸å€¼â€ï¼ˆoutliersï¼‰å’Œâ€œæ­£å¸¸å€¼â€ï¼ˆinliersï¼‰ï¼Œå¯¹äºŒè€…é‡‡ç”¨ä¸åŒå­˜å‚¨ä¸é‡åŒ–ç­–ç•¥ã€‚  \nğŸ”¸å°†é«˜ç²¾åº¦çš„å¼‚å¸¸å€¼å­˜äºæŠ—å™ªæ€§å¼ºçš„MRAMä¸­ï¼Œç¡®ä¿å…³é”®å‚æ•°ç¨³å®šæ€§ï¼›å¤§é‡æ­£å¸¸å€¼åˆ™å‹ç¼©åå­˜å…¥é«˜å¯†åº¦ReRAMã€‚  \nğŸ”¸è®¾è®¡å™ªå£°æ„ŸçŸ¥çš„é‡åŒ–ç®—æ³•ï¼Œåœ¨é‡åŒ–è¿‡ç¨‹ä¸­å»ºæ¨¡ReRAMè®¾å¤‡å™ªå£°ï¼Œä¼˜åŒ–ç¼©æ”¾å› å­ä»¥é™ä½ç¡¬ä»¶è¯¯å·®å½±å“ã€‚  \nğŸ”¸æ„å»ºç»Ÿä¸€çš„æ¨¡å‹æƒé‡æ§åˆ¶å™¨ï¼Œåè°ƒMRAMã€ReRAMä¸LPDDR5ä¹‹é—´çš„æ•°æ®è®¿é—®ï¼Œå®ç°å¹¶è¡Œè¯»å–ä¸ä½å»¶è¿ŸåŒæ­¥ã€‚  \nğŸ”¸æ•´ä¸ªæ¡†æ¶æ— éœ€æ¨¡å‹é‡è®­ç»ƒï¼Œå…¼å®¹å¤šç§SLMæ¶æ„ï¼Œé€‚ç”¨äºé€šç”¨åŠ é€Ÿå™¨ï¼Œå…·å¤‡è‰¯å¥½å¯éƒ¨ç½²æ€§ã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨Hymbaã€LLaMAç­‰SLMä¸ŠéªŒè¯ï¼ŒQMCåœ¨2-bit/3-bit ReRAMæ¨¡å¼ä¸‹å®ç°4.44å€å‹ç¼©æ¯”ï¼Œä¼˜äºINT4å’ŒMXINT4ã€‚  \nğŸ”¸ç›¸æ¯”FP16ï¼ŒQMCå‡å°‘6.3â€“7.3å€å†…å­˜å ç”¨ã€7.6å€å¤–éƒ¨æ•°æ®ä¼ è¾“ã€11.7å€èƒ½è€—ã€12.5å€å»¶è¿Ÿã€‚  \nğŸ”¸ä¸ç°æœ‰NVMååŒè®¾è®¡eMEMsç›¸æ¯”ï¼ŒQMCåœ¨èƒ½æ•ˆã€å»¶è¿Ÿå’Œå®¹é‡ä¸Šåˆ†åˆ«æå‡1.35Ã—ã€1.9Ã—å’Œ1.82Ã—ã€‚  \nğŸ”¸å®éªŒè¡¨æ˜0.3çš„å¼‚å¸¸å€¼æ¯”ä¾‹å¯åœ¨ç²¾åº¦ä¸æ•ˆç‡é—´å–å¾—æœ€ä½³å¹³è¡¡ï¼Œè¿‡é«˜ä¼šå› MRAMç“¶é¢ˆå¢åŠ å»¶è¿Ÿã€‚  \nğŸ”¸ç³»ç»Ÿå¼€é”€åˆ†ææ˜¾ç¤ºï¼Œå°½ç®¡å¼•å…¥çº¦21.62 mmÂ²é¢ç§¯å¼€é”€å’Œå°‘é‡åŒæ­¥å»¶è¿Ÿï¼Œä½†æ•´ä½“æ”¶ç›Šè¿œè¶…ä»£ä»·ã€‚  \n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nQMCçš„åˆ›æ–°åœ¨äºå°†ç®—æ³•çº§é‡åŒ–ä¸ç¡¬ä»¶çº§å†…å­˜æ¶æ„æ·±åº¦è€¦åˆï¼Œçªç ´ä¼ ç»Ÿé‡åŒ–ä»…ä¾èµ–è½¯ä»¶ä¼˜åŒ–çš„å±€é™ã€‚å…¶æ ¸å¿ƒæ€æƒ³â€”â€”â€œå¼‚å¸¸å€¼ä¿æŠ¤+å™ªå£°æ„ŸçŸ¥é‡åŒ–+å¼‚æ„å­˜å‚¨â€â€”â€”ä¸ºè¾¹ç¼˜AIæä¾›äº†ä¸€æ¡å®ç”¨ä¸”é«˜æ•ˆçš„éƒ¨ç½²è·¯å¾„ã€‚å°¤å…¶åœ¨æ— éœ€é‡è®­ç»ƒçš„å‰æä¸‹å®ç°é«˜æ€§èƒ½ï¼Œæå¤§å¢å¼ºäº†å®é™…åº”ç”¨å¯è¡Œæ€§ã€‚è¯¥å·¥ä½œä»£è¡¨äº†ä»â€œç®—æ³•é€‚é…ç¡¬ä»¶â€å‘â€œè½¯ç¡¬åŸç”ŸååŒâ€çš„é‡è¦æ¼”è¿›ã€‚\n    "
    },
    {
        "title": "Designing KRIYA: An AI Companion for Wellbeing Self-Reflection",
        "authors": [
            "Shanshan Zhu",
            "Wenxuan Song",
            "Jiayue Melissa Shi",
            "Dong Whi Yoo",
            "Karthik S. Bhat",
            "Koustuv Saha"
        ],
        "categories": [
            "cs.HC",
            "cs.AI",
            "cs.CL",
            "cs.CY"
        ],
        "published_date": "2026-01-21",
        "arxiv_id": "2601.14589v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14589v1",
        "summary": "Most personal wellbeing apps present summative dashboards of health and physical activity metrics, yet many users struggle to translate this information into meaningful understanding. These apps commonly support engagement through goals, reminders, and structured targets, which can reinforce comparison, judgment, and performance anxiety. To explore a complementary approach that prioritizes self-reflection, we design KRIYA, an AI wellbeing companion that supports co-interpretive engagement with personal wellbeing data. KRIYA aims to collaborate with users to explore questions, explanations, and future scenarios through features such as Comfort Zone, Detective Mode, and What-If Planning. We conducted semi-structured interviews with 18 college students interacting with a KRIYA prototype using hypothetical data. Our findings show that through KRIYA interaction, users framed engaging with wellbeing data as interpretation rather than performance, experienced reflection as supportive or pressuring depending on emotional framing, and developed trust through transparency. We discuss design implications for AI companions that support curiosity, self-compassion, and reflective sensemaking of personal health data.",
        "category": "Agent",
        "success": true,
        "file_path": "./output/2026-01-21/papers\\2601.14589v1ã€Agentã€‘Designing KRIYA_ An AI Companion for Wellbeing Self-Reflection.pdf",
        "institution": "University of Illinois at Urbana-Champaignã€Indiana University Indianapolisã€Drexel University",
        "note": "ğŸ“–æ ‡é¢˜ï¼šDesigning KRIYA: An AI Companion for Wellbeing Self-Reflection\nğŸŒæ¥æºï¼šarXiv, 2601.14589v1\n\nç¬”è®°æ ‡é¢˜ï¼šAIåŠ©åŠ›è‡ªæˆ‘å…³æ€€åæ€  \nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è®¾è®¡ä¸€ä¸ªAIä¼´ä¾£æ¥æ”¯æŒç”¨æˆ·å¯¹ä¸ªäººå¥åº·æ•°æ®çš„å…±æƒ…å¼è‡ªæˆ‘åæ€ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºå¹¶è¯„ä¼°äº†KRIYAâ€”â€”ä¸€ç§ä»¥å…±è§£é‡Šã€æƒ…æ„Ÿå…±é¸£å’Œä½å‹åŠ›æ¢ç´¢ä¸ºæ ¸å¿ƒçš„äººå·¥æ™ºèƒ½å¥åº·ä¼´ä¾£åŸå‹ã€‚  \n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡KRIYAä½œä¸ºå¯¹è¯å¼AIä¼´ä¾£ï¼Œé›†æˆæ™¨é—´é¢„æµ‹ã€èˆ’é€‚åŒºè®¾å®šã€æ™šé—´å¤ç›˜ã€ä¾¦æ¢æ¨¡å¼ä¸â€œå¦‚æœâ€è§„åˆ’ç­‰åŠŸèƒ½æ¨¡å—ï¼Œæ”¯æŒç”¨æˆ·ä¸æ•°æ®è¿›è¡Œåä½œæ€§è§£è¯»ã€‚  \nğŸ”¸é‡‡ç”¨å‡è®¾æƒ…å¢ƒä¸æ¨¡æ‹Ÿæ•°æ®å¼€å±•åŠç»“æ„åŒ–è®¿è°ˆï¼Œé¿å…éšç§é£é™©çš„åŒæ—¶å¼•å¯¼18åå¤§å­¦ç”Ÿä½“éªŒç³»ç»Ÿäº¤äº’ã€‚  \nğŸ”¸å¼•å…¥â€œèˆ’é€‚åŒºâ€æ›¿ä»£å›ºå®šç›®æ ‡ï¼Œç”¨æ¦‚ç‡åŒ–åé¦ˆå’Œå¯é€†å»ºè®®é™ä½è¡Œä¸ºæ”¹å˜çš„å¿ƒç†è´Ÿæ‹…ã€‚  \nğŸ”¸å¼ºè°ƒéè¯„åˆ¤æ€§è¯­è¨€ã€ä¸ç¡®å®šæ€§è¡¨è¾¾å’Œç”¨æˆ·ä¿®æ­£æœºåˆ¶ï¼Œæ„å»ºæƒ…æ„Ÿå®‰å…¨çš„åæ€ç¯å¢ƒã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸ç”¨æˆ·å°†æ•°æ®è§†ä¸ºè§£é‡Šçº¿ç´¢è€Œéç»©æ•ˆæŒ‡æ ‡ï¼Œä»â€œæˆ‘å¤±è´¥äº†å—ï¼Ÿâ€è½¬å‘â€œä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿâ€ï¼Œä¿ƒè¿›ä¸»åŠ¨æ„ä¹‰å»ºæ„ã€‚  \nğŸ”¸å…±è§£é‡Šè¿‡ç¨‹é€šè¿‡æä¾›æƒ…å¢ƒåŒ–å½’å› ï¼ˆå¦‚å¤©æ°”ã€æ—¥ç¨‹ï¼‰å¸®åŠ©ç”¨æˆ·ç†è§£æ³¢åŠ¨ï¼Œå‡å°‘è‡ªè´£æ„Ÿï¼Œå¢å¼ºå¿ƒç†æ¥çº³ã€‚  \nğŸ”¸éè¯„åˆ¤æ€§è¯­æ°”æ˜¾è‘—æå‡ä½¿ç”¨æ„æ„¿ï¼Œå°¤å…¶å¯¹ä¸å¸¸ä½¿ç”¨è€…è€Œè¨€ï¼Œç³»ç»Ÿæ˜¾å¾—æ›´æ˜“æ¥è¿‘ä¸”æ— å‹è¿«æ„Ÿã€‚  \nğŸ”¸é€æ˜å±•ç¤ºæ¨ç†é€»è¾‘å’Œä¸ç¡®å®šæ€§æ¯”ç»å¯¹å‡†ç¡®æ›´èƒ½å»ºç«‹ä¿¡ä»»ï¼›ç”¨æˆ·æ¥å—â€œå¯èƒ½â€ä½†æ‹’ç»è¶Šç•Œæ¨æµ‹ã€‚  \nğŸ”¸éƒ¨åˆ†ç”¨æˆ·è®¤ä¸ºæ¦‚ç‡è¾“å‡ºå’Œå¤šæ­¥éª¤äº’åŠ¨è®¤çŸ¥è´Ÿè·è¾ƒé«˜ï¼ŒæœŸå¾…çµæ´»è°ƒèŠ‚å‚ä¸æ·±åº¦ä»¥é€‚åº”æ—¥å¸¸èŠ‚å¥ã€‚  \n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥ç ”ç©¶åˆ›æ–°åœ°å°†AIå®šä½ä¸ºâ€œåæ€åä½œè€…â€è€Œéâ€œè¡Œä¸ºæ•™ç»ƒâ€ï¼Œçªç ´ä¼ ç»Ÿå¥åº·åº”ç”¨ä¾èµ–ç›®æ ‡ä¸æé†’çš„è®¾è®¡èŒƒå¼ã€‚å…¶æ ¸å¿ƒä»·å€¼åœ¨äºé€šè¿‡å¯¹è¯ç»“æ„åˆ†æ‹…ç”¨æˆ·çš„è§£é‡Šä¸æƒ…ç»ªåŠ³åŠ¨ï¼Œå®ç°ä»ç›‘æ§åˆ°å¥½å¥‡ã€ä»è¯„åˆ¤åˆ°ç†è§£çš„èŒƒå¼è½¬ç§»ã€‚ç‰¹åˆ«å€¼å¾—è‚¯å®šçš„æ˜¯å¯¹â€œæƒ…æ„Ÿè¯­è°ƒå†…åµŒäºåˆ†ææµç¨‹â€çš„å®è·µï¼Œä½¿å…±æƒ…æˆä¸ºç³»ç»Ÿé€»è¾‘çš„ä¸€éƒ¨åˆ†è€Œéè¡¨é¢ä¿®é¥°ã€‚æœªæ¥å¯æ¢ç´¢åŠ¨æ€é€‚é…ç”¨æˆ·çŠ¶æ€çš„äº¤äº’æ·±åº¦ï¼Œå¹¶ç»“åˆçœŸå®é•¿æœŸä½¿ç”¨éªŒè¯æ•ˆæœã€‚\n    "
    }
]