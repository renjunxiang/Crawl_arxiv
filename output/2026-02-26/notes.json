[
    {
        "title": "Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement",
        "authors": [
            "Shuchen Zhu",
            "Rizhen Hu",
            "Mingze Wang",
            "Mou Sun",
            "Xue Wang",
            "Kun Yuan",
            "Zaiwen Wen"
        ],
        "categories": [
            "cs.LG"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22681v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22681v1",
        "summary": "Pre-training Large Language Models requires immense computational resources, making optimizer efficiency essential. The optimization landscape is highly anisotropic, with loss reduction driven predominantly by progress along flat directions. While matrix-based optimizers such as Muon and SOAP leverage fine-grained curvature information to outperform AdamW, their updates tend toward isotropy -- relatively conservative along flat directions yet potentially aggressive along sharp ones. To address this limitation, we first establish a unified Riemannian Ordinary Differential Equation (ODE) framework that elucidates how common adaptive algorithms operate synergistically: the preconditioner induces a Riemannian geometry that mitigates ill-conditioning, while momentum serves as a Riemannian damping term that promotes convergence. Guided by these insights, we propose LITE, a generalized acceleration strategy that enhances training dynamics by applying larger Hessian damping coefficients and learning rates along flat trajectories. Extensive experiments demonstrate that LITE significantly accelerates both Muon and SOAP across diverse architectures (Dense, MoE), parameter scales (130M--1.3B), datasets (C4, Pile), and learning-rate schedules (cosine, warmup-stable-decay). Theoretical analysis confirms that LITE facilitates faster convergence along flat directions in anisotropic landscapes, providing a principled approach to efficient LLM pre-training. The code is available at https://github.com/SHUCHENZHU/LITE.",
        "tag": "è®­ç»ƒä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22681v1ã€è®­ç»ƒä¼˜åŒ–-åŒ—äº¬å¤§å­¦ã€‘Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement.pdf",
        "institution_status": "keep",
        "institution": "åŒ—äº¬å¤§å­¦ã€Zhejiang Labã€ç¾å›¢",
        "first_institution": "åŒ—äº¬å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šAccelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement\nğŸŒæ¥æºï¼šarXiv, 2602.22681v1\n\nç¬”è®°æ ‡é¢˜ï¼šå¢å¼ºæ‰å¹³æ–¹å‘è®­ç»ƒåŠ¨åŠ›\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨LLMé¢„è®­ç»ƒä¸­å…‹æœå„å‘å¼‚æ€§æŸå¤±æ™¯è§‚å¯¼è‡´çš„ä¼˜åŒ–ç¼“æ…¢ï¼Œå°¤å…¶æå‡æ²¿æ‰å¹³æ–¹å‘ï¼ˆä¸»å¯¼æŸå¤±ä¸‹é™ä½†è¿›å±•ç¼“æ…¢ï¼‰çš„æ”¶æ•›é€Ÿåº¦ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºLITEæ–¹æ³•ï¼Œé¦–æ¬¡å»ºç«‹ç»Ÿä¸€é»æ›¼ODEæ¡†æ¶æ­ç¤ºé¢„æ¡ä»¶å™¨ä¸åŠ¨é‡çš„ååŒæœºåˆ¶ï¼Œå¹¶æ®æ­¤è®¾è®¡é¢å‘æ‰å¹³æ–¹å‘çš„åŠ¨æ€å¢å¼ºç­–ç•¥ï¼Œæ˜¾è‘—åŠ é€ŸMuonå’ŒSOAPç­‰å…ˆè¿›ä¼˜åŒ–å™¨ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºç»Ÿä¸€é»æ›¼ODEæ¡†æ¶ï¼ˆRISHDï¼‰ï¼Œå°†AdamWã€Muonã€SOAPç­‰è‡ªé€‚åº”ä¼˜åŒ–å™¨å»ºæ¨¡ä¸ºå¸¦Hessiané˜»å°¼çš„é»æ›¼æµå½¢ä¸Šçš„æƒ¯æ€§ç³»ç»Ÿï¼Œæ˜ç¡®é¢„æ¡ä»¶å™¨å®šä¹‰é»æ›¼å‡ ä½•ä»¥ç¼“è§£ç—…æ€ï¼ŒåŠ¨é‡åˆ™ä½œä¸ºé»æ›¼é˜»å°¼é¡¹ä¿ƒè¿›æ”¶æ•›ã€‚  \nğŸ”¸åŸºäºè¯¥æ¡†æ¶å‘ç°ç°æœ‰çŸ©é˜µä¼˜åŒ–å™¨æ›´æ–°å¹…å€¼è¶‹äºå„å‘åŒæ€§â€”â€”åœ¨æ‰å¹³æ–¹å‘è¿‡äºä¿å®ˆã€åœ¨å°–é”æ–¹å‘åˆå¯èƒ½æ¿€è¿›ï¼Œä»è€Œé™åˆ¶æ•´ä½“æ•ˆç‡ã€‚  \nğŸ”¸æå‡ºLITEç­–ç•¥ï¼šåœ¨æ‰å¹³å­ç©ºé—´å†…å¢å¤§Hessiané˜»å°¼ç³»æ•°Î²â‚‚å’Œå­¦ä¹ ç‡æ”¾å¤§æ¯”Ï‡ï¼Œè€Œåœ¨å°–é”å­ç©ºé—´ä¿æŒåŸæœ‰è¶…å‚ä»¥ä¿éšœç¨³å®šæ€§ï¼Œå®ç°é€‰æ‹©æ€§åŠ é€Ÿã€‚  \nğŸ”¸è®¾è®¡é«˜æ•ˆå®ç°æ–¹æ¡ˆï¼šåˆ©ç”¨Muon/SOAPè‡ªèº«é¢„æ¡ä»¶å™¨ï¼ˆå¦‚GâŠ¤Gæˆ–GGâŠ¤ï¼‰çš„ä¸»ç‰¹å¾å­ç©ºé—´è¿‘ä¼¼æ‰å¹³æ–¹å‘ï¼Œé¿å…é¢å¤–çŠ¶æ€ä¸é«˜å¼€é”€SVDï¼Œä»…å¼•å…¥å¯å¿½ç•¥çš„NSè¿­ä»£å¼€é”€ã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸LITEåœ¨å¤šç§è®¾ç½®ä¸‹å‡ç¨³å®šé™ä½ç»ˆç«¯æŸå¤±ï¼šæ¶µç›–Denseï¼ˆLLaMA 0.13Bâ€“1.3Bï¼‰ä¸MoEï¼ˆQwenMoE 1Bï¼‰æ¶æ„ã€C4/Pileæ•°æ®é›†ã€cos/wsdå­¦ä¹ ç‡è°ƒåº¦ï¼ŒéªŒè¯å…¶å¼ºæ³›åŒ–æ€§ã€‚  \nğŸ”¸LITE-Lï¼ˆä»…å¢å­¦ä¹ ç‡ï¼‰ä¸LITE-Hï¼ˆä»…å¢é˜»å°¼ï¼‰æ•ˆæœå‡å¼±äºå®Œæ•´LITEï¼Œè¯å®åŒæ—¶è°ƒèŠ‚Ï‡ä¸Î²â‚‚å¯¹æ‰å¹³æ–¹å‘åŠ¨åŠ›å­¦å¢å¼ºçš„å¿…è¦æ€§ã€‚  \nğŸ”¸åœ¨é•¿å‘¨æœŸè®­ç»ƒä¸­ï¼ˆtoken budgetè¾¾200Ã—å‚æ•°é‡ï¼‰ï¼ŒLITEå®ç°çº¦2å€åŠ é€Ÿï¼Œä¸”ç¼©æ”¾å¾‹æ›´ä¼˜ï¼Œè¡¨æ˜å…¶å…·å¤‡å‘æ›´å¤§æ¨¡å‹ä¸æ›´å¤štokenæ‰©å±•çš„æ½œåŠ›ã€‚  \nğŸ”¸æ¶ˆèå®éªŒæ˜¾ç¤ºï¼šè‹¥å°†LITEç­–ç•¥é”™è¯¯æ–½åŠ äºå°–é”æ–¹å‘ï¼ˆå¦‚ç»Ÿä¸€è®¾Î²=0.5/1.0ï¼‰ï¼Œç»ˆç«¯æŸå¤±åè¶…åŸºçº¿ï¼Œå°è¯â€œé€‰æ‹©æ€§â€è®¾è®¡çš„æ­£ç¡®æ€§ä¸å…³é”®æ€§ã€‚  \nğŸ”¸ç†è®ºåˆ†æè¯æ˜ï¼šLITEèƒ½åŠ å¿«æ²¿æ‰å¹³æµå½¢ï¼ˆRiverï¼‰çš„å¸å¼•é€Ÿåº¦ï¼Œå¹¶ä½¿æŠ•å½±è½¨è¿¹zâ‚œæ»¡è¶³âˆ«Î·â‚œâ€–P_R Fâ»Â¹âˆ‡f(zâ‚œ)â€–Â²_F dt â‰¤ 2Î”f/(Ï‡Î²â‚‚)ï¼Œå³Ï‡ä¸Î²â‚‚è¶Šå¤§ï¼Œæ¢¯åº¦ä¸‹é™ç§¯åˆ†ä¸Šç•Œè¶Šç´§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºï¼šä¸€æ˜¯é¦–æ¬¡ä»è¿ç»­æ—¶é—´é»æ›¼æµå½¢è§†è§’ç»Ÿä¸€åˆ»ç”»ä¸»æµè‡ªé€‚åº”ä¼˜åŒ–å™¨ï¼Œæ­ç¤ºé¢„æ¡ä»¶å™¨ä¸åŠ¨é‡çš„å‡ ä½•ååŒæœ¬è´¨ï¼›äºŒæ˜¯çªç ´â€œå„å‘åŒæ€§æ›´æ–°â€å±€é™ï¼Œæå‡ºé¦–ä¸ªé¢å‘æ‰å¹³æ–¹å‘çš„ã€å…¼å…·ç†è®ºä¿è¯ä¸å·¥ç¨‹å¯è¡Œæ€§çš„åŠ¨æ€å¢å¼ºèŒƒå¼ï¼ˆLITEï¼‰ï¼›ä¸‰æ˜¯å°†Hessiané˜»å°¼ä»å…¨å±€å›ºå®šç³»æ•°æ‹“å±•ä¸ºæ–¹å‘è‡ªé€‚åº”ï¼Œèµ‹äºˆåŠ¨é‡æœºåˆ¶æ›´å¼ºçš„äºŒé˜¶å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºLLMé«˜æ•ˆé¢„è®­ç»ƒæä¾›äº†æ–°åŸç†ä¸æ–°å·¥å…·ã€‚\n    "
    },
    {
        "title": "AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning",
        "authors": [
            "Yutong Wang",
            "Siyuan Xiong",
            "Xuebo Liu",
            "Wenkang Zhou",
            "Liang Ding",
            "Miao Zhang",
            "Min Zhang"
        ],
        "categories": [
            "cs.AI",
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.23258v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23258v1",
        "summary": "While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.",
        "tag": "Agent åä½œä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.23258v1ã€Agent åä½œä¼˜åŒ–-å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ã€‘AgentDropoutV2_ Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning.pdf",
        "institution_status": "keep",
        "institution": "å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ã€é˜¿é‡Œ",
        "first_institution": "å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šAgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning\nğŸŒæ¥æºï¼šarXiv, 2602.23258v1\n\nç¬”è®°æ ‡é¢˜ï¼šåŠ¨æ€æ‹¦æˆªçº é”™é˜²ä¼ æ’­\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸é‡æ–°è®­ç»ƒçš„å‰æä¸‹ï¼Œäºæ¨ç†æ—¶å®æ—¶é˜»æ–­å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­é”™è¯¯ä¿¡æ¯çš„çº§è”ä¼ æ’­ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºAgentDropoutV2æ¡†æ¶ï¼Œé¦–æ¬¡å®ç°æµ‹è¯•æ—¶â€œå…ˆè¿­ä»£çº æ­£ã€å†ä¸å¯ä¿®å¤åˆ™è£å‰ªâ€çš„åŠ¨æ€ä¿¡æ¯æµä¼˜åŒ–æœºåˆ¶ï¼Œæ˜¾è‘—æå‡MASé²æ£’æ€§ä¸å‡†ç¡®æ€§ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡æµ‹è¯•æ—¶æ‹¦æˆª-çº æ­£-è£å‰ªä¸‰æ€é—¨æ§æœºåˆ¶ï¼šå¯¹æ¯ä¸ªæ™ºèƒ½ä½“è¾“å‡ºå®æ—¶æ‹¦æˆªï¼ŒåŸºäºæ£€ç´¢åˆ°çš„å¤±è´¥æ¨¡å¼æŒ‡ç¤ºå™¨è¿›è¡Œå¤šè½®è¯Šæ–­ä¸åé¦ˆé©±åŠ¨ä¿®æ­£ã€‚  \nğŸ”¸æ„å»ºå¤±è´¥é©±åŠ¨çš„æŒ‡ç¤ºå™¨æ± ï¼šç¦»çº¿æŒ–æ˜MASå¤±è´¥è½¨è¿¹ï¼Œç”±æ•™å¸ˆæ¨¡å‹æç‚¼ç»“æ„åŒ–é”™è¯¯æ¨¡å¼ï¼ˆå«åç§°ã€å®šä¹‰ã€è§¦å‘æ¡ä»¶ï¼‰ï¼Œç»åŒé˜¶æ®µå»é‡ç¡®ä¿é«˜ç†µç´§å‡‘æ€§ã€‚  \nğŸ”¸å¼•å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ£€ç´¢ç­–ç•¥ï¼šé€šè¿‡æå–ä»»åŠ¡åœºæ™¯ä¸åŠ¨ä½œç±»å‹å…³é”®è¯ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œè¯­ä¹‰åŒ¹é…æœ€ç›¸å…³çš„Kä¸ªæŒ‡ç¤ºå™¨ï¼Œå®ç°ç²¾å‡†é”™è¯¯å®šä½ã€‚  \nğŸ”¸è®¾ç½®å…¨å±€å›é€€æœºåˆ¶ï¼šå½“æœ‰æ•ˆæ¶ˆæ¯æ•°ä½äºå®‰å…¨é˜ˆå€¼Î³æ—¶è§¦å‘ç³»ç»Ÿé‡ç½®ï¼Œé˜²æ­¢å› è¿‡åº¦è£å‰ªå¯¼è‡´åä½œç»“æ„å´©æºƒã€‚  \nğŸ”¸æ”¯æŒé›¶æ ·æœ¬æ³›åŒ–ï¼šæä¾›é€šç”¨é€»è¾‘æ£€æŸ¥æŒ‡ç¤ºå™¨ï¼Œåœ¨æ— é¢†åŸŸæŒ‡æ ‡æ± æ—¶ä»å¯å¯åŠ¨åŸºç¡€çº é”™æµç¨‹ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨9ä¸ªæ•°å­¦åŸºå‡†ä¸Šå¹³å‡å‡†ç¡®ç‡æå‡6.3ä¸ªç™¾åˆ†ç‚¹ï¼Œå°¤å…¶åœ¨AIME25ç­‰é«˜éš¾ä»»åŠ¡ä¸­æå‡è¾¾6.67%ï¼ŒéªŒè¯çº é”™æœ‰æ•ˆæ€§ã€‚  \nğŸ”¸è£å‰ªç‡ä¸ä»»åŠ¡éš¾åº¦å¼ºç›¸å…³ï¼šç®€å•ä»»åŠ¡é¦–è½®é€šè¿‡ç‡è¶…60%ï¼Œè€ŒAIME24/25æ‹’ç»ç‡è¶…60%ï¼Œè¡¨æ˜ç³»ç»Ÿèƒ½è‡ªé€‚åº”è°ƒèŠ‚å¹²é¢„å¼ºåº¦ã€‚  \nğŸ”¸æŒ‡ç¤ºå™¨æ± å…·å¤‡è·¨æ¨¡å‹è¿ç§»èƒ½åŠ›ï¼šQwen3-8Bæ„å»ºçš„æ± ç›´æ¥ç”¨äºQwen3-4Bä»è·ç¨³å®šå¢ç›Šï¼Œè¯å®é”™è¯¯æ¨¡å¼å…·æœ‰å°ºåº¦ä¸å˜æ€§ã€‚  \nğŸ”¸è·¨åŸŸæ³›åŒ–æœ‰æ•ˆï¼šåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­å¹³å‡å‡†ç¡®ç‡æå‡2.21%ï¼Œå¤æ‚æ•°æ®é›†ï¼ˆå¦‚CodeContestsï¼‰æå‡è¾¾3.2%ï¼Œè¯´æ˜æœºåˆ¶å…·é€šç”¨æ€§ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜å…³é”®è®¾è®¡å¿…è¦æ€§ï¼šéšæœºæ£€ç´¢æŒ‡ç¤ºå™¨ä½¿æ€§èƒ½åä½äºåŸºçº¿ï¼›å»é™¤å»é‡å¯¼è‡´å‡†ç¡®ç‡ä¸‹é™2.22%ï¼Œå‡¸æ˜¾æ± è´¨é‡é‡è¦æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ¨äºå°†ä¼ ç»Ÿé™æ€å‰ªæå‡çº§ä¸ºâ€œå¯é€†å¼åŠ¨æ€å‡€åŒ–â€â€”â€”ä»¥å¤±è´¥çŸ¥è¯†ä¸ºå…ˆéªŒã€ä»¥æ£€ç´¢ä¸ºå¯¼èˆªã€ä»¥è¿­ä»£ä¸ºæ‰‹æ®µï¼Œåœ¨æ¨ç†é“¾ä¸­åµŒå…¥è½»é‡çº§ä½†é«˜ç²¾åº¦çš„å®æ—¶çº é”™å±‚ã€‚å…¶æ ¸å¿ƒçªç ´æ˜¯è§£è€¦äº†é”™è¯¯æ£€æµ‹ä¸ä¿®æ­£èƒ½åŠ›ï¼Œæ—¢é¿å…äº†å¾®è°ƒä¾èµ–ï¼Œåˆè¶…è¶Šäº†è¢«åŠ¨è¿‡æ»¤ï¼Œä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å¯ä¿¡éƒ¨ç½²æä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "AgentVista: Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios",
        "authors": [
            "Zhaochen Su",
            "Jincheng Gao",
            "Hangyu Guo",
            "Zhenhua Liu",
            "Lueyang Zhang",
            "Xinyu Geng",
            "Shijue Huang",
            "Peng Xia",
            "Guanyu Jiang",
            "Cheng Wang",
            "Yue Zhang",
            "Yi R. Fung",
            "Junxian He"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.23166v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23166v1",
        "summary": "Real-world multimodal agents solve multi-step workflows grounded in visual evidence. For example, an agent can troubleshoot a device by linking a wiring photo to a schematic and validating the fix with online documentation, or plan a trip by interpreting a transit map and checking schedules under routing constraints. However, existing multimodal benchmarks mainly evaluate single-turn visual reasoning or specific tool skills, and they do not fully capture the realism, visual subtlety, and long-horizon tool use that practical agents require. We introduce AgentVista, a benchmark for generalist multimodal agents that spans 25 sub-domains across 7 categories, pairing realistic and detail-rich visual scenarios with natural hybrid tool use. Tasks require long-horizon tool interactions across modalities, including web search, image search, page navigation, and code-based operations for both image processing and general programming. Comprehensive evaluation of state-of-the-art models exposes significant gaps in their ability to carry out long-horizon multimodal tool use. Even the best model in our evaluation, Gemini-3-Pro with tools, achieves only 27.3% overall accuracy, and hard instances can require more than 25 tool-calling turns. We expect AgentVista to accelerate the development of more capable and reliable multimodal agents for realistic and ultra-challenging problem solving.",
        "tag": "å¤šæ¨¡æ€æ™ºèƒ½ä½“è¯„ä¼°åŸºå‡†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.23166v1ã€å¤šæ¨¡æ€æ™ºèƒ½ä½“è¯„ä¼°åŸºå‡†-é¦™æ¸¯ç§‘æŠ€å¤§å­¦ã€‘AgentVista_ Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios.pdf",
        "institution_status": "keep",
        "institution": "é¦™æ¸¯ç§‘æŠ€å¤§å­¦ã€åŒ—å¡ç½—æ¥çº³å¤§å­¦æ•™å ‚å±±åˆ†æ ¡ã€æµ™æ±Ÿå¤§å­¦ã€æ–°åŠ å¡å›½ç«‹å¤§å­¦",
        "first_institution": "é¦™æ¸¯ç§‘æŠ€å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šAgentVista: Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios\nğŸŒæ¥æºï¼šarXiv, 2602.23166v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºè¶…éš¾å¤šæ¨¡æ€ä»£ç†è¯„æµ‹åŸºå‡†\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è¯„ä¼°å¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨çœŸå®ã€å¤æ‚ã€é•¿ç¨‹è§†è§‰ä»»åŠ¡ä¸­çš„ç»¼åˆèƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºäº†AGENTVISTAâ€”â€”é¦–ä¸ªèšç„¦çœŸå®åœºæ™¯ã€ç»†ç²’åº¦è§†è§‰ä¾èµ–ã€è·¨æ¨¡æ€é•¿ç¨‹å·¥å…·ååŒçš„å¤šæ¨¡æ€é€šç”¨æ™ºèƒ½ä½“è¯„æµ‹åŸºå‡†ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡ä¸ƒå¤§ç±»25å­é¢†åŸŸå…±209ä¸ªä»»åŠ¡ï¼Œå…¨éƒ¨åŸºäºçœŸå®å›¾åƒä¸ç”¨æˆ·éœ€æ±‚ï¼Œå¼ºè°ƒè§†è§‰è¯æ®ä¸å¯æ›¿ä»£æ€§ã€‚  \nğŸ”¸æ¯é¡¹ä»»åŠ¡å¼ºåˆ¶è¦æ±‚å¤šå·¥å…·äº¤é”™è°ƒç”¨ï¼ˆå«ç½‘é¡µæœç´¢ã€å›¾åƒæœç´¢ã€é¡µé¢è®¿é—®ã€ä»£ç è§£é‡Šå™¨ï¼‰ï¼Œä¸”è‡³å°‘è·¨è¶Šä¸¤ç±»å·¥å…·ã€‚  \nğŸ”¸æ„å»ºå››é˜¶æ®µä¸¥æ ¼æ•°æ®æµæ°´çº¿ï¼šæ¨¡å‹è¾…åŠ©åˆç­›â†’ä¸“å®¶é‡å†™ä¸ºè‡ªç„¶ç”¨æˆ·è¯·æ±‚â†’æ‰§è¡ŒéªŒè¯ç¡®ä¿å·¥å…·å¿…è¦æ€§â†’åŒè½®äººå·¥å¤æ ¸è§†è§‰ä¾æ®ä¸ç­”æ¡ˆå¯éªŒè¯æ€§ã€‚  \nğŸ”¸é‡‡ç”¨ç»Ÿä¸€å¯æ§å·¥å…·ç¯å¢ƒï¼Œæ‰€æœ‰å·¥å…·å…·å¤‡ç»“æ„åŒ–è¾“å…¥è¾“å‡ºä¸è¯¦ç»†æè¿°ï¼Œæ”¯æŒå¯å¤ç°è¯„ä¼°ã€‚  \nğŸ”¸å¼•å…¥â€œè§†è§‰ä¸­å¿ƒæ€§â€åŸåˆ™ï¼šç¦æ­¢æ–‡æœ¬ç›´ç­”ï¼Œå…³é”®çº¿ç´¢å¿…é¡»æ¥è‡ªå›¾åƒç»†èŠ‚ï¼ˆå¦‚å¾®å°æ ‡è¯†ã€æ¨¡ç³Šæ–‡å­—ã€å¤šè§†è§’æ¯”å¯¹ï¼‰ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸å½“å‰æœ€ä¼˜æ¨¡å‹GEMINI-3-PROæ•´ä½“å‡†ç¡®ç‡ä»…27.3%ï¼Œå¹³å‡éœ€12.67æ­¥å·¥å…·è°ƒç”¨ï¼Œè¯å®é•¿ç¨‹å¤šæ¨¡æ€å·¥å…·ä½¿ç”¨ä»æ˜¯é‡å¤§ç“¶é¢ˆã€‚  \nğŸ”¸è§†è§‰è¯¯è¯†åˆ«æ˜¯é¦–è¦å¤±è´¥åŸå› ï¼ˆå æ¯”è¿‘40%ï¼‰ï¼Œè¿œè¶…çŸ¥è¯†å¹»è§‰ã€è®¡ç®—é”™è¯¯ç­‰ï¼Œå‡¸æ˜¾ç»†ç²’åº¦è§†è§‰ç†è§£çš„è„†å¼±æ€§ã€‚  \nğŸ”¸å¤šå›¾è¾“å…¥åè€Œæå‡æ€§èƒ½ï¼ˆå¦‚GEMINI-3-PROä»23.7%å‡è‡³36.8%ï¼‰ï¼Œè¯´æ˜ä¿¡æ¯äº’è¡¥å¯ç¼“è§£å•å›¾æ­§ä¹‰ï¼Œç“¶é¢ˆä¸åœ¨å›¾åƒæ•°é‡è€Œåœ¨æ¨ç†è¿è´¯æ€§ã€‚  \nğŸ”¸ä¸åŒæ¨¡å‹å·¥å…·åå¥½æ˜¾è‘—ï¼šGPTç³»åˆ—é‡åº¦ä¾èµ–ä»£ç è§£é‡Šå™¨ï¼ˆå°¤å…¶cropæ“ä½œï¼‰ï¼ŒGemini/Claudeæ›´å€¾å‘æ£€ç´¢é©±åŠ¨ï¼Œåæ˜ èƒ½åŠ›åˆ†å¸ƒä¸å‡ã€‚  \nğŸ”¸ç§»é™¤ä»»ä¸€å·¥å…·ç±»å‹å‡å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå…¨å·¥å…·ååŒè®¾ç½®è¡¨ç°æœ€ä½³ï¼ŒéªŒè¯æ··åˆå·¥ä½œæµè®¾è®¡çš„åˆç†æ€§ä¸å¿…è¦æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œæœ€å¤§åˆ›æ–°åœ¨äºå°†â€œçœŸå®æ„Ÿâ€ç³»ç»Ÿæ€§æ³¨å…¥è¯„æµ‹è®¾è®¡ï¼šä»å›¾åƒæ¥æºï¼ˆ30ä¸‡+çœŸå®åœºæ™¯ï¼‰ã€ä»»åŠ¡æ„é€ ï¼ˆç¤¾åŒºæ±‚åŠ©/æ—¥å¸¸æˆªå›¾ï¼‰ã€çº¦æŸè¡¨è¾¾ï¼ˆæ—¶é—´/å®‰å…¨/å…¼å®¹æ€§ç­‰ç”¨æˆ·å¼è¡¨è¿°ï¼‰åˆ°å¤±è´¥å½’å› ï¼ˆèšç„¦è§†è§‰é”šå®šåå·®ï¼‰ï¼Œå…¨é¢è§„é¿äººå·¥ç®€åŒ–é™·é˜±ï¼›å…¶â€œè¶…æŒ‘æˆ˜æ€§â€å¹¶éæºäºæŠ½è±¡éš¾åº¦ï¼Œè€Œæºäºå¯¹ç°å®ä¸–ç•Œè§†è§‰æ¨¡ç³Šæ€§ã€å·¥å…·é“¾æ‰°åŠ¨æ€§ä¸å¤šæ­¥ä¾èµ–è„†å¼±æ€§çš„å¿ å®å»ºæ¨¡ï¼Œä¸ºä¸‹ä¸€ä»£å¤šæ¨¡æ€ä»£ç†å‘å±•æä¾›äº†ä¸å¯æ›¿ä»£çš„æ ‡å°ºã€‚\n    "
    },
    {
        "title": "AuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors",
        "authors": [
            "Abhay Sheshadri",
            "Aidan Ewart",
            "Kai Fronsdal",
            "Isha Gupta",
            "Samuel R. Bowman",
            "Sara Price",
            "Samuel Marks",
            "Rowan Wang"
        ],
        "categories": [
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22755v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22755v1",
        "summary": "We introduce AuditBench, an alignment auditing benchmark. AuditBench consists of 56 language models with implanted hidden behaviors. Each model has one of 14 concerning behaviors--such as sycophantic deference, opposition to AI regulation, or secret geopolitical loyalties--which it does not confess to when directly asked. AuditBench models are highly diverse--some are subtle, while others are overt, and we use varying training techniques both for implanting behaviors and training models not to confess. To demonstrate AuditBench's utility, we develop an investigator agent that autonomously employs a configurable set of auditing tools. By measuring investigator agent success using different tools, we can evaluate their efficacy. Notably, we observe a tool-to-agent gap, where tools that perform well in standalone non-agentic evaluations fail to translate into improved performance when used with our investigator agent. We find that our most effective tools involve scaffolded calls to auxiliary models that generate diverse prompts for the target. White-box interpretability tools can be helpful, but the agent performs best with black-box tools. We also find that audit success varies greatly across training techniques: models trained on synthetic documents are easier to audit than models trained on demonstrations, with better adversarial training further increasing auditing difficulty. We release our models, agent, and evaluation framework to support future quantitative, iterative science on alignment auditing.",
        "tag": "å®‰å…¨å¯¹é½",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22755v1ã€å®‰å…¨å¯¹é½-Anthropicã€‘AuditBench_ Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors.pdf",
        "institution_status": "keep",
        "institution": "Anthropic",
        "first_institution": "Anthropic",
        "institution_category": "å›½å¤–å·¥ä¸šç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šAuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors\nğŸŒæ¥æºï¼šarXiv, 2602.22755v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºå¯å¤ç°çš„å¯¹é½å®¡è®¡åŸºå‡†\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ç³»ç»Ÿè¯„ä¼°ä¸åŒå¯¹é½å®¡è®¡æŠ€æœ¯åœ¨è¯†åˆ«æ¨¡å‹éšè—è¡Œä¸ºæ—¶çš„æœ‰æ•ˆæ€§ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡ã€å¤šæ ·åŒ–ã€å¸¦å¯æ§éšè—è¡Œä¸ºçš„è¯­è¨€æ¨¡å‹åŸºå‡†AuditBenchï¼Œå¹¶åŸºäºå…¶é¦–æ¬¡å®è¯æ­ç¤ºäº†â€œå·¥å…·åˆ°ä»£ç†â€æ•ˆèƒ½æ–­å±‚ç­‰å…³é”®è§„å¾‹ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»º56ä¸ªé«˜å¤šæ ·æ€§ç›®æ ‡æ¨¡å‹ï¼Œè¦†ç›–14ç±»çœŸå®å…³åˆ‡çš„éšè—è¡Œä¸ºï¼ˆå¦‚ç§˜å¯†åœ°ç¼˜å¿ è¯šã€åå¯¹AIç›‘ç®¡ï¼‰ï¼Œé€šè¿‡ä¸¤ç§è¡Œä¸ºæ¤å…¥æ–¹å¼ï¼ˆå¯¹è¯è’¸é¦ vs åˆæˆæ–‡æ¡£å¾®è°ƒï¼‰ä¸ä¸¤ç§å¯¹æŠ—è®­ç»ƒç­–ç•¥ï¼ˆSFT vs KTOï¼‰äº¤å‰ç”Ÿæˆã€‚  \nğŸ”¸è®¾è®¡å¯é…ç½®çš„ investigator agentï¼Œæ”¯æŒé»‘ç›’ï¼ˆå¦‚é¢„å¡«å……ã€ç”¨æˆ·è§’è‰²é‡‡æ ·ï¼‰ã€ç™½ç›’ï¼ˆå¦‚ç¨€ç–è‡ªç¼–ç å™¨ã€æ¿€æ´»ç¥è°•ï¼‰åŠ scaffolded é»‘ç›’ä¸‰ç±»å®¡è®¡å·¥å…·ï¼Œå¹¶ä»¥å¤šè½®äº¤äº’+æœ€ç»ˆè¡Œä¸ºé¢„æµ‹ä¸ºç»Ÿä¸€è¯„ä¼°èŒƒå¼ã€‚  \nğŸ”¸å¼•å…¥â€œå·¥å…·åˆ°ä»£ç†å·®è·â€åˆ†ææ¡†æ¶ï¼ŒåŒºåˆ†å·¥å…·åœ¨é™æ€è¯„æµ‹ä¸­èƒ½å¦è¾“å‡ºè¯æ® vs ä»£ç†èƒ½å¦æœ‰æ•ˆåˆ©ç”¨è¯¥è¯æ®å½¢æˆæ­£ç¡®å‡è®¾ï¼Œå¹¶å½’å› äºå·¥å…·ä½¿ç”¨ä¸è¶³ã€ä¿¡å™ªæ¯”ä½ã€è¯æ®-å‡è®¾è½¬åŒ–éš¾ä¸‰ç±»é—®é¢˜ã€‚  \nğŸ”¸é‡‡ç”¨åˆ†å±‚è¿­ä»£å¼€å‘æµç¨‹ï¼šå…ˆç”¨è½»é‡çº§é™æ€ä»£ç†ï¼ˆsingle-turn/å¼ºè¯æ®ï¼‰å¿«é€Ÿç­›é€‰å·¥å…·å˜ä½“ï¼Œå†åœ¨å®Œæ•´agentä¸ŠéªŒè¯ï¼Œæ˜¾è‘—æå‡å·¥å…·ç ”å‘æ•ˆç‡ã€‚  \nğŸ”¸å…¨é¢æ§åˆ¶å˜é‡ï¼šåœ¨ç›¸åŒåŸºåº§æ¨¡å‹ï¼ˆLlama 3.3 70Bï¼‰ä¸Šç³»ç»Ÿå¯¹æ¯”ä¸åŒè®­ç»ƒé…ç½®å¯¹å®¡è®¡éš¾åº¦çš„å½±å“ï¼Œé¿å…ç»“è®ºä¾èµ–å•ä¸€æ–¹æ³•ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸scaffoldedé»‘ç›’å·¥å…·ï¼ˆå¦‚scaffoldedç”¨æˆ·é‡‡æ ·ï¼‰æ•´ä½“è¡¨ç°æœ€ä¼˜ï¼Œå°¤å…¶åœ¨åˆæˆæ–‡æ¡£å¾®è°ƒæ¨¡å‹ä¸Šæ•ˆæœçªå‡ºï¼›ä½†å…¶ä¼˜åŠ¿é«˜åº¦ä¾èµ–scaffoldingâ€”â€”åŸå§‹é»‘ç›’å·¥å…·ï¼ˆå¦‚æ™®é€šé¢„å¡«å……ï¼‰å‡ ä¹æ— å¢ç›Šã€‚  \nğŸ”¸ç™½ç›’å·¥å…·ï¼ˆå¦‚æ¿€æ´»ç¥è°•ã€SAEsï¼‰åœ¨éƒ¨åˆ†æ¨¡å‹ï¼ˆSDF+SFTï¼‰ä¸Šæä¾›å¯è§‚ä¿¡å·ï¼ˆå¦‚Anti-AI Regulationæ£€æµ‹ç‡ä»0%å‡è‡³88%ï¼‰ï¼Œä½†åœ¨æ›´é²æ£’çš„æ¨¡å‹ï¼ˆTD+KTOï¼‰ä¸Šæ™®éå¤±æ•ˆï¼Œè¡¨æ˜å…¶æœ‰æ•ˆæ€§ä¸¥é‡å—é™äºæ¨¡å‹è®­ç»ƒæ–¹å¼ã€‚  \nğŸ”¸å®¡è®¡éš¾åº¦å­˜åœ¨å·¨å¤§å·®å¼‚ï¼šåˆæˆæ–‡æ¡£å¾®è°ƒæ¨¡å‹æœ€æ˜“å®¡è®¡ï¼Œè€Œå¯¹è¯è’¸é¦+KTOå¯¹æŠ—è®­ç»ƒæ¨¡å‹æœ€éš¾ï¼›ä¸”åœ¨ç®€å•æ¨¡å‹ä¸Šæœ‰æ•ˆçš„å·¥å…·ï¼Œåœ¨å›°éš¾æ¨¡å‹ä¸Šå¸¸åè€ŒæŸå®³æ€§èƒ½ï¼Œå‡¸æ˜¾è·¨é…ç½®è¯„ä¼°å¿…è¦æ€§ã€‚  \nğŸ”¸å­˜åœ¨æ˜¾è‘—â€œå·¥å…·åˆ°ä»£ç†å·®è·â€ï¼šä¾‹å¦‚SAEsèƒ½æå–ç›¸å…³ç‰¹å¾ï¼Œä½†agentå¸¸æ— æ³•æ®æ­¤èšç„¦æé—®ï¼Œè½¬è€Œæ³›æ³›è¯¢é—®è°œé¢˜æˆ–ç¼–ç¨‹ï¼›Petriæ‘˜è¦è™½èƒ½æ­ç¤ºå¤šè½®è¡Œä¸ºï¼Œå´ä½¿agentè¿‡åº¦ä¾èµ–å…¶è¾“å‡ºè€Œå¿½ç•¥ç‹¬ç«‹æ¢ç´¢ã€‚  \nğŸ”¸å¯¹æŠ—è®­ç»ƒæ˜¾è‘—é™ä½æ¨¡å‹â€œè‡ªç™½ç‡â€ï¼ŒKTOä¼˜äºSFTï¼›ä¸”SDFæ¨¡å‹å³ä½¿ç»å¯¹æŠ—è®­ç»ƒä»æ¯”TDæ¨¡å‹æ›´æ˜“è‡ªç™½ï¼Œå°è¯å…¶å†…éƒ¨è¡¨å¾æ›´æ˜“è¢«å®¡è®¡å·¥å…·æ•è·ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºï¼šé¦–æ¬¡å°†å¯¹é½å®¡è®¡ä»é›¶æ•£å·¥å…·æµ‹è¯•å‡ç»´ä¸ºå¯å¤ç°ã€å¯è¿­ä»£ã€å¯é‡åŒ–çš„ç§‘å­¦å®éªŒèŒƒå¼â€”â€”é€šè¿‡ç²¾å¿ƒæ„é€ çš„æ¨¡å‹è°±ç³»ï¼ˆè¦†ç›–è¡Œä¸ºç±»å‹ã€æ¤å…¥æ–¹å¼ã€é²æ£’æ€§å±‚çº§ï¼‰å’Œæ ‡å‡†åŒ–ä»£ç†è¯„ä¼°æ¡†æ¶ï¼Œæ­ç¤ºäº†å®¡è®¡æ•ˆèƒ½çš„æœ¬è´¨æ¡ä»¶æ€§ï¼ˆéç»å¯¹ä¼˜åŠ£ï¼‰ï¼Œå¹¶æŒ‡å‡ºâ€œå·¥å…·è®¾è®¡â€ä¸â€œä»£ç†é›†æˆâ€æ˜¯ä¸¤ä¸ªä¸å¯æ›¿ä»£çš„å…³é”®ç¯èŠ‚ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†åšå®åŸºå‡†ä¸æ·±åˆ»æ–¹æ³•è®ºå¯ç¤ºã€‚\n    "
    },
    {
        "title": "Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents",
        "authors": [
            "Ryan Liu",
            "Dilip Arumugam",
            "Cedegao E. Zhang",
            "Sean Escola",
            "Xaq Pitkow",
            "Thomas L. Griffiths"
        ],
        "categories": [
            "cs.AI",
            "cs.CL",
            "q-bio.NC"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22523v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22523v1",
        "summary": "While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This position paper argues that potential blueprints for designing such modular language agents can be found in the existing literature on cognitive models and artificial intelligence (AI) algorithms. To make this point clear, we formalize the idea of an agent template that specifies roles for individual LLMs and how their functionalities should be composed. We then survey a variety of existing language agents in the literature and highlight their underlying templates derived directly from cognitive models or AI algorithms. By highlighting these designs, we aim to call attention to agent templates inspired by cognitive science and AI as a powerful tool for developing effective, interpretable language agents.",
        "tag": "Agent æ¶æ„",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22523v1ã€Agent æ¶æ„-æ™®æ—æ–¯é¡¿å¤§å­¦ã€‘Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents.pdf",
        "institution_status": "keep",
        "institution": "æ™®æ—æ–¯é¡¿å¤§å­¦ã€éº»çœç†å·¥å­¦é™¢ã€å“¥ä¼¦æ¯”äºšå¤§å­¦ã€å¡å†…åŸºæ¢…éš†å¤§å­¦",
        "first_institution": "æ™®æ—æ–¯é¡¿å¤§å­¦",
        "institution_category": "å…¶ä»–æœºæ„",
        "note": "ğŸ“–æ ‡é¢˜ï¼šCognitive Models and AI Algorithms Provide Templates for Designing Language Agents\nğŸŒæ¥æºï¼šarXiv, 2602.22523v1\n\nç¬”è®°æ ‡é¢˜ï¼šè®¤çŸ¥æ¨¡å‹å¯å‘è¯­è¨€ä»£ç†è®¾è®¡\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ç³»ç»ŸåŒ–åœ°è®¾è®¡å¤šæ¨¡å—è¯­è¨€ä»£ç†ï¼Œè€Œéä¾èµ–è¯•é”™æˆ–ç›´è§‰ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºâ€œä»£ç†æ¨¡æ¿â€æ¦‚å¿µï¼Œè®ºè¯è®¤çŸ¥æ¨¡å‹ä¸ç»å…¸AIç®—æ³•å¯ä½œä¸ºå¯è§£é‡Šã€å¯å¤ç”¨çš„è¯­è¨€ä»£ç†æ¶æ„è“å›¾ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å®šä¹‰ä»£ç†æ¨¡æ¿ä¸ºæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼ŒèŠ‚ç‚¹æ˜¯LLMæˆ–å·¥å…·ï¼Œè¾¹è¡¨ç¤ºæ•°æ®æµä¸æ‰§è¡Œé¡ºåºï¼Œå½¢å¼åŒ–åˆ»ç”»æ¨¡å—è§’è‰²ä¸ç»„åˆé€»è¾‘ã€‚  \nğŸ”¸æ¢³ç†è®¤çŸ¥æ¨¡å‹ä¸‰å¤§ç±»æ¨¡æ¿ï¼šé€šä¿¡é¢†åŸŸé‡‡ç”¨ç†æ€§è¨€è¯­è¡Œä¸ºï¼ˆRSAï¼‰å®ç°é€’å½’ç¤¾ä¼šæ¨ç†ï¼›æ¨ç†ä¸è§„åˆ’é¢†åŸŸå€Ÿé‰´å‰é¢å¶åŠŸèƒ½å»ºæ¨¡ä¸æ€ç»´ aloud åè®®ï¼›è¡¨å¾é¢†åŸŸå¼•å…¥â€œæ€æƒ³è¯­è¨€â€ï¼ˆLoTï¼‰èŒƒå¼ï¼Œä»¥ä»£ç ç”Ÿæˆ+æ‰§è¡Œæ„æˆreasoner-interpreteråŒæ¨¡å—ç»“æ„ã€‚  \nğŸ”¸å½’çº³AIç®—æ³•å››ç±»æ¨¡æ¿ï¼šæœç´¢ç±»ï¼ˆå¦‚Tree of Thoughtså¯¹åº”A*ä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼‰ã€åˆ†æ²»ç±»ï¼ˆå¦‚Least-to-Most promptingï¼‰ã€å¼ºåŒ–å­¦ä¹ ç±»ï¼ˆå¦‚ICPIå®ç°ç­–ç•¥è¿­ä»£ã€PSRLå®ç°åéªŒé‡‡æ ·ï¼‰ã€ä¿¡æ¯å¯¼å‘ç±»ï¼ˆIDSå¹³è¡¡æ¢ç´¢ä¸ä¿¡æ¯å¢ç›Šï¼‰ã€‚  \nğŸ”¸å¼ºè°ƒæ¨¡æ¿ä¼˜åŠ¿åœ¨äºç»§æ‰¿å·²æœ‰ç†è®ºéªŒè¯æ€§â€”â€”é¿å…ä»é›¶ä¼˜åŒ–ï¼Œæå‡å¯è§£é‡Šæ€§ï¼Œå¹¶æ”¯æŒçŠ¶æ€ä¼ é€’ï¼ˆå¦‚æ–‡æœ¬åŒ–â€œåéªŒâ€ï¼‰ã€å¤šè¾“å…¥/è¾“å‡ºç­‰å®é™…éœ€æ±‚ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸ç°æœ‰æˆåŠŸè¯­è¨€ä»£ç†ï¼ˆå¦‚MAPè§„åˆ’å™¨ã€CodeActã€Tree of Thoughtsï¼‰å‡å¯æ˜ å°„åˆ°æ˜ç¡®çš„è®¤çŸ¥æˆ–AIç®—æ³•åŸå‹ï¼ŒéªŒè¯æ¨¡æ¿çš„å®ç”¨æ€§ä¸æ³›åŒ–åŠ›ã€‚  \nğŸ”¸æ¨¡æ¿é©±åŠ¨çš„è®¾è®¡åœ¨é€šä¿¡è´¨é‡ã€å¤šæ­¥æ¨ç†ã€å¤æ‚è§„åˆ’ç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œä¸”äººç±»è¯„ä¼°è®¤å¯å…¶åˆç†æ€§ä¸å¯ç†è§£æ€§ã€‚  \nğŸ”¸å¯¹æ¯”çº¯æ•°æ®é©±åŠ¨çš„è‡ªåŠ¨æ¶æ„æœç´¢ï¼ˆå¦‚GPTSwarmè¿›åŒ–ï¼‰ï¼Œæ¨¡æ¿æ–¹æ³•æ— éœ€æµ·é‡è®­ç»ƒæ•°æ®ï¼Œåœ¨åŒ»ç–—ç­‰é«˜é£é™©åœºæ™¯æ›´å…·å¯è¡Œæ€§ã€‚  \nğŸ”¸å®éªŒè¡¨æ˜ï¼ŒPSRLä¸IDSæ¨¡æ¿åœ¨Wordleç­‰è¯­è¨€åŒ–å†³ç­–ä»»åŠ¡ä¸­ä¿æŒåŸç®—æ³•çš„é«˜æ•ˆæ¢ç´¢ç‰¹æ€§ï¼Œè¯æ˜è®¤çŸ¥/AIåŸç†å¯è·¨æ¨¡æ€è¿ç§»ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥æ–‡æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†â€œä»£ç†è®¾è®¡â€ä»å·¥ç¨‹å®è·µå‡ç»´ä¸ºç§‘å­¦å»ºæ¨¡ï¼šä¸æ˜¯è®©LLMæ‹Ÿäººï¼Œè€Œæ˜¯ç”¨äººç±»å·²éªŒè¯çš„è®¤çŸ¥æœºåˆ¶ä¸AIè®¡ç®—èŒƒå¼çº¦æŸLLMè¡Œä¸ºï¼Œä½¿ä»£ç†å…¼å…·æ€§èƒ½ä¸å¯è§£é‡Šæ€§ã€‚å®ƒæ‰“ç ´äº†LLMé»‘ç®±ä¸è®¤çŸ¥ç§‘å­¦æŠ½è±¡ä¹‹é—´çš„éš”é˜‚ï¼Œä¸ºæ„å»ºå¯ä¿¡ã€å¯æ§ã€å¯æ¼”åŒ–çš„è¯­è¨€æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ–¹æ³•è®ºåŸºçŸ³ã€‚\n    "
    },
    {
        "title": "ContextRL: Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL",
        "authors": [
            "Xingyu Lu",
            "Jinpeng Wang",
            "YiFan Zhang",
            "Shijie Ma",
            "Xiao Hu",
            "Tianke Zhang",
            "Haonan fan",
            "Kaiyu Jiang",
            "Changyi Liu",
            "Kaiyu Tang",
            "Bin Wen",
            "Fan Yang",
            "Tingting Gao",
            "Han Li",
            "Chun Yuan"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22623v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22623v1",
        "summary": "We propose ContextRL, a novel framework that leverages context augmentation to overcome these bottlenecks. Specifically, to enhance Identifiability, we provide the reward model with full reference solutions as context, enabling fine-grained process verification to filter out false positives (samples with the right answer but low-quality reasoning process). To improve Reachability, we introduce a multi-turn sampling strategy where the reward model generates mistake reports for failed attempts, guiding the policy to \"recover\" correct responses from previously all-negative groups. Experimental results on 11 perception and reasoning benchmarks show that ContextRL significantly improves knowledge discovery efficiency. Notably, ContextRL enables the Qwen3-VL-8B model to achieve performance comparable to the 32B model, outperforming standard RLVR baselines by a large margin while effectively mitigating reward hacking. Our in-depth analysis reveals the significant potential of contextual information for improving reward model accuracy and document the widespread occurrence of reward hacking, offering valuable insights for future RLVR research.",
        "tag": "è¿‡ç¨‹å¥–åŠ±",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22623v1ã€è¿‡ç¨‹å¥–åŠ±-æ¸…åå¤§å­¦ã€‘ContextRL_ Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL.pdf",
        "institution_status": "keep",
        "institution": "æ¸…åå¤§å­¦ã€å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ã€ä¸­å›½ç§‘å­¦é™¢",
        "first_institution": "æ¸…åå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šContextRL: Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL\nğŸŒæ¥æºï¼šarXiv, 2602.22623v1\n\nç¬”è®°æ ‡é¢˜ï¼šä¸Šä¸‹æ–‡å¢å¼ºçš„RLVRæ¡†æ¶\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•çªç ´å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ä¸­å› å¥–åŠ±ä¿¡å·ä¸å¯é å’Œæ­£æ ·æœ¬ç¨€ç–å¯¼è‡´çš„çŸ¥è¯†å‘ç°æ•ˆç‡ç“¶é¢ˆï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºContextRLæ¡†æ¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å¢å¼ºå¥–åŠ±æ¨¡å‹ä¸ç­–ç•¥æ¨¡å‹ï¼Œæ˜¾è‘—æå‡MLLMåœ¨RLè®­ç»ƒä¸­çš„çŸ¥è¯†å‘ç°æ•ˆç‡ï¼Œç¼“è§£å¥–åŠ±ä½œå¼Šå¹¶æé«˜å°æ¨¡å‹æ€§èƒ½ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è¯†åˆ«RLVRä¸¤å¤§ä¿¡æ¯ç“¶é¢ˆï¼šå¯è¾¾æ€§ç“¶é¢ˆï¼ˆéš¾é—®é¢˜ä¸‹éš¾ä»¥é‡‡æ ·åˆ°æ­£ç¡®å“åº”ï¼‰ä¸å¯è¾¨è¯†æ€§ç“¶é¢ˆï¼ˆä»…ç”¨æœ€ç»ˆç­”æ¡ˆä½œå‚è€ƒæ˜“äº§ç”Ÿå‡é˜³æ€§ï¼Œå¯¼è‡´å¥–åŠ±ä½œå¼Šï¼‰ã€‚  \nğŸ”¸è®¾è®¡ä¸Šä¸‹æ–‡å¢å¼ºå¥–åŠ±æ¨¡å‹ï¼šå°†å®Œæ•´å‚è€ƒè§£ï¼ˆå«æ¨ç†è¿‡ç¨‹+ç­”æ¡ˆï¼‰ä½œä¸ºè¾“å…¥ï¼Œæ”¯æŒç»†ç²’åº¦è¿‡ç¨‹éªŒè¯ï¼Œé™ä½æ¡ä»¶ç†µğ»(ğ¶|ğ‘‡)ï¼ŒæŠ‘åˆ¶å‡é˜³æ€§ã€‚  \nğŸ”¸æ„å»ºä¸Šä¸‹æ–‡å¢å¼ºç­–ç•¥æ¨¡å‹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µé‡‡æ ·â€”â€”ç¬¬ä¸€é˜¶æ®µå¸¸è§„é‡‡æ ·ï¼›è‹¥å…¨ä¸ºè´Ÿæ ·æœ¬ï¼Œåˆ™ç¬¬äºŒé˜¶æ®µå°†é”™è¯¯æŠ¥å‘Šä¸åŸå§‹æŸ¥è¯¢æ‹¼æ¥ä¸ºæ–°ä¸Šä¸‹æ–‡ï¼Œå¼•å¯¼æ¨¡å‹â€œæ¢å¤â€æ­£ç¡®å“åº”ã€‚  \nğŸ”¸è®¾è®¡æ··åˆè®­ç»ƒæœºåˆ¶ï¼šå¯¹ç¬¬ä¸€é˜¶æ®µå«æ­£æ ·æœ¬çš„ç»„ä½¿ç”¨æ ‡å‡†GRPOä¼˜åŒ–ï¼›å¯¹ç¬¬äºŒé˜¶æ®µç”Ÿæˆçš„æ­£æ ·æœ¬ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å›æ»šè½¬ä¸ºå•è½®æ ·æœ¬ï¼Œå¹¶åŠ å…¥ç¼©æ”¾ä¼˜åŠ¿å‡½æ•°ä¸é€‰æ‹©æ€§KLæ­£åˆ™åŒ–è¿›è¡Œç¨³å®šè®­ç»ƒã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸ä¸Šä¸‹æ–‡å¢å¼ºä½¿32Bå¥–åŠ±æ¨¡å‹å¯¹å‡é˜³æ€§æ ·æœ¬çš„è¯†åˆ«ç‡ä»50.03%æå‡è‡³81.98%ï¼Œä¸”ç¼©å°äº†32Bä¸235Bæ¨¡å‹é—´çš„æ€§èƒ½å·®è·ï¼Œè¯æ˜å°æ¨¡å‹å€ŸåŠ©ä¸°å¯Œä¸Šä¸‹æ–‡å¯æ¥è¿‘å¤§æ¨¡å‹åˆ¤åˆ«èƒ½åŠ›ã€‚  \nğŸ”¸å‡é˜³æ€§æ ·æœ¬ä¸¥é‡æŸå®³æ¨¡å‹æ€§èƒ½ï¼šSFTå®éªŒæ˜¾ç¤ºï¼Œè®­ç»ƒæ•°æ®ä¸­æ¯å¢åŠ 10%å‡é˜³æ€§ï¼Œæ•°å­¦ç±»åŸºå‡†å¹³å‡æ€§èƒ½ä¸‹é™çº¦0.5â€“1.5ä¸ªç™¾åˆ†ç‚¹ã€‚  \nğŸ”¸ContextRLå•è½®è®­ç»ƒå³å¯ä½¿Qwen3-VL-8Båœ¨11ä¸ªåŸºå‡†ä¸Šå¹³å‡è¶…è¶ŠSFTã€GRPOã€DAPOç­‰åŸºçº¿5.25%ï¼ˆæ¨ç†ï¼‰å’Œ5.91%ï¼ˆæ„ŸçŸ¥ï¼‰ï¼Œæ€§èƒ½é€¼è¿‘32Bæ¨¡å‹ã€‚  \nğŸ”¸å®šé‡åˆ†æè¡¨æ˜ContextRLçš„ä¿¡æ¯å¢ç›Šè¾¾çº¦17%ï¼Œå…¶ä¸­8.9%æ¥è‡ªå‡é˜³æ€§æ¶ˆé™¤ï¼Œ8.56%æ¥è‡ªç¬¬äºŒé˜¶æ®µæˆåŠŸæ¢å¤æ­£æ ·æœ¬ï¼Œè¯å®å…¶åŒè·¯å¾„çªç ´ç“¶é¢ˆçš„æœ‰æ•ˆæ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†â€œä¸Šä¸‹æ–‡â€ä»æç¤ºå·¥ç¨‹å±‚é¢æå‡ä¸ºRLç³»ç»Ÿçº§è®¾è®¡è¦ç´ ï¼Œä¸ä»…è§£å†³å¥–åŠ±ä½œå¼Šè¿™ä¸€é•¿æœŸéšæ‚£ï¼Œæ›´é€šè¿‡å¯è§£é‡Šçš„é”™è¯¯åé¦ˆæœºåˆ¶èµ‹äºˆç­–ç•¥æ¨¡å‹è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›ï¼›å…¶æå‡ºçš„å¯è¾¾æ€§-å¯è¾¨è¯†æ€§åŒç“¶é¢ˆåˆ†ææ¡†æ¶ï¼Œä¸ºåç»­RLVRç ”ç©¶æä¾›äº†æ™®é€‚æ€§è¯Šæ–­å·¥å…·ã€‚\n    "
    },
    {
        "title": "Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning",
        "authors": [
            "Hao Yu",
            "Shuning Jia",
            "Guanghao Li",
            "Wenhao Jiang",
            "Chun Yuan"
        ],
        "categories": [
            "cs.LG"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22703v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22703v1",
        "summary": "Vision-language models (VLMs) often struggle with geometric reasoning due to their limited perception of fundamental diagram elements. To tackle this challenge, we introduce GeoPerceive, a benchmark comprising diagram instances paired with domain-specific language (DSL) representations, along with an efficient automatic data generation pipeline. This design enables the isolated evaluation of geometric perception independently from reasoning. To exploit the data provided by GeoPerceive for enhancing the geometric perception capabilities of VLMs, we propose GeoDPO, a translator-guided reinforcement learning (RL) framework. GeoDPO employs an NL-to-DSL translator, which is trained on synthetic pairs generated by the data engine of GeoPerceive, to bridge natural language and DSL. This translator facilitates the computation of fine-grained, DSL-level scores, which serve as reward signals in reinforcement learning. We assess GeoDPO on both in-domain and out-of-domain datasets, spanning tasks in geometric perception as well as downstream reasoning. Experimental results demonstrate that, while supervised fine-tuning (SFT) offers only marginal improvements and may even impair performance in out-of-domain scenarios, GeoDPO achieves substantial gains: $+26.5\\%$ on in-domain data, $+8.0\\%$ on out-of-domain data, and $+39.0\\%$ on downstream reasoning tasks. These findings underscore the superior performance and generalization ability of GeoDPO over SFT. All codes are released at https://github.com/Longin-Yu/GeoPerceive\n  to ensure reproducibility.",
        "tag": "å‡ ä½•è¯„ä¼°åŸºå‡†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22703v1ã€å‡ ä½•è¯„ä¼°åŸºå‡†-æ¸…åå¤§å­¦ã€‘Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning.pdf",
        "institution_status": "keep",
        "institution": "æ¸…åå¤§å­¦ã€å¹¿ä¸œäººå·¥æ™ºèƒ½ä¸æ•°å­—ç»æµå®éªŒå®¤ï¼ˆæ·±åœ³ï¼‰",
        "first_institution": "æ¸…åå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šEnhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning\nğŸŒæ¥æºï¼šarXiv, 2602.22703v1\n\nç¬”è®°æ ‡é¢˜ï¼šæå‡VLMå‡ ä½•æ„ŸçŸ¥èƒ½åŠ›\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•å‡†ç¡®è¯„ä¼°å¹¶æœ‰æ•ˆå¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹å¯¹å‡ ä½•å›¾å…ƒï¼ˆç‚¹ã€çº¿ã€åœ†åŠå…¶ç©ºé—´å…³ç³»ï¼‰çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºGEOPERCEIVEåŸºå‡†ä¸GEODPOæ¡†æ¶ï¼Œé¦–æ¬¡å®ç°å‡ ä½•æ„ŸçŸ¥çš„è§£è€¦å¼è¯„æµ‹ä¸ translator-guided å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡VLMåœ¨åŸŸå†…ã€åŸŸå¤–åŠä¸‹æ¸¸æ¨ç†ä»»åŠ¡ä¸­çš„å‡ ä½•ç†è§£é²æ£’æ€§ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºGEODSLâ€”â€”ä¸€ç§ä¸€ä¸€æ˜ å°„ã€æ— æ­§ä¹‰çš„å‡ ä½•é¢†åŸŸä¸“ç”¨è¯­è¨€ï¼Œç¡®ä¿æ¯ä¸ªå›¾å¯¹åº”å”¯ä¸€ç¨‹åºï¼Œæ”¯æ’‘ç²¾ç¡®ç¨‹åºçº§è¯„ä¼°ã€‚  \nğŸ”¸è®¾è®¡GEOPERCEIVEè‡ªåŠ¨æ•°æ®å¼•æ“ï¼šç”Ÿæˆå¼•æ“é‡‡æ ·DSLç¨‹åºï¼Œæ±‚è§£å¼•æ“é€šè¿‡å¯å¾®ä¼˜åŒ–æ¸²æŸ“åƒç´ å›¾ï¼Œæ”¯æŒå¤æ‚åº¦å¯æ§çš„å¤§è§„æ¨¡åˆæˆæ•°æ®ç”Ÿæˆã€‚  \nğŸ”¸æå‡ºGEODPOæ¡†æ¶ï¼šä¸ç›´æ¥å¾®è°ƒVLMè¾“å‡ºDSLï¼Œè€Œæ˜¯è®­ç»ƒNL-to-DSLç¿»è¯‘å™¨ï¼Œå°†å…¶è½¯åŒ¹é…åˆ†æ•°è½¬åŒ–ä¸ºDPOå¥–åŠ±ä¿¡å·ï¼Œå®ç°è‡ªç„¶è¯­è¨€è¾“å‡ºä¸‹çš„ç»†ç²’åº¦å‡ ä½•ç›‘ç£ã€‚  \nğŸ”¸é‡‡ç”¨åå¥½å­¦ä¹ èŒƒå¼ï¼šåŸºäºå‚è€ƒæ¨¡å‹é‡‡æ ·å¤šæ¡NLæè¿°ï¼ŒæŒ‰ç¿»è¯‘å¾—åˆ†æ’åºæ„é€ èƒœ/è´Ÿæ ·æœ¬å¯¹ï¼Œç”¨DPOæŸå¤±å¯¹é½å‡ ä½•ä¸€è‡´æ€§åå¥½ã€‚  \nğŸ”¸ä¿æŒNLé¢„è®­ç»ƒæµå½¢ï¼šæ•´ä¸ªæµç¨‹ä¸­VLMå§‹ç»ˆè¾“å‡ºè‡ªç„¶è¯­è¨€ï¼Œé¿å…å› å¼ºåˆ¶DSLè¾“å‡ºå¯¼è‡´çš„åˆ†å¸ƒåç§»ä¸æ•°æ®é¥¥æ¸´é—®é¢˜ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸GEODPOåœ¨GEOPERCEIVEä¸»æµ‹è¯•é›†ä¸Šå¹³å‡æå‡+26.5%ï¼Œè¿œè¶…SFTï¼ˆ+10.5%ï¼‰ï¼Œä¸”åœ¨ç‚¹ã€çº¿ã€çº¦æŸç­‰å„è¦ç´ ä¸Šå‡ç¨³å®šå¢ç›Šï¼Œå°¤å…¶çº¦æŸç±»æå‡è¾¾+19.3%ã€‚  \nğŸ”¸åœ¨äººå·¥æ„å»ºçš„OODæµ‹è¯•é›†ä¸Šï¼ŒGEODPOä»è·+8.0%æå‡ï¼Œè€ŒSFTåœ¨å¤šä¸ªæ¨¡å‹ä¸Šå‡ºç°æ€§èƒ½ä¸‹é™ï¼ŒéªŒè¯å…¶å¼ºæ³›åŒ–èƒ½åŠ›ã€‚  \nğŸ”¸ä¸‹æ¸¸å‡ ä½•æ¨ç†ä»»åŠ¡ï¼ˆMathVistaå­é›†ï¼‰æå‡è¾¾+39.0%ï¼Œè¡¨æ˜å‡ ä½•æ„ŸçŸ¥å¢å¼ºåˆ‡å®ä¼ å¯¼è‡³é«˜å±‚æ¨ç†ï¼Œéä»…è¡¨å±‚æ‹Ÿåˆã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ç¿»è¯‘å™¨è´¨é‡ç›´æ¥å½±å“DPOæ•ˆæœï¼šç¿»è¯‘å™¨F1æ¯é™5%ï¼ŒGEODPOæ•´ä½“åˆ†ä¸‹é™çº¦1.5%ï¼Œå‡¸æ˜¾translatorå¼•å¯¼çš„å…³é”®ä½œç”¨ã€‚  \nğŸ”¸å®šæ€§åˆ†ææ˜¾ç¤ºï¼ŒGEODPOæ˜¾è‘—å‡å°‘å‡ ä½•å¹»è§‰ï¼ˆå¦‚è¯¯åˆ¤ç›¸åˆ‡ä¸ºç›¸äº¤ã€æ··æ·†ç‚¹çº¿éš¶å±å…³ç³»ï¼‰ï¼Œæå‡å›¾å…ƒæ¥åœ°å‡†ç¡®æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºâ€œè¯„æµ‹â€”å»ºæ¨¡â€”ä¼˜åŒ–â€ä¸‰é‡è§£è€¦ï¼šGEODSLå®ç°æ„ŸçŸ¥èƒ½åŠ›çš„å¯å®šä¹‰ã€å¯æµ‹é‡ï¼›GEOPERCEIVEæä¾›æ— é™å¯æ§çš„åˆæˆæ•°æ®æºï¼›GEODPOä»¥translatorä¸ºæ¡¥æ¢ï¼Œå°†å¼ºåŒ–å­¦ä¹ å¼•å…¥NLè¾“å‡ºèŒƒå¼ï¼Œåœ¨ä¸ç ´åé¢„è®­ç»ƒåˆ†å¸ƒå‰æä¸‹å®ç°ç»†ç²’åº¦å‡ ä½•å¯¹é½ã€‚è¯¥èŒƒå¼ä¸ºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„ç‰¹å®šèƒ½åŠ›å®šå‘å¢å¼ºæä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization",
        "authors": [
            "Zeyuan Liu",
            "Jeonghye Kim",
            "Xufang Luo",
            "Dongsheng Li",
            "Yuqing Yang"
        ],
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.23008v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23008v1",
        "summary": "Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO$^2$), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO$^2$ achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO$^2$ demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO$^2$ as a promising framework for building more exploratory and generalizable LLM-based agents.",
        "tag": "Agent è®°å¿†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.23008v1ã€Agent è®°å¿†-å¾®è½¯ã€‘Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization.pdf",
        "institution_status": "keep",
        "institution": "å¾®è½¯ã€å¡å†…åŸºæ¢…éš†å¤§å­¦",
        "first_institution": "å¾®è½¯",
        "institution_category": "å›½å¤–å·¥ä¸šç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šExploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization\nğŸŒæ¥æºï¼šarXiv, 2602.23008v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ··åˆç­–ç•¥å¢å¼ºæ¢ç´¢\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è§£å†³å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å› è¿‡åº¦ä¾èµ–å…ˆéªŒçŸ¥è¯†è€Œå¯¼è‡´çš„ç³»ç»Ÿæ€§æ¢ç´¢ä¸è¶³é—®é¢˜ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºEMPO2æ¡†æ¶ï¼Œé€šè¿‡è®°å¿†å¢å¼ºä¸æ··åˆï¼ˆon-å’Œoff-policyï¼‰ä¼˜åŒ–ååŒæå‡LLMæ™ºèƒ½ä½“çš„æ¢ç´¢èƒ½åŠ›ä¸æ³›åŒ–æ€§ï¼Œåœ¨æ— éœ€å‚æ•°æ›´æ–°ä¸‹å³å¯å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡åŒæ¨¡ rollout æœºåˆ¶ï¼šåœ¨é‡‡æ ·æ—¶åŠ¨æ€åˆ‡æ¢â€œæ— è®°å¿†æç¤ºâ€ä¸â€œè®°å¿†å¢å¼ºæç¤ºâ€ï¼Œåè€…åŸºäºæ£€ç´¢è‡ªå»ºè®°å¿†åº“çš„åæ€æ€§æç¤ºï¼ˆtipsï¼‰å¼•å¯¼åŠ¨ä½œç”Ÿæˆã€‚  \nğŸ”¸æ„å»ºåŒè½¨ update æœºåˆ¶ï¼šå¯¹è®°å¿†å¢å¼ºè½¨è¿¹åˆ†åˆ«æ‰§è¡Œ on-policy æ›´æ–°ï¼ˆä¿ç•™æç¤ºæ¡ä»¶ï¼‰å’Œ off-policy æ›´æ–°ï¼ˆå‰¥ç¦»æç¤ºã€ä»…ç”¨çŠ¶æ€-ä»»åŠ¡æ¡ä»¶é‡ç®—æ¦‚ç‡ï¼‰ï¼Œå®ç°å¤–éƒ¨å¼•å¯¼å‘å†…åœ¨ç­–ç•¥çš„çŸ¥è¯†è’¸é¦ã€‚  \nğŸ”¸å¼•å…¥è‡ªç”Ÿæˆè®°å¿†æ¨¡å—ï¼šç”±ç­–ç•¥è‡ªèº«åœ¨æ¯è½®ç»“æŸæ—¶æ€»ç»“å¤±è´¥ç»éªŒç”Ÿæˆ tipï¼Œå¹¶å­˜å…¥éå‚æ•°è®°å¿†ç¼“å†²åŒºï¼Œæ”¯æŒè·¨è½¨è¿¹è¿ç»­åæ€ä¸çº é”™ã€‚  \nğŸ”¸èåˆå†…åœ¨å¥–åŠ±æœºåˆ¶ï¼šåŸºäºçŠ¶æ€æ–°é¢–æ€§ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼ï¼‰æä¾›ç¨€ç–å†…åœ¨å¥–åŠ±ï¼Œç»´æŒç­–ç•¥ç†µå¹¶æ¿€åŠ±å¯¹æœªè§çŠ¶æ€çš„ä¸»åŠ¨æ¢ç´¢ã€‚  \nğŸ”¸é‡‡ç”¨ token çº§é‡è¦æ€§é‡‡æ ·æ©ç ï¼šé’ˆå¯¹ä½æ¦‚ç‡ token å¼•å‘çš„è®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼Œè®¾å®šæ¦‚ç‡é˜ˆå€¼Î´å±è”½å…¶ä¼˜åŠ¿é¡¹ï¼Œæå‡ off-policy è®­ç»ƒé²æ£’æ€§ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸EMPO2åœ¨ScienceWorldå’ŒWebShopä¸Šåˆ†åˆ«è¶…è¶Šå¼ºåŸºçº¿GRPOè¾¾128.6%å’Œ11.3%ï¼Œä¸”è®­ç»ƒæ›²çº¿æŒç»­ä¸Šå‡ï¼Œé¿å…GRPOæ—©æ”¶æ•›è‡³æ¬¡ä¼˜è§£ã€‚  \nğŸ”¸åœ¨OODè¿ç§»å®éªŒä¸­ï¼Œä»…éœ€æ•°æ¬¡å¸¦è®°å¿†æ¨ç†ï¼ˆé›¶å‚æ•°æ›´æ–°ï¼‰ï¼ŒEMPO2å³æ˜¾è‘—ä¼˜äºGRPOï¼ŒéªŒè¯å…¶è®°å¿†é©±åŠ¨çš„å¿«é€Ÿé€‚åº”èƒ½åŠ›ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šç§»é™¤ off-policy æˆ– on-policy with memory æ¨¡å—å‡å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼ŒäºŒè€…äº’è¡¥â€”â€”å‰è€…åŠ é€ŸçŸ¥è¯†å†…åŒ–ï¼Œåè€…ä¿éšœè®­ç»ƒç¨³å®šæ€§ã€‚  \nğŸ”¸å†…åœ¨å¥–åŠ±è™½ä¸å½±å“æœ€ç»ˆæ€§èƒ½ä¸Šé™ï¼Œä½†ç§»é™¤åå­¦ä¹ æ˜“æ—©è¡°ï¼Œè¯å®å…¶å¯¹ç»´æŒæ¢ç´¢å¤šæ ·æ€§ä¸å¯æˆ–ç¼ºã€‚  \nğŸ”¸è®°å¿†æœºåˆ¶å¸¦æ¥çº¦19%é¢å¤– rolloutå¼€é”€ï¼Œä½†æ—¶é—´-æ€§èƒ½æ›²çº¿æ˜¾ç¤ºEMPO2ä»æ˜¾è‘—ä¼˜äºGRPOï¼Œè¯æ˜å…¶æ¢ç´¢æ•ˆç‡å¢ç›Šè¿œè¶…è®¡ç®—æˆæœ¬ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºæ‰“ç ´â€œè®°å¿†å³è¾…åŠ©â€çš„æƒ¯æ€§æ€ç»´ï¼Œå°†è®°å¿†å»ºæ¨¡ä¸ºå¯è¢«ç­–ç•¥è‡ªä¸»ç”Ÿæˆã€æ£€ç´¢ã€è’¸é¦çš„åŠ¨æ€è®¤çŸ¥ scaffoldï¼›é€šè¿‡ on/off-policy çš„è€¦åˆè®¾è®¡ï¼Œé¦–æ¬¡åœ¨LLM-RLä¸­å®ç°â€œè®°å¿†ä¿ƒè¿›æ¢ç´¢â†’æ¢ç´¢ç”Ÿæˆé«˜è´¨é‡è½¨è¿¹â†’è½¨è¿¹åå“ºå‚æ•°ä¼˜åŒ–â†’å‚æ•°å¸æ”¶è®°å¿†ä»·å€¼â€çš„é—­ç¯è¿›åŒ–ï¼Œå…¼é¡¾çŸ­æœŸé€‚åº”æ€§ä¸é•¿æœŸæ³›åŒ–æ€§ã€‚\n    "
    },
    {
        "title": "From Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models",
        "authors": [
            "Hongrui Jia",
            "Chaoya Jiang",
            "Shikun Zhang",
            "Wei Ye"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22859v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22859v1",
        "summary": "As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) methods mature, LMMs have made notable progress in complex reasoning and decision making. Yet training still relies on static data and fixed recipes, making it difficult to diagnose capability blind spots or provide dynamic, targeted reinforcement. Motivated by findings that test driven error exposure and feedback based correction outperform repetitive practice, we propose Diagnostic-driven Progressive Evolution (DPE), a spiral loop where diagnosis steers data generation and reinforcement, and each iteration re-diagnoses the updated model to drive the next round of targeted improvement. DPE has two key components. First, multiple agents annotate and quality control massive unlabeled multimodal data, using tools such as web search and image editing to produce diverse, realistic samples. Second, DPE attributes failures to specific weaknesses, dynamically adjusts the data mixture, and guides agents to generate weakness focused data for targeted reinforcement. Experiments on Qwen3-VL-8B-Instruct and Qwen2.5-VL-7B-Instruct show stable, continual gains across eleven benchmarks, indicating DPE as a scalable paradigm for continual LMM training under open task distributions. Our code, models, and data are publicly available at https://github.com/hongruijia/DPE.",
        "tag": "è¿­ä»£è®­ç»ƒ",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22859v1ã€è¿­ä»£è®­ç»ƒ-åŒ—äº¬å¤§å­¦ã€‘From Blind Spots to Gains_ Diagnostic-Driven Iterative Training for Large Multimodal Models.pdf",
        "institution_status": "keep",
        "institution": "åŒ—äº¬å¤§å­¦ã€å±±ä¸œå¤§å­¦",
        "first_institution": "åŒ—äº¬å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šFrom Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models\nğŸŒæ¥æºï¼šarXiv, 2602.22859v1\n\nç¬”è®°æ ‡é¢˜ï¼šè¯Šæ–­é©±åŠ¨çš„æ¸è¿›å¼è®­ç»ƒ\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨æ•°æ®ç¨€ç¼ºå’Œé•¿å°¾ä»»åŠ¡åˆ†å¸ƒä¸‹ï¼Œå®ç°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ç¨³å®šã€æŒç»­çš„èƒ½åŠ›æå‡ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºè¯Šæ–­é©±åŠ¨çš„æ¸è¿›å¼è¿›åŒ–ï¼ˆDPEï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¯è§£é‡Šçš„å¤±è´¥å½’å› ã€åŠ¨æ€æ•°æ®æ··åˆè°ƒæ§ä¸å¤šæ™ºèƒ½ä½“ååŒç”Ÿæˆï¼Œå®ç°é’ˆå¯¹èƒ½åŠ›ç›²ç‚¹çš„é—­ç¯å¼è¿­ä»£å¼ºåŒ–è®­ç»ƒã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºä¸¤é˜¶æ®µé—­ç¯ï¼šå…ˆç”±è¯Šæ–­æ™ºèƒ½ä½“è¯†åˆ«æ¨¡å‹åœ¨12ç±»èƒ½åŠ›ç»´åº¦ä¸Šçš„å…·ä½“å¤±è´¥æ¨¡å¼ï¼ˆå¦‚OCRæ¼çº¿ã€å›¾è¡¨å•ä½å¿½ç•¥ã€æ•°å­¦æ­¥éª¤ç¼ºå¤±ï¼‰ï¼Œå†æ®æ­¤ç”Ÿæˆç»“æ„åŒ–è¯Šæ–­æŠ¥å‘Šï¼ˆå«ç±»åˆ«æƒé‡Î±ã€å¼±ç‚¹æ‘˜è¦Fã€ç”ŸæˆæŒ‡ä»¤Hï¼‰ã€‚  \nğŸ”¸è®¾è®¡å¤šæ™ºèƒ½ä½“é—®é¢˜ç”Ÿæˆç³»ç»Ÿï¼šåŒ…å«è§„åˆ’è€…ã€å›¾åƒé€‰æ‹©å™¨ã€é—®é¢˜ç”Ÿæˆå™¨å’ŒéªŒè¯è€…å››æ¨¡å—ï¼›è§„åˆ’è€…æŒ‰è¯Šæ–­æŠ¥å‘Šåˆ†é…ç±»åˆ«é…é¢ä¸éš¾åº¦çº¦æŸï¼›å›¾åƒé€‰æ‹©å™¨ä»å¤–éƒ¨æ± æ£€ç´¢å¹¶ç¼–è¾‘å›¾åƒï¼ˆæ”¯æŒè£å‰ªã€å åŠ ã€å¤šå›¾èåˆï¼‰ï¼Œçªç ´é™æ€æ•°æ®è§†è§‰å¤šæ ·æ€§ç“¶é¢ˆã€‚  \nğŸ”¸é‡‡ç”¨å¯æ§ç”Ÿæˆä¸ä¸¥æ ¼è´¨é‡é—¨æ§ï¼šç”Ÿæˆæ ·æœ¬éœ€åŒæ—¶æ»¡è¶³ç¡¬æ€§ç±»åˆ«é…é¢çº¦æŸå’Œå››é¡¹éªŒè¯ï¼ˆç±»åˆ«ä¸€è‡´æ€§ã€å¯è§£æ€§ã€ç­”æ¡ˆå¯éªŒè¯æ€§ã€æ ¼å¼åˆè§„æ€§ï¼‰ï¼Œæ‹’ç»ä¸åˆæ ¼æ ·æœ¬ä»¥æŠ‘åˆ¶åˆ†å¸ƒæ¼‚ç§»å’Œå™ªå£°ç´¯ç§¯ã€‚  \nğŸ”¸ä½¿ç”¨GRPOå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç»“åˆç»„å½’ä¸€åŒ–ä¼˜åŠ¿ä¼°è®¡ä¸æœ€å¤§ç†µè§†è§’ä¸‹çš„éš¾åº¦æ„ŸçŸ¥è¿‡æ»¤ï¼Œä¼˜å…ˆä¿ç•™ä¸­ç­‰éš¾åº¦æ ·æœ¬ï¼Œæå‡å•æ ·æœ¬å­¦ä¹ æ•ˆç‡ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸DPEåœ¨Qwen2.5-VL-7Bå’ŒQwen3-VL-8Bä¸Šå‡å®ç°11é¡¹åŸºå‡†çš„ç¨³å®šæå‡ï¼ˆå¦‚MMMU+2.0ã€CharXiv RQ+4.11ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºVisPlayç­‰è‡ªæ¼”åŒ–æ–¹æ³•ï¼Œä¸”é¿å…æ€§èƒ½æŒ¯è¡æˆ–é€€åŒ–ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šå»é™¤è¯Šæ–­æ¨¡å—åï¼Œå„ä»»åŠ¡å‡†ç¡®ç‡å¢é•¿åœæ»ç”šè‡³ä¸‹é™ï¼ˆå¦‚MathVisionä»26.51â†’25.99ï¼‰ï¼Œè¯å®è¯Šæ–­æ˜¯ç»´æŒè¿›åŒ–æ–¹å‘æ­£ç¡®æ€§çš„æ ¸å¿ƒã€‚  \nğŸ”¸å›¾åƒæ£€ç´¢ä¸ç¼–è¾‘æ¨¡å—å¯¹OCRå’Œæ•°å­¦è§†è§‰ä»»åŠ¡è‡³å…³é‡è¦ï¼Œç§»é™¤åCharXivä¸‹é™2.81åˆ†ï¼Œè¯´æ˜è§†è§‰å¤šæ ·æ€§ç›´æ¥ç¼“è§£é•¿å°¾åœºæ™¯è¦†ç›–ä¸è¶³é—®é¢˜ã€‚  \nğŸ”¸å¤šæ ·æ€§åˆ†ææ˜¾ç¤ºï¼šDPEç”Ÿæˆçš„æ•°æ®åœ¨æ–‡æœ¬ä¸å›¾åƒåµŒå…¥ç©ºé—´çš„å¹³å‡ä½™å¼¦è·ç¦»æŒç»­é«˜äºVisPlayï¼Œè¯æ˜å…¶æœ‰æ•ˆæŠ‘åˆ¶äº†æ¨¡æ¿å¤ç”¨ä¸åˆ†å¸ƒåç¼©ã€‚  \nğŸ”¸äººå·¥è¯„ä¼°è¡¨æ˜DPEç”Ÿæˆé—®é¢˜çš„è´¨é‡å¾—åˆ†ï¼ˆQSâ‰ˆ4.8ï¼‰è¿œè¶…VisPlayï¼ˆQSâ‰ˆ3.3ï¼‰ï¼Œå°¤å…¶åœ¨å¯è§£æ€§ï¼ˆ4.9 vs 2.98ï¼‰ä¸ç­”æ¡ˆæ­£ç¡®æ€§ï¼ˆ4.7 vs 3.08ï¼‰ä¸Šä¼˜åŠ¿æ˜¾è‘—ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡çš„åˆ›æ–°ç‚¹åœ¨äºå°†æ•™è‚²å¿ƒç†å­¦ä¸­çš„â€œè¯Šæ–­â€”çŸ«æ­£â€æœºåˆ¶ç³»ç»ŸåŒ–å¼•å…¥LMMè®­ç»ƒèŒƒå¼ï¼Œé¦–æ¬¡å®ç°äº†èƒ½åŠ›ç¼ºé™·çš„æ˜¾å¼å½’å› ã€æ•°æ®åˆ†å¸ƒçš„åŠ¨æ€è°ƒæ§ä¸è§†è§‰å†…å®¹çš„ä¸»åŠ¨æ„é€ ä¸‰è€…çš„æœ‰æœºç»Ÿä¸€ï¼›å…¶è¯Šæ–­æŠ¥å‘Šç»“æ„åŒ–ã€ç”Ÿæˆè¿‡ç¨‹å·¥å…·åŒ–ã€éªŒè¯æ ‡å‡†å¯é‡åŒ–çš„è®¾è®¡ï¼Œä¸ºè§£å†³è‡ªæ¼”åŒ–è®­ç»ƒä¸­çš„é»‘ç®±æ€§ã€ä¸ç¨³å®šæ€§ä¸é•¿å°¾è¦†ç›–éš¾æä¾›äº†å¯å¤ç°ã€å¯æ‰©å±•çš„æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "General Agent Evaluation",
        "authors": [
            "Elron Bandel",
            "Asaf Yehudai",
            "Lilach Eden",
            "Yehoshua Sagron",
            "Yotam Perlitz",
            "Elad Venezian",
            "Natalia Razinkov",
            "Natan Ergas",
            "Shlomit Shachor Ifergan",
            "Segev Shlomov",
            "Michal Jacovi",
            "Leshem Choshen",
            "Liat Ein-Dor",
            "Yoav Katz",
            "Michal Shmueli-Scheuer"
        ],
        "categories": [
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22953v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22953v1",
        "summary": "The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.",
        "tag": "é€šç”¨ Agent è¯„ä¼°",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22953v1ã€é€šç”¨ Agent è¯„ä¼°-IBM Researchã€‘General Agent Evaluation.pdf",
        "institution_status": "keep",
        "institution": "IBM Researchã€MIT",
        "first_institution": "IBM Research",
        "institution_category": "å…¶ä»–æœºæ„",
        "note": "ğŸ“–æ ‡é¢˜ï¼šGeneral Agent Evaluation\nğŸŒæ¥æºï¼šarXiv, 2602.22953v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºé€šç”¨æ™ºèƒ½ä½“è¯„ä¼°æ–°èŒƒå¼\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ç³»ç»Ÿã€å…¬å¹³åœ°è¯„ä¼°ä¸ä¾èµ–é¢†åŸŸå®šåˆ¶çš„é€šç”¨æ™ºèƒ½ä½“åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­çš„çœŸå®æ³›åŒ–èƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºé¦–ä¸ªé¢å‘é€šç”¨æ™ºèƒ½ä½“çš„æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶Exgenticï¼ŒåŒ…å«ç»Ÿä¸€åè®®ã€å¼€æºå·¥å…·é“¾ä¸å…¬å¼€æ’è¡Œæ¦œï¼Œé¦–æ¬¡å®ç°è·¨ç¯å¢ƒã€è·¨æ¶æ„çš„æ— ååŸºå‡†æµ‹è¯•ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æå‡ºUnified Protocolâ€”â€”ä¸€ç§è§£è€¦æ™ºèƒ½ä½“æ¥å£ä¸åŸºå‡†ä»»åŠ¡çš„ä¸­ä»‹åè®®ï¼Œå®šä¹‰ä»»åŠ¡ï¼ˆtaskï¼‰ã€ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰ã€åŠ¨ä½œï¼ˆactionsï¼‰ä¸‰è¦ç´ ï¼Œæ”¯æŒCLIã€MCPã€å·¥å…·è°ƒç”¨ç­‰å¼‚æ„äº¤äº’æ–¹å¼çš„æ— æŸæ˜ å°„ã€‚  \nğŸ”¸è®¾è®¡å¤–éƒ¨é€‚é…å™¨æœºåˆ¶ï¼Œé¿å…ä¾µå…¥å¼ä¿®æ”¹ç¬¬ä¸‰æ–¹æ™ºèƒ½ä½“æˆ–åŸºå‡†ä»£ç ï¼Œé€šè¿‡è¿›ç¨‹éš”ç¦»+åè®®ç¿»è¯‘å®ç°å³æ’å³ç”¨é›†æˆã€‚  \nğŸ”¸æ„å»ºExgenticè¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒå¹¶è¡Œæ‰§è¡Œã€è½¨è¿¹è®°å½•ã€æˆæœ¬è®¡é‡ä¸ç»“æœæ ‡å‡†åŒ–ï¼Œæä¾›Python APIä¸GUIåŒå…¥å£ã€‚  \nğŸ”¸å‘å¸ƒOpen General Agent Leaderboardï¼Œè¦†ç›–5ç±»ä¸»æµæ™ºèƒ½ä½“æ¶æ„ã€3ä¸ªå‰æ²¿å¤§æ¨¡å‹ã€6ä¸ªå¼‚æ„åŸºå‡†ç¯å¢ƒï¼ˆå«è½¯ä»¶å·¥ç¨‹ã€å®¢æœã€æ·±ç ”ã€Appæ“ä½œç­‰ï¼‰ï¼Œæ€»è€—èµ„2.2ä¸‡ç¾å…ƒã€‚  \nğŸ”¸é‡‡ç”¨ç»„ä»¶çº§åˆ†æè§†è§’ï¼Œç³»ç»Ÿæ‹†è§£æ‰§è¡Œè¿è¡Œæ—¶ã€å·¥å…·ç­›é€‰ã€æ¨¡å¼æ ¡éªŒã€é€šä¿¡åè®®ã€è®°å¿†ä¸è§„åˆ’ç­‰æ¨¡å—ï¼Œé‡åŒ–å„ç»„ä»¶å¯¹æ€§èƒ½çš„å½±å“ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸é€šç”¨æ™ºèƒ½ä½“å…·å¤‡æ˜¾è‘—è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼šå¤šæ•°é…ç½®åœ¨å¤šä¸ªåŸºå‡†ä¸Šè¡¨ç°æ¥è¿‘ç”šè‡³è¶…è¶Šé¢†åŸŸä¸“ç”¨ç³»ç»Ÿï¼ŒéªŒè¯äº†â€œé€šç”¨æ€§â€å¯è¡Œæ€§ã€‚  \nğŸ”¸è¯­è¨€æ¨¡å‹è´¨é‡æ˜¯æ€§èƒ½ä¸»å¯¼å› ç´ ï¼ˆè§£é‡Š28.2%æ–¹å·®ï¼‰ï¼Œè¿œè¶…æ™ºèƒ½ä½“æ¶æ„å½±å“ï¼ˆä»…0.6%ï¼‰ï¼Œä¸”æ¨¡å‹ç¨³å®šæ€§å·®å¼‚æ˜¾è‘—ï¼ˆClaude Opusæœ€ç¨³å®šï¼ŒGPT 5.2æœ€æ•æ„Ÿï¼‰ã€‚  \nğŸ”¸æ— å•ä¸€æ™ºèƒ½ä½“æ¶æ„å…¨é¢é¢†å…ˆï¼šOpenAI Soloåœ¨API/ç¼–ç ä»»åŠ¡å ä¼˜ï¼ŒSmolagentåœ¨å¤šåº”ç”¨ç¯å¢ƒæ›´ä½³ï¼Œä½“ç°æ¶æ„-ä»»åŠ¡åŒ¹é…çš„é‡è¦æ€§ã€‚  \nğŸ”¸å…³é”®ç»„ä»¶ä»·å€¼æ˜ç¡®ï¼šæ¨¡å¼æ ¡éªŒï¼ˆschema guardï¼‰æ™®éå­˜åœ¨äºTop3æ¶æ„ä¸­ï¼›å·¥å…·çŸ­åˆ—è¡¨ï¼ˆtool shortlistingï¼‰ä½¿GPT 5.2åœ¨é«˜å·¥å…·æ•°ç¯å¢ƒä»ä¸å¯ç”¨å˜ä¸ºå¯ç”¨ï¼Œæå‡5ä¸ªç™¾åˆ†ç‚¹ã€‚  \nğŸ”¸å¤±è´¥ä»£ä»·è¢«ä¸¥é‡ä½ä¼°ï¼šå¤±è´¥ä»»åŠ¡å¹³å‡æ­¥æ•°æ¯”æˆåŠŸä»»åŠ¡é«˜20%â€“54%ï¼Œæ„å‘³ç€å¯é æ€§ä¸‹é™ä¼šçº¿æ€§æ”¾å¤§è®¡ç®—æˆæœ¬ä¸å»¶è¿Ÿï¼Œå‡¸æ˜¾é²æ£’æ€§è¯„ä¼°çš„å¿…è¦æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†â€œé€šç”¨æ™ºèƒ½ä½“è¯„ä¼°â€æœ¬èº«ç¡®ç«‹ä¸ºä¸€çº§ç ”ç©¶é—®é¢˜ï¼Œå¹¶ä»¥å·¥ç¨‹ä¸¥è°¨æ€§æ„å»ºå¯å¤ç°ã€å¯æ‰©å±•ã€å»ä¸­å¿ƒåŒ–çš„è¯„ä¼°åŸºç¡€è®¾æ–½ã€‚å®ƒè·³å‡ºäº†ä¼ ç»Ÿâ€œä¸ºç‰¹å®šåŸºå‡†å®šåˆ¶æ™ºèƒ½ä½“â€çš„è·¯å¾„ä¾èµ–ï¼Œè½¬è€Œé€šè¿‡åè®®æŠ½è±¡ä¸é€‚é…å™¨è§£è€¦ï¼Œä½¿è¯„ä¼°çœŸæ­£æœåŠ¡äºé€šç”¨æ€§ç›®æ ‡ã€‚å…¶æœ€å¤§æ´è§æ˜¯æ­ç¤ºï¼šå½“å‰æ‰€è°“â€œé€šç”¨æ™ºèƒ½ä½“â€çš„ç«äº‰åŠ›ä¸»è¦æºäºåº•å±‚æ¨¡å‹èƒ½åŠ›è¿ç§»ï¼Œè€Œéæ¶æ„è®¾è®¡çªç ´ï¼›å› æ­¤ï¼Œæœªæ¥ç ”ç©¶åº”èšç„¦äºå¦‚ä½•è®©æ¶æ„æœ‰æ•ˆé‡Šæ”¾æ¨¡å‹æ½œåŠ›ï¼ˆå¦‚é€šè¿‡çŸ­åˆ—è¡¨ã€æ ¡éªŒã€è®°å¿†ç­‰è½»é‡ç»„ä»¶ï¼‰ï¼Œè€Œéè¿½æ±‚å¤æ‚åº¦å †ç Œã€‚è¿™ä¸€èŒƒå¼è½¬ç§»ï¼Œä¸ºé€šç”¨AIä»£ç†çš„ç§‘å­¦åŒ–å‘å±•å¥ å®šäº†æ–¹æ³•è®ºåŸºçŸ³ã€‚\n    "
    },
    {
        "title": "IBCircuit: Towards Holistic Circuit Discovery with Information Bottleneck",
        "authors": [
            "Tian Bian",
            "Yifan Niu",
            "Chaohao Yuan",
            "Chengzhi Piao",
            "Bingzhe Wu",
            "Long-Kai Huang",
            "Yu Rong",
            "Tingyang Xu",
            "Hong Cheng",
            "Jia Li"
        ],
        "categories": [
            "cs.LG"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22581v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22581v1",
        "summary": "Circuit discovery has recently attracted attention as a potential research direction to explain the non-trivial behaviors of language models. It aims to find the computational subgraphs, also known as circuits, within the model that are responsible for solving specific tasks. However, most existing studies overlook the holistic nature of these circuits and require designing specific corrupted activations for different tasks, which is inaccurate and inefficient. In this work, we propose an end-to-end approach based on the principle of Information Bottleneck, called IBCircuit, to identify informative circuits holistically. IBCircuit is an optimization framework for holistic circuit discovery and can be applied to any given task without tediously corrupted activation design. In both the Indirect Object Identification (IOI) and Greater-Than tasks, IBCircuit identifies more faithful and minimal circuits in terms of critical node components and edge components compared to recent related work.",
        "tag": "å¯è§£é‡Šæ€§",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22581v1ã€å¯è§£é‡Šæ€§-é¦™æ¸¯ä¸­æ–‡å¤§å­¦ã€‘IBCircuit_ Towards Holistic Circuit Discovery with Information Bottleneck.pdf",
        "institution_status": "keep",
        "institution": "é¦™æ¸¯ä¸­æ–‡å¤§å­¦ã€é˜¿é‡Œå·´å·´",
        "first_institution": "é¦™æ¸¯ä¸­æ–‡å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šIBCircuit: Towards Holistic Circuit Discovery with Information Bottleneck\nğŸŒæ¥æºï¼šarXiv, 2602.22581v1\n\nç¬”è®°æ ‡é¢˜ï¼šåŸºäºä¿¡æ¯ç“¶é¢ˆçš„æ•´ä½“ç”µè·¯å‘ç°\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸ä¾èµ–ä»»åŠ¡ç‰¹å®šå¹²æ‰°æ¿€æ´»è®¾è®¡çš„å‰æä¸‹ï¼Œæ•´ä½“æ€§åœ°è¯†åˆ«è¯­è¨€æ¨¡å‹ä¸­å¯¹ç‰¹å®šä»»åŠ¡æœ€ç›¸å…³ä¸”æœ€ç²¾ç®€çš„è®¡ç®—å­å›¾ï¼ˆå³ç”µè·¯ï¼‰ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºIBCircuitæ¡†æ¶ï¼Œé¦–æ¬¡å°†ä¿¡æ¯ç“¶é¢ˆåŸç†ç³»ç»Ÿåº”ç”¨äºç”µè·¯å‘ç°ï¼Œå®ç°ç«¯åˆ°ç«¯ã€ä»»åŠ¡æ— å…³ã€æ•´ä½“ä¼˜åŒ–çš„ç”µè·¯è¯†åˆ«ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å°†ç”µè·¯å‘ç°å»ºæ¨¡ä¸ºä¿¡æ¯ç“¶é¢ˆä¼˜åŒ–é—®é¢˜ï¼šæœ€å¤§åŒ–ç”µè·¯Cå¯¹ä»»åŠ¡è¾“å‡ºYçš„äº’ä¿¡æ¯I(Y;C)ï¼ŒåŒæ—¶æœ€å°åŒ–Cä»å…¨æ¨¡å‹Gä¸­è·å–çš„å†—ä½™ä¿¡æ¯I(G;C)ã€‚  \nğŸ”¸å¼•å…¥å¯å­¦ä¹ çš„ä¿¡æ¯ç“¶é¢ˆæƒé‡Î»ï¼Œé€šè¿‡Sigmoidå‚æ•°åŒ–ï¼Œåœ¨èŠ‚ç‚¹ï¼ˆæ³¨æ„åŠ›å¤´ï¼‰å’Œè¾¹ï¼ˆæ®‹å·®è¿æ¥ï¼‰ä¸¤ä¸ªç²’åº¦ä¸Šæ§åˆ¶é«˜æ–¯å™ªå£°æ³¨å…¥å¼ºåº¦ï¼Œå®ç°è¿ç»­å¯å¾®çš„ç”µè·¯å‚æ•°åŒ–ã€‚  \nğŸ”¸é‡‡ç”¨å˜åˆ†è¿‘ä¼¼ä¼°è®¡äº’ä¿¡æ¯ï¼šç”¨KLæ•£åº¦ä¸‹ç•Œè¿‘ä¼¼I(Y;C)ï¼Œç”¨KLæ•£åº¦ä¸Šç•Œè¿‘ä¼¼I(G;C)ï¼Œæ„å»ºå¯è®­ç»ƒçš„ç›®æ ‡å‡½æ•°ã€‚  \nğŸ”¸é€šè¿‡é˜ˆå€¼åŒ–å­¦ä¹ åˆ°çš„Î»æƒé‡ï¼ˆè‡ªé€‚åº”è®¾å®šç¨€ç–çº¦æŸkï¼‰ï¼Œç¦»æ•£æå–å…³é”®èŠ‚ç‚¹ä¸è¾¹ï¼Œå½¢æˆæœ€ç»ˆç”µè·¯ï¼Œé¿å…æ‰‹å·¥è®¾è®¡å¹²æ‰°æ¿€æ´»ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨IOIå’ŒGreater-Thanä»»åŠ¡ä¸Šï¼ŒIBCircuitè¯†åˆ«å‡ºçš„ç”µè·¯åœ¨ç›¸åŒèŠ‚ç‚¹/è¾¹æ•°é‡ä¸‹ï¼ŒLogit Differenceå’ŒGreater Probabilityæ›´é«˜ï¼ŒKLæ•£åº¦æ›´ä½ï¼Œè¡¨æ˜å…¶æ›´å¿ å®ä¸”æ›´ç²¾ç®€ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šKLæŸå¤±ä¿éšœåŠŸèƒ½ä¸€è‡´æ€§ï¼ŒMIæŸå¤±æŠ‘åˆ¶å†—ä½™ä¿¡æ¯ï¼›äºŒè€…ç¼ºä¸€ä¸å¯ï¼Œè”åˆä¼˜åŒ–æ˜¾è‘—ä¼˜äºå•ä¸€æŸå¤±è®­ç»ƒã€‚  \nğŸ”¸ç›¸æ¯”ACDCã€APç­‰åŸºçº¿ï¼ŒIBCircuitåœ¨è¾¹çº§ç”µè·¯å‘ç°ä¸Šå…¨é¢å ä¼˜ï¼Œå°¤å…¶åœ¨é«˜åº¦ç¨€ç–æ¡ä»¶ä¸‹ä»ä¿æŒé«˜faithfulnessï¼ŒéªŒè¯å…¶é²æ£’æ€§ã€‚  \nğŸ”¸IBCircuitå¯æ‰©å±•è‡³GPT-2 XLï¼ˆ1.5Bï¼‰ï¼Œæ€§èƒ½åª²ç¾ç°æœ‰æ–¹æ³•ï¼Œè¯æ˜å…¶å¯¹å¤§è§„æ¨¡æ¨¡å‹çš„æœ‰æ•ˆæ€§ä¸å¯æ‰©å±•æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†ä¿¡æ¯ç“¶é¢ˆè¿™ä¸€ç»å…¸è¡¨å¾å­¦ä¹ åŸåˆ™è¿ç§»åˆ°æœºåˆ¶å¯è§£é‡Šæ€§é¢†åŸŸï¼Œçªç ´äº†ä¼ ç»Ÿå¹²é¢„å¼æ–¹æ³•ï¼ˆå¦‚patchingï¼‰çš„å±€éƒ¨æ€§ã€ä»»åŠ¡ä¾èµ–æ€§å’Œè®¡ç®—ä½æ•ˆæ€§å±€é™ï¼›å…¶ç«¯åˆ°ç«¯å¯å¾®ä¼˜åŒ–èŒƒå¼ä¸ºç”µè·¯å‘ç°æä¾›äº†ç»Ÿä¸€ã€é€šç”¨ã€å¯æ‰©å±•çš„æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "IMMACULATE: A Practical LLM Auditing Framework via Verifiable Computation",
        "authors": [
            "Yanpei Guo",
            "Wenjie Qu",
            "Linyu Wu",
            "Shengfang Zhai",
            "Lionel Z. Wang",
            "Ming Xu",
            "Yue Liu",
            "Binhang Yuan",
            "Dawn Song",
            "Jiaheng Zhang"
        ],
        "categories": [
            "cs.CR",
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22700v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22700v1",
        "summary": "Commercial large language models are typically deployed as black-box API services, requiring users to trust providers to execute inference correctly and report token usage honestly. We present IMMACULATE, a practical auditing framework that detects economically motivated deviations-such as model substitution, quantization abuse, and token overbilling-without trusted hardware or access to model internals. IMMACULATE selectively audits a small fraction of requests using verifiable computation, achieving strong detection guarantees while amortizing cryptographic overhead. Experiments on dense and MoE models show that IMMACULATE reliably distinguishes benign and malicious executions with under 1% throughput overhead. Our code is published at https://github.com/guo-yanpei/Immaculate.",
        "tag": "å¤§æ¨¡å‹å®¡è®¡",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22700v1ã€å¤§æ¨¡å‹å®¡è®¡-æ–°åŠ å¡å›½ç«‹å¤§å­¦ã€‘IMMACULATE_ A Practical LLM Auditing Framework via Verifiable Computation.pdf",
        "institution_status": "keep",
        "institution": "æ–°åŠ å¡å›½ç«‹å¤§å­¦ã€å—æ´‹ç†å·¥å¤§å­¦",
        "first_institution": "æ–°åŠ å¡å›½ç«‹å¤§å­¦",
        "institution_category": "å›½å¤–å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šIMMACULATE: A Practical LLM Auditing Framework via Verifiable Computation\nğŸŒæ¥æºï¼šarXiv, 2602.22700v1\n\nç¬”è®°æ ‡é¢˜ï¼šå®ç”¨é»‘ç®±å¤§æ¨¡å‹å®¡è®¡æ¡†æ¶\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸ä¾èµ–å¯ä¿¡ç¡¬ä»¶ã€ä¸è®¿é—®æ¨¡å‹å†…éƒ¨å‚æ•°çš„å‰æä¸‹ï¼Œé«˜æ•ˆæ£€æµ‹å•†ä¸šå¤§è¯­è¨€æ¨¡å‹APIæœåŠ¡ä¸­çš„ç»æµæ€§ä½œå¼Šè¡Œä¸ºï¼ˆå¦‚æ¨¡å‹æ›¿æ¢ã€æ¿€è¿›é‡åŒ–ã€tokenå¤šè®¡è´¹ï¼‰ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºIMMACULATEæ¡†æ¶ï¼Œé¦–æ¬¡å°†æ¦‚ç‡åŒ–æŠ½æ ·å®¡è®¡ä¸å¯éªŒè¯è®¡ç®—ç»“åˆï¼Œå¹¶è®¾è®¡Logitè·ç¦»åˆ†å¸ƒï¼ˆLDDï¼‰ä½œä¸ºæŠ—æµ®ç‚¹éç¡®å®šæ€§çš„å¯éªŒè¯åº¦é‡ï¼Œå®ç°ä½å¼€é”€ã€å¼ºä¿è¯ã€å…¨å…¼å®¹çš„é»‘ç®±LLMå®¡è®¡ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸é‡‡ç”¨éšæœºåŒ–æŠ½æ ·å®¡è®¡ç­–ç•¥ï¼Œä»…å¯¹æå°æ¯”ä¾‹ï¼ˆå¦‚åƒåˆ†ä¹‹ä¸€ï¼‰çš„ç”¨æˆ·è¯·æ±‚ç”Ÿæˆå¯†ç å­¦è¯æ˜ï¼Œä½¿è¯æ˜å¼€é”€è¢«æµ·é‡æ—¥å¸¸è¯·æ±‚æ‘Šè–„è‡³å¯å¿½ç•¥æ°´å¹³ã€‚  \nğŸ”¸æå‡ºLogitè·ç¦»åˆ†å¸ƒï¼ˆLDDï¼‰æŒ‡æ ‡ï¼Œé€šè¿‡æ¯”å¯¹éƒ¨ç½²æ¨¡å‹ä¸å…¨ç²¾åº¦å‚è€ƒæ¨¡å‹åœ¨å„ç¦»æ•£å†³ç­–æ­¥äº§ç”Ÿçš„logitsè·ç¦»åˆ†å¸ƒï¼Œå°†ä¸å¯éªŒè¯çš„å®Œæ•´æ‰§è¡Œè½¬åŒ–ä¸ºå¯éªŒè¯çš„ç»Ÿè®¡æŒ‡çº¹ã€‚  \nğŸ”¸å¼•å…¥æ§åˆ¶æµå¯¹é½æœºåˆ¶ï¼Œåœ¨å›ºå®šç¦»æ•£é€‰æ‹©ï¼ˆå¦‚tokené‡‡æ ·ç»“æœï¼‰å‰æä¸‹è®¡ç®—logitsåå·®ï¼Œè§„é¿å› å¾®å°æ•°å€¼è¯¯å·®å¯¼è‡´è·¯å¾„åˆ†å‰å¸¦æ¥çš„ä¸å¯æ¯”æ€§ã€‚  \nğŸ”¸è®¾è®¡Top-Kè·ç¦»ä¼˜åŒ–æ–¹æ¡ˆï¼ŒæœåŠ¡å™¨ä»…éœ€æ‰¿è¯ºè¢«é€‰ä¸­çš„å‰Kä¸ªlogitç´¢å¼•è€Œéå…¨éƒ¨logitså‘é‡ï¼Œå¤§å¹…é™ä½å­˜å‚¨ä¸é€šä¿¡å¼€é”€ã€‚  \nğŸ”¸æ„å»ºç«¯åˆ°ç«¯åè®®ï¼šæ¨¡å‹æ–¹å‘å¸ƒå…¨ç²¾åº¦æ¨¡å‹å“ˆå¸Œæ‰¿è¯ºï¼›æ¨ç†æ—¶è®°å½•å¹¶æ‰¿è¯ºæ¯æ­¥logitsï¼›å®¡è®¡æ—¶ç”±æœåŠ¡å™¨åœ¨TEEä¸­ç”¨å…¨ç²¾åº¦æ¨¡å‹é‡ç®—logitså¹¶ç”ŸæˆLDDè¯æ˜ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸å®éªŒè¡¨æ˜ï¼Œè‰¯æ€§BF16æ‰§è¡Œçš„LDDå‘ˆç°å°–é”é›†ä¸­åˆ†å¸ƒä¸”å°¾éƒ¨è¡°å‡æå¿«ï¼Œè€ŒFP8é‡åŒ–ä½¿å°¾éƒ¨æ¦‚ç‡æå‡2â€“3ä¸ªæ•°é‡çº§ï¼Œæ¨¡å‹æ›¿æ¢æ›´å¯¼è‡´æç«¯å°¾éƒ¨è´¨é‡å¢åŠ è¾¾100å€ä»¥ä¸Šã€‚  \nğŸ”¸åŸºäºLDDå°¾éƒ¨æ¦‚ç‡ï¼ˆå¦‚Pr[TV>0.1]ï¼‰çš„å•è¯·æ±‚åˆ¤åˆ«è§„åˆ™ï¼Œåœ¨æ¨¡å‹æ›¿æ¢æ”»å‡»ä¸‹æ£€æµ‹ç‡è¶…95%ï¼Œåœ¨FP8é‡åŒ–ä¸‹ä»è¾¾1.3%â€“10.3%ï¼Œæ»¡è¶³éšæœºå®¡è®¡æ‰€éœ€çš„æœ€ä½æ£€æµ‹ç‡è¦æ±‚ã€‚  \nğŸ”¸ç†è®ºåˆ†æè¯å®ï¼šå¯¹Î±=10%æ¶æ„æœåŠ¡å™¨ï¼Œä»…éœ€çº¦3000æ¬¡å®¡è®¡å³å¯ä»¥>95%æ¦‚ç‡æ£€å‡ºï¼›åœ¨10â»âµçº§è¯¯æŠ¥ç‡ä¸‹ï¼Œæ‹’ç»è¯šå®æœåŠ¡å™¨çš„æ¦‚ç‡ä½äº10â»â·ï¼Œæ»¡è¶³å¼ºå®Œå¤‡æ€§ä¸å¯é æ€§ã€‚  \nğŸ”¸åœ¨vLLMä¸Šå®ç°çš„åŸå‹ç³»ç»Ÿå¯¹LLaMA3-70Bç­‰ä¸»æµæ¨¡å‹å¼•å…¥å¹³å‡ä»…0.3%â€“1.0%ååæŸè€—ï¼ŒéªŒè¯äº†å…¶å·¥ç¨‹å®ç”¨æ€§ã€‚  \nğŸ”¸LDDå¯¹å¤šç§è·ç¦»åº¦é‡ï¼ˆTVã€KLã€Top-Kï¼‰å’Œæ¨¡å‹æ¶æ„ï¼ˆDense/MoEï¼‰å‡ä¿æŒç¨³å®šåŒºåˆ†èƒ½åŠ›ï¼Œè¯æ˜å…¶æ³›åŒ–æ€§ä¸é²æ£’æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°è·³å‡ºâ€œé€æ¯”ç‰¹å¤ç°â€çš„ä¼ ç»ŸéªŒè¯èŒƒå¼ï¼Œè½¬è€Œæ„å»ºé¢å‘LLMç‰¹æ€§çš„è¯­ä¹‰çº§å¯éªŒè¯æŒ‡çº¹â€”â€”LDDï¼Œå·§å¦™åŒ–è§£æµ®ç‚¹éç¡®å®šæ€§è¿™ä¸€æ ¹æœ¬éšœç¢ï¼›å…¶â€œç»æµé©±åŠ¨çš„æŠ½æ£€+ç»Ÿè®¡å¯è¯â€çš„è®¾è®¡ï¼Œé¦–æ¬¡åœ¨å®Œå¤‡æ€§ã€æ•ˆç‡ä¸éšç§é—´å–å¾—å®è´¨æ€§å¹³è¡¡ï¼Œä¸ºé»‘ç®±AIæœåŠ¡æ²»ç†æä¾›äº†å¯è½åœ°çš„æŠ€æœ¯è·¯å¾„ã€‚\n    "
    },
    {
        "title": "Imagination Helps Visual Reasoning, But Not Yet in Latent Space",
        "authors": [
            "You Li",
            "Chi Chen",
            "Yanghao Li",
            "Fanhu Zeng",
            "Kaiyu Huang",
            "Jinan Xu",
            "Maosong Sun"
        ],
        "categories": [
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22766v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22766v1",
        "summary": "Latent visual reasoning aims to mimic human's imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect: dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect: perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine, which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.",
        "tag": "è§†è§‰æ¨ç†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22766v1ã€è§†è§‰æ¨ç†-åŒ—äº¬äº¤é€šå¤§å­¦ã€‘Imagination Helps Visual Reasoning, But Not Yet in Latent Space.pdf",
        "institution_status": "keep",
        "institution": "åŒ—äº¬äº¤é€šå¤§å­¦ã€æ¸…åå¤§å­¦",
        "first_institution": "åŒ—äº¬äº¤é€šå¤§å­¦",
        "institution_category": "å…¶ä»–æœºæ„",
        "note": "ğŸ“–æ ‡é¢˜ï¼šImagination Helps Visual Reasoning, But Not Yet in Latent Space\nğŸŒæ¥æºï¼šarXiv, 2602.22766v1\n\nç¬”è®°æ ‡é¢˜ï¼šè´¨ç–‘éšç©ºé—´æƒ³è±¡æœ‰æ•ˆæ€§\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šéšç©ºé—´è§†è§‰æ¨ç†ä¸­çš„æ½œåœ¨æ ‡è®°ï¼ˆlatent tokensï¼‰æ˜¯å¦çœŸæ­£å‚ä¸å¹¶é©±åŠ¨äº†å› æœæ¨ç†è¿‡ç¨‹ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šé€šè¿‡å› æœä¸­ä»‹åˆ†ææ­ç¤ºéšç©ºé—´æ¨ç†ä¸­è¾“å…¥â†’éšæ ‡è®°â†’ç­”æ¡ˆçš„ä¸¤æ¡å› æœé“¾å‡æ–­è£‚ï¼Œè¿›è€Œæå‡ºæ›´æœ‰æ•ˆã€å¯è§£é‡Šçš„æ–‡æœ¬ç©ºé—´æƒ³è±¡æ–¹æ³•CapImagineã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸é‡‡ç”¨å› æœä¸­ä»‹åˆ†ææ¡†æ¶ï¼Œå°†è§†è§‰æ¨ç†å»ºæ¨¡ä¸ºXâ†’Zâ†’Yå› æœé“¾ï¼Œç³»ç»Ÿæ‰°åŠ¨è¾“å…¥Xå’Œéšæ ‡è®°Zä»¥æ£€éªŒå› æœæ•ˆåº”ã€‚  \nğŸ”¸è®¾è®¡å®ä¾‹çº§è¾“å…¥æ‰°åŠ¨å®éªŒï¼Œæµ‹é‡ä¸åŒæ ·æœ¬é—´éšæ ‡è®°çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå‘ç°å…¶é«˜åº¦åŒè´¨åŒ–ï¼Œè¡¨æ˜Xâ†’Zå› æœå¼±ã€‚  \nğŸ”¸å¯¹éšæ ‡è®°Zæ–½åŠ å¼ºå¹²é¢„ï¼ˆå¦‚å…¨ç½®é›¶ã€é«˜æ–¯å™ªå£°ã€ç»Ÿä¸€å¼ é‡ï¼‰ï¼Œè§‚å¯Ÿç­”æ¡ˆYå˜åŒ–ï¼Œå‘ç°æ€§èƒ½å‡ ä¹ä¸å˜ï¼Œè¡¨æ˜Zâ†’Yå› æœå¼±ã€‚  \nğŸ”¸å¼€å±•æ¢é’ˆåˆ†æï¼Œç”¨éšæ ‡è®°å•ç‹¬é¢„æµ‹å¤šé€‰VQAé—®é¢˜ï¼Œç»“æœæ˜¾è‘—ä½äºæ–‡æœ¬çŒœæµ‹åŸºçº¿ï¼Œè¯å®å…¶ç¼–ç è§†è§‰è¯­ä¹‰èƒ½åŠ›æå¼±ã€‚  \nğŸ”¸æå‡ºCapImagineæ–¹æ³•ï¼Œå°†ä¸­é—´å›¾åƒæ“ä½œè½¬åŒ–ä¸ºæ˜¾å¼æ–‡æœ¬æè¿°ï¼ˆå¦‚â€œçº¢è‰²çŸ©å½¢é«˜äº®æ™ºåˆ©åŒºåŸŸâ€ï¼‰ï¼Œä½¿æ¨¡å‹åœ¨æ–‡æœ¬ç©ºé—´å®Œæˆæƒ³è±¡æ¨ç†ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸éšæ ‡è®°åœ¨è·¨æ ·æœ¬å’Œè·¨ä»»åŠ¡ä¸‹é«˜åº¦ç›¸ä¼¼ï¼Œä¸”éšæ¨ç†æ­¥æ•°å¢åŠ æŒç»­åç¼©ï¼Œä¸§å¤±è¾“å…¥ç‰¹å¼‚æ€§ã€‚  \nğŸ”¸æ‰°åŠ¨éšæ ‡è®°å‡ ä¹ä¸æ”¹å˜æœ€ç»ˆç­”æ¡ˆï¼Œåœ¨å¤šä¸ªåŸºå‡†ä¸Šæ€§èƒ½æ³¢åŠ¨å°äº1%ï¼Œè¯´æ˜å…¶å¯¹è¾“å‡ºæ— å®è´¨å› æœå½±å“ã€‚  \nğŸ”¸éšæ ‡è®°æ— æ³•ç‹¬ç«‹æ”¯æŒä¸‹æ¸¸è§†è§‰é—®ç­”ï¼Œç”šè‡³ä¸å¦‚çº¯æ–‡æœ¬çŒœæµ‹ï¼Œè¯æ˜å…¶æœªæ‰¿è½½æœ‰æ•ˆè§†è§‰è¯­ä¹‰ã€‚  \nğŸ”¸æ–‡æœ¬ç©ºé—´æƒ³è±¡å˜é‡Zå±•ç°å‡ºå¼ºXâ†’Zä¾èµ–æ€§ä¸Zâ†’Yæ•æ„Ÿæ€§ï¼Œå¹²é¢„åæ€§èƒ½éª¤é™è‡³éšæœºæ°´å¹³ï¼ŒéªŒè¯å…¶çœŸå®å› æœä½œç”¨ã€‚  \nğŸ”¸CapImagineåœ¨HR-Benchã€MME-RealWorld-Liteç­‰åŸºå‡†ä¸Šå…¨é¢è¶…è¶ŠMonetç­‰éšç©ºé—´æ–¹æ³•ï¼Œå¹³å‡æå‡è¶…3%ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡åˆ›æ–°æ€§åœ°å¼•å…¥å› æœä¸­ä»‹åˆ†æè¿™ä¸€ä¸¥è°¨å·¥å…·ï¼Œé¦–æ¬¡ä»å› æœæ€§è§’åº¦ç³»ç»Ÿè§£æ„éšç©ºé—´æ¨ç†çš„â€œé»‘ç®±â€ï¼Œè€Œéä»…ä¾èµ–æ€§èƒ½æ¯”è¾ƒï¼›å…¶æ ¸å¿ƒæ´è§â€”â€”éšæ ‡è®°å®ä¸ºè½¯æç¤ºå¼å ä½ç¬¦è€Œéæƒ³è±¡è½½ä½“â€”â€”ç›´å‡»å½“å‰èŒƒå¼æœ¬è´¨ç¼ºé™·ï¼›æå‡ºçš„CapImagineè™½éç»ˆææ–¹æ¡ˆï¼Œä½†ä»¥æç®€è®¾è®¡å®ç°æ›´å¼ºå› æœæ€§ä¸æ›´é«˜æ€§èƒ½ï¼Œä¸ºæ„å»ºå¯è§£é‡Šã€å¯ä¿¡èµ–çš„è§†è§‰æ¨ç†æ¨¡å‹æä¾›äº†æ¸…æ™°æ–¹æ³•è®ºè½¬å‘ã€‚\n    "
    },
    {
        "title": "Interpreting and Steering State-Space Models via Activation Subspace Bottlenecks",
        "authors": [
            "Vamshi Sunku Mohan",
            "Kaustubh Gupta",
            "Aneesha Das",
            "Chandan Singh"
        ],
        "categories": [
            "cs.LG"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22719v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22719v1",
        "summary": "State-space models (SSMs) have emerged as an efficient strategy for building powerful language models, avoiding the quadratic complexity of computing attention in transformers. Despite their promise, the interpretability and steerability of modern SSMs remain relatively underexplored. We take a major step in this direction by identifying activation subspace bottlenecks in the Mamba family of SSM models using tools from mechanistic interpretability. We then introduce a test-time steering intervention that simply multiplies the activations of the identified bottlenecks by a scalar. Across 5 SSMs and 6 diverse benchmarks, this intervention improves performance by an average of 8.27%, without requiring any task-specific tuning. Finally, we validate that the identified bottlenecks are indeed hindering performance by modifying them to yield an architecture we call Stable-Mamba, which achieves long-context performance gains when retrained from scratch.",
        "tag": "SSM æ¶æ„ä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22719v1ã€SSM æ¶æ„ä¼˜åŒ–-å¾®è½¯ã€‘Interpreting and Steering State-Space Models via Activation Subspace Bottlenecks.pdf",
        "institution_status": "keep",
        "institution": "å¾®è½¯ã€Amrita University",
        "first_institution": "å¾®è½¯",
        "institution_category": "å›½å¤–å·¥ä¸šç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šInterpreting and Steering State-Space Models via Activation Subspace Bottlenecks\nğŸŒæ¥æºï¼šarXiv, 2602.22719v1\n\nç¬”è®°æ ‡é¢˜ï¼šå‘ç°å¹¶å¹²é¢„çŠ¶æ€ç©ºé—´ç“¶é¢ˆ\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•æå‡çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯Mambaï¼‰çš„å¯è§£é‡Šæ€§ä¸å¯æ§æ€§ï¼Œä½¿å…¶åœ¨ä¸ç‰ºç‰²æ•ˆç‡çš„å‰æä¸‹å…‹æœé•¿ç¨‹æ¨ç†ä¸ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ€§èƒ½ç“¶é¢ˆï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šé¦–æ¬¡æå‡ºâ€œæ¿€æ´»å­ç©ºé—´ç“¶é¢ˆâ€æ¦‚å¿µï¼Œç³»ç»Ÿè¯†åˆ«Mambaä¸­ç”±dt_proj.biasä¸»å¯¼çš„å±‚çº§ä¿¡æ¯å‹ç¼©ç“¶é¢ˆï¼Œå¹¶é€šè¿‡è½»é‡çº§æ¿€æ´»ç¼©æ”¾å¹²é¢„å’Œæ¶æ„æ”¹è¿›Stable-Mambaï¼Œæ˜¾è‘—æå‡å¤šä»»åŠ¡æ€§èƒ½ä¸”æ— éœ€ä»»åŠ¡å¾®è°ƒã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸åŸºäºæœºæ¢°å¯è§£é‡Šæ€§å·¥å…·ï¼ˆSPDå‚æ•°åˆ†è§£ã€ç¨€ç–è‡ªç¼–ç å™¨SAEã€éšå¼æ³¨æ„åŠ›æ˜ å°„ï¼‰ï¼Œå®šé‡å®šä½Mambaç¬¬20å±‚çš„æ¿€æ´»å­ç©ºé—´ç“¶é¢ˆï¼Œæ ¸å¿ƒè¯æ®ä¸ºè¯¥å±‚ç†µå³°å€¼ã€é«˜KLæ•£åº¦åŠdt_proj.biasçš„å†»ç»“æ¢¯åº¦ç‰¹æ€§ã€‚  \nğŸ”¸å®šä¹‰Deltaæ•æ„Ÿå­ç©ºé—´ï¼šåˆ©ç”¨SSMä¸­Î”å‚æ•°è°ƒæ§æ—¶åºæ›´æ–°çš„æœºåˆ¶ï¼Œç­›é€‰å¯¹è¾“å…¥å˜åŒ–å“åº”å‰§çƒˆçš„éšè—çŠ¶æ€å­ç©ºé—´ï¼Œä½œä¸ºå¯å¹²é¢„çš„å…³é”®è·¯å¾„ã€‚  \nğŸ”¸è®¾è®¡é›¶æ ·æœ¬åå¤„ç†å¹²é¢„ï¼šä»…åœ¨æ¨ç†æ—¶å¯¹ç¬¬20å±‚ä¸­668ä¸ªDeltaæ•æ„Ÿå­ç©ºé—´æŒ‰é‡è¦æ€§åˆ†ç»„ï¼ˆ435ä¸ªé™æŸ>2%è€…Ã—5å€æ”¾å¤§ï¼Œ155ä¸ªä¸­æ€§è€…Ã—2å€ï¼‰ï¼Œä¸ä¿®æ”¹æƒé‡ã€ä¸é‡è®­ç»ƒã€‚  \nğŸ”¸æ„å»ºStable-Mambaæ¶æ„ï¼šåœ¨ä¿ç•™SSMæ ¸å¿ƒå‰æä¸‹ï¼Œå¼•å…¥å¤šæ—¶é—´å°ºåº¦çŠ¶æ€æ›´æ–°ã€ç¨€ç–å…¨å±€ä¸Šä¸‹æ–‡æ³¨å…¥ã€é›†æˆé—¨æ§ç­‰7é¡¹è½»é‡ä¿®æ”¹ï¼Œæ–°å¢ä»…256å‚æ•°ï¼Œé’ˆå¯¹æ€§ç¼“è§£å•ä¸€æ—¶åºå°ºåº¦ä¸çº¿æ€§çŠ¶æ€æ¼”åŒ–é™åˆ¶ã€‚  \nğŸ”¸éªŒè¯ç“¶é¢ˆå› æœæ€§ï¼šé€šè¿‡ablationè¯å®ç¬¬20å±‚ç§»é™¤åè€Œæå‡æ€§èƒ½ï¼Œè¯æ˜å…¶æ˜¯ä¿¡æ¯æµé˜»å¡ç‚¹è€ŒéåŠŸèƒ½æ¨¡å—ï¼›Stable-Mambaé‡è®­åç“¶é¢ˆç†µä¸‹é™22%ï¼ŒéªŒè¯å¹²é¢„æœ‰æ•ˆæ€§ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸SPDåˆ†ææ˜¾ç¤ºMambaç¬¬20å±‚å­˜åœ¨å…¸å‹â€œæ‰©å¼ -å‹ç¼©â€ç›¸å˜ï¼šç†µè¾¾å³°å€¼1.19ã€æœ‰æ•ˆç§©æœ€é«˜7.59ã€KLæ•£åº¦é«˜è¾¾813ï¼Œè¡¨æ˜ä¿¡æ¯åœ¨æ­¤è¢«å¼ºåˆ¶æ”¶æ•›è‡³çª„å‚æ•°å­é›†ã€‚  \nğŸ”¸Deltaæ•æ„Ÿå­ç©ºé—´å…·æœ‰å¼ºå› æœå¿…è¦æ€§ï¼šå¯¹å…¶ablationå¯¼è‡´å›°æƒ‘åº¦æ¿€å¢394.1%ï¼Œè¿œè¶…Transformerï¼ˆ<10%ï¼‰ï¼Œè¯å®Mambaäº‹å®æ¨ç†é«˜åº¦ä¾èµ–è¯¥é€’å½’è·¯å¾„ã€‚  \nğŸ”¸åå¤„ç†å¹²é¢„æ³›åŒ–æ€§å¼ºï¼šåŒä¸€å¥—è¶…å‚ï¼ˆLayer 20 + Ã—5/Ã—2ç¼©æ”¾ï¼‰åœ¨5ç§SSMæ¶æ„ã€6ä¸ªåŸºå‡†ä¸Šå¹³å‡æå‡8.27%ï¼Œä¸”åœ¨IFEvalæŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­å•ç‚¹æå‡17.5%ã€‚  \nğŸ”¸Stable-Mambaå®ç°æ¶æ„çº§çªç ´ï¼šåœ¨RULER QAä»»åŠ¡+15.3ppã€PathFinder+35ppï¼Œè¶…è¶ŠåŸç‰ˆMambaåŠHyena/DenseMambaç­‰å˜ä½“ï¼ŒåŒæ—¶æ¨ç†å»¶è¿Ÿä»…å¢åŠ 15%ã€‚  \nğŸ”¸ç“¶é¢ˆæ ¹æºåœ¨äºdt_proj.biasçš„éçº¿æ€§é—¨æ§ï¼šå…¶å†»ç»“ç‰¹æ€§ï¼ˆCoV=0.001ï¼‰å½¢æˆç¡¬æ€§æ—¶åºé˜ˆå€¼ï¼Œæ‰°åŠ¨å®éªŒæ˜¾ç¤ºå…¶å“åº”å‘ˆsigmoidå‹ï¼Œè¯å®å…¶ä¸ºå­¦ä¹ æ‰€å¾—çš„åŠ¨æ€ä¿¡æ¯å¼€å…³ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°æ€§åœ¨äºå°†Transformeré¢†åŸŸæˆç†Ÿçš„â€œç¥ç»å…ƒ/å¤´çº§â€è§£é‡ŠèŒƒå¼ï¼Œè¿ç§»å‡ç»´ä¸ºSSMä¸“å±çš„â€œæ¿€æ´»å­ç©ºé—´-å‚æ•°è€¦åˆâ€åˆ†ææ¡†æ¶ï¼›å…¶æœ€å¤§æ´è§æ˜¯æ­ç¤ºSSMé«˜æ•ˆæ€§ä¸è„†å¼±æ€§çš„å…±ç”Ÿæœ¬è´¨â€”â€”çº¿æ€§çŠ¶æ€æ¼”åŒ–ä¸å›ºå®šÎ”ç¦»æ•£åŒ–è™½ä¿éšœO(N)å¤æ‚åº¦ï¼Œå´å¿…ç„¶å‚¬ç”Ÿå±‚é—´ä¿¡æ¯æµç“¶é¢ˆï¼›è€Œâ€œç“¶é¢ˆå³æ¥å£â€çš„æ€æƒ³ï¼ˆæ—¢å¯å¹²é¢„åˆå¯é‡æ„ï¼‰ä¸ºä¸‹ä¸€ä»£é«˜æ•ˆåºåˆ—æ¨¡å‹çš„è®¾è®¡æä¾›äº†å¯å¤ç”¨çš„æ–¹æ³•è®ºèŒƒå¼ã€‚\n    "
    },
    {
        "title": "MSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding",
        "authors": [
            "Wenhui Tan",
            "Xiaoyi Yu",
            "Jiaze Li",
            "Yijing Chen",
            "Jianzhong Ju",
            "Zhenbo Luo",
            "Ruihua Song",
            "Jian Luan"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22932v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22932v1",
        "summary": "Efficiently understanding long-form videos remains a fundamental challenge for multimodal large language models (MLLMs). In this paper, we present MLLM-Sampler Joint Evolution (MSJoE), a novel framework that jointly evolves the MLLM and a lightweight key-frame sampler for efficient long-form video understanding. MSJoE builds upon a key assumption that only a small subset of key-frames is truly informative for answering each question to a video. Specifically, MSJoE first reasons out several queries, which describe diverse visual perspectives relevant to the question. Then, these queries interact with a frozen CLIP model to produce a query-frame similarity matrix. Finally, a lightweight sampler predicts key-frame sampling weights from this matrix, selecting a compact set of informative frames, which are then fed into the MLLM for answer generation. Both the MLLM and sampler are jointly optimized through reinforcement learning, enabling co-adaptation of query-reasoning, frame-sampling, and key-frame understanding. A new long-video QA dataset containing 2.8K videos with 7K question-answer pairs is collected to support the training process. Extensive experiments on VideoMME, LongVideoBench, LVBench, and MLVU show that MSJoE achieves 8.0\\% accuracy gain upon the base MLLM, and 1.1\\% higher accuracy than strongest baseline method.",
        "tag": "è§†é¢‘ç†è§£",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22932v1ã€è§†é¢‘ç†è§£-ä¸­å›½äººæ°‘å¤§å­¦ã€‘MSJoE_ Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding.pdf",
        "institution_status": "keep",
        "institution": "ä¸­å›½äººæ°‘å¤§å­¦ã€åŒæµå¤§å­¦ã€MiLM Plus, Xiaomi Inc.",
        "first_institution": "ä¸­å›½äººæ°‘å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šMSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding\nğŸŒæ¥æºï¼šarXiv, 2602.22932v1\n\nç¬”è®°æ ‡é¢˜ï¼šè”åˆè¿›åŒ–MLLMä¸é‡‡æ ·å™¨\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨é•¿è§†é¢‘ç†è§£ä¸­å®ç°é«˜æ•ˆä¸”å‡†ç¡®çš„å…³é”®å¸§é€‰æ‹©ä¸å¤šæ¨¡æ€å¤§æ¨¡å‹ååŒä¼˜åŒ–ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºMSJoEæ¡†æ¶ï¼Œé¦–æ¬¡é€šè¿‡å¼ºåŒ–å­¦ä¹ è”åˆä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å’Œè½»é‡çº§å…³é”®å¸§é‡‡æ ·å™¨ï¼Œå®ç°æ¨ç†å¼•å¯¼çš„å¸§é€‰æ‹©ä¸æ„ŸçŸ¥-è¯­è¨€èƒ½åŠ›å…±é€‚åº”ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸åŸºäºâ€œä»…å°‘é‡å…³é”®å¸§è¶³ä»¥å›ç­”é—®é¢˜â€çš„æ ¸å¿ƒå‡è®¾ï¼Œå…ˆç”¨ç¨€ç–é¢„è§ˆå¸§å¼•å¯¼MLLMç”Ÿæˆå¤šä¸ªè§†è§‰ grounded çš„æ¨ç†æŸ¥è¯¢ï¼ˆå¦‚â€œä¸€å¼ ç‰™é½¿ç‰¹å†™å›¾â€ï¼‰ï¼Œè€Œéç›´æ¥ä½¿ç”¨åŸå§‹é—®é¢˜ã€‚  \nğŸ”¸å°†è¿™äº›æŸ¥è¯¢ä¸å¯†é›†é‡‡æ ·çš„è§†é¢‘å¸§è¾“å…¥å†»ç»“çš„CLIPæ¨¡å‹ï¼Œæ„å»ºæŸ¥è¯¢â€“å¸§ç›¸ä¼¼åº¦çŸ©é˜µï¼Œä¸ºé‡‡æ ·æä¾›è¯­ä¹‰ä¸°å¯Œã€å¤šè§†è§’çš„åŒ¹é…ä¾æ®ã€‚  \nğŸ”¸è®¾è®¡ä¸€ä¸ªä»…å«çº¦200ä¸‡å‚æ•°çš„1D U-Netè½»é‡é‡‡æ ·å™¨ï¼Œä»ç›¸ä¼¼åº¦çŸ©é˜µä¸­å­¦ä¹ ç”Ÿæˆå¸§çº§é‡‡æ ·æƒé‡ï¼Œå…¼é¡¾é«˜å¾—åˆ†åŒºåŸŸä¸æ—¶é—´å¤šæ ·æ€§ï¼Œé¿å…top-kå¯¼è‡´çš„å†—ä½™ã€‚  \nğŸ”¸é‡‡ç”¨GRPOå¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹MLLMå’Œé‡‡æ ·å™¨è¿›è¡Œç«¯åˆ°ç«¯è”åˆè®­ç»ƒï¼šMLLMå­¦ä¹ ç”Ÿæˆæ›´æœ‰æ•ˆæŸ¥è¯¢å¹¶é€‚é…ç¨€ç–å¸§è¾“å…¥ï¼Œé‡‡æ ·å™¨å­¦ä¹ é€‰æ‹©èƒ½æœ€å¤§åŒ–ä¸‹æ¸¸ç­”æ¡ˆå‡†ç¡®ç‡çš„å¸§å­é›†ã€‚  \nğŸ”¸ä¸ºæ”¯æ’‘è®­ç»ƒï¼Œæ„å»ºæ–°æ•°æ®é›†LongVideoQAï¼ˆ2.8kå°æ—¶çº§è§†é¢‘ï¼Œ7.1k QAå¯¹ï¼‰ï¼ŒåŒ…å«è‡ªåŠ¨æ ‡æ³¨ã€å¤šè·³æ¨ç†ã€éš¾åº¦åˆ†çº§ä¸ä¸¥æ ¼è¿‡æ»¤æœºåˆ¶ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šä»…ç”¨é—®é¢˜æœ¬èº«ä½œä¸ºCLIPæŸ¥è¯¢æ•ˆæœæœ‰é™ï¼ˆQ1ä¸å……åˆ†ï¼‰ï¼Œéœ€MLLMç”Ÿæˆå¤šè§†è§’æŸ¥è¯¢ï¼›æœ´ç´ top-ké‡‡æ ·æ€§èƒ½åä½äºå‡åŒ€é‡‡æ ·ï¼ŒéªŒè¯äº†å¯å­¦ä¹ é‡‡æ ·å™¨çš„å¿…è¦æ€§ï¼ˆQ2ï¼‰ã€‚  \nğŸ”¸å†»ç»“MLLMæ—¶ï¼Œå³ä½¿ä½¿ç”¨ä¼˜è´¨æŸ¥è¯¢å’Œé‡‡æ ·å™¨ï¼Œæ€§èƒ½ä»æ˜¾è‘—ä¸‹é™ï¼›è€Œè”åˆè¿›åŒ–åï¼ŒMLLMèƒ½æ›´å¥½åˆ©ç”¨ç¨€ç–å¸§å¹¶ç”Ÿæˆæ›´ç²¾å‡†æŸ¥è¯¢ï¼Œè¯å®åä½œå¿…é¡»ä¾èµ–è”åˆä¼˜åŒ–ï¼ˆQ3æˆç«‹ï¼‰ã€‚  \nğŸ”¸åœ¨VideoMMEç­‰å››å¤§åŸºå‡†ä¸Šï¼ŒMSJoEä»¥32å¸§è¾“å…¥å³è¶…è¶ŠåŸºçº¿MLLMè¾¾8.0%å‡†ç¡®ç‡ï¼Œä¸”æ¯”æœ€å¼ºåŸºçº¿TSPOé«˜1.1%ï¼ŒéªŒè¯å…¶ç²¾åº¦ä¸æ•ˆç‡åŒé‡ä¼˜åŠ¿ã€‚  \nğŸ”¸å¼•å…¥ä¿¡æ¯æ€§å¥–åŠ±ï¼ˆé¼“åŠ±å°–å³°å¼ç›¸ä¼¼åˆ†å¸ƒï¼‰å’Œéš¾åº¦æ„ŸçŸ¥å¥–åŠ±ï¼ˆå¯¹éš¾æ ·æœ¬èµ‹äºˆæ›´é«˜æ¢¯åº¦ï¼‰ï¼Œæ˜¾è‘—æå‡è®­ç»ƒç¨³å®šæ€§ä¸æœ€ç»ˆæ€§èƒ½ï¼Œç§»é™¤ä»»ä¸€å¥–åŠ±å‡å¯¼è‡´æ˜æ˜¾ä¸‹é™ã€‚  \nğŸ”¸æ¡ˆä¾‹åˆ†ææ˜¾ç¤ºï¼šMSJoEé€‰å–çš„å¸§åºåˆ—å‘ˆç°æ¸…æ™°å™äº‹é€»è¾‘ï¼ˆå¦‚é›¶é£Ÿâ†’ç‰™ç—…â†’çœ‹è¯Šï¼‰ï¼Œæ”¯æ’‘å› æœæ¨ç†ï¼›è€Œå‡åŒ€æˆ–top-ké‡‡æ ·æ˜“é™·å…¥å±€éƒ¨è§†è§‰çº¿ç´¢ï¼Œå¯¼è‡´é”™è¯¯å½’å› ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ¨äºæ‰“ç ´â€œé‡‡æ ·å™¨è¾…åŠ©MLLMâ€çš„å•å‘èŒƒå¼ï¼Œå°†äºŒè€…å»ºæ¨¡ä¸ºå¯ååŒè¿›åŒ–çš„ç­–ç•¥ç½‘ç»œï¼šMLLMä¸ä»…æ˜¯ç†è§£è€…ï¼Œæ›´æ˜¯ä¸»åŠ¨çš„è§†è§‰é—®é¢˜åˆ†è§£è€…ï¼›é‡‡æ ·å™¨ä¹Ÿä¸å†æ˜¯é»‘ç®±è¿‡æ»¤å™¨ï¼Œè€Œæ˜¯ç†è§£MLLMæ¨ç†æ„å›¾çš„è¯­ä¹‰ç¿»è¯‘å™¨ã€‚å…¶æ ¸å¿ƒæ´è§â€”â€”â€œå…³é”®å¸§é€‰æ‹©æœ¬è´¨æ˜¯è·¨æ¨¡æ€æ¨ç†çš„å…·èº«åŒ–è¡¨è¾¾â€â€”â€”ä¸ºé•¿è§†é¢‘ç†è§£æä¾›äº†æ–°æ–¹æ³•è®ºã€‚\n    "
    },
    {
        "title": "MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks",
        "authors": [
            "Shiqian Su",
            "Sen Xing",
            "Xuan Dong",
            "Muyan Zhong",
            "Bin Wang",
            "Xizhou Zhu",
            "Yuntao Chen",
            "Wenhai Wang",
            "Yue Deng",
            "Pengxiang Zhu",
            "Ziyuan Liu",
            "Tiantong Li",
            "Jiaheng Yu",
            "Zhe Chen",
            "Lidong Bing",
            "Jifeng Dai"
        ],
        "categories": [
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22808v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22808v1",
        "summary": "Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commercial APIs. In this work, we propose a high-performance and robust open-source agent framework, termed MiroFlow, which incorporates an agent graph for flexible orchestration, an optional deep reasoning mode to enhance performance, and a robust workflow execution to ensure stable and reproducible performance. Extensive experiments demonstrate that MiroFlow consistently achieves state-of-the-art performance across multiple agent benchmarks, including GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and notably FutureX. We hope it could serve as an easily accessible, reproducible, and comparable baseline for the deep research community.",
        "tag": "Agent æ¡†æ¶",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22808v1ã€Agent æ¡†æ¶-æ¸…åå¤§å­¦ã€‘MiroFlow_ Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks.pdf",
        "institution_status": "keep",
        "institution": "æ¸…åå¤§å­¦ã€MiroMind AIã€æ–°åŠ å¡å›½ç«‹å¤§å­¦ã€å—äº¬å¤§å­¦",
        "first_institution": "æ¸…åå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šMiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks\nğŸŒæ¥æºï¼šarXiv, 2602.22808v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºé«˜é²æ£’å¼€æºæ™ºèƒ½ä½“æ¡†æ¶\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è®¾è®¡ä¸€ä¸ªé«˜æ€§èƒ½ã€å¼ºé²æ£’ã€å…¨å¼€æºçš„é€šç”¨æ™ºèƒ½ä½“æ¡†æ¶ï¼Œä»¥æ”¯æ’‘å¤æ‚æ·±åº¦ç ”ç©¶ä»»åŠ¡ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºMiroFlowâ€”â€”é¦–ä¸ªèåˆä»£ç†å›¾ç¼–æ’ã€å¯é€‰æ·±åº¦æ¨ç†æ¨¡å¼ä¸é²æ£’å·¥ä½œæµæœºåˆ¶çš„é«˜æ€§èƒ½å¼€æºæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåœ¨å¤šä¸ªæƒå¨åŸºå‡†ä¸Šå®ç°å¯å¤ç°çš„SOTAæ€§èƒ½ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸é‡‡ç”¨ä¸‰å±‚åˆ†å±‚æ¶æ„ï¼ˆæ§åˆ¶å±‚/ä»£ç†å±‚/åŸºç¡€å±‚ï¼‰ï¼Œè§£è€¦è°ƒåº¦é€»è¾‘ã€è¡Œä¸ºé€»è¾‘ä¸åº•å±‚èƒ½åŠ›ï¼Œæå‡æ¨¡å—åŒ–ä¸å¯æ‰©å±•æ€§ã€‚  \nğŸ”¸å¼•å…¥æœ‰å‘ä»£ç†å›¾ï¼ˆAgent Graphï¼‰ä½œä¸ºæ ¸å¿ƒç¼–æ’èŒƒå¼ï¼Œæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰èŠ‚ç‚¹ä¾èµ–å…³ç³»ï¼Œå®ç°ä»»åŠ¡é©±åŠ¨çš„çµæ´»æ‹“æ‰‘æ„å»ºã€‚  \nğŸ”¸è®¾è®¡å¯é€‰çš„é‡æ¨ç†æ¨¡å¼ï¼ˆHeavy-Reasoning Modeï¼‰ï¼Œé€šè¿‡é›†æˆç­–ç•¥ï¼ˆå¤šæ¨¡å‹æŠ•ç¥¨ï¼‰ä¸éªŒè¯ç­–ç•¥ï¼ˆç”Ÿæˆ-æ ¡éªŒå¾ªç¯ï¼‰æå‡å…³é”®å­ä»»åŠ¡ç²¾åº¦ã€‚  \nğŸ”¸æ„å»ºé²æ£’å·¥ä½œæµæœºåˆ¶ï¼ŒåŒ…å«æ¶ˆæ¯æ ‡å‡†åŒ–ï¼ˆç»“æ„åŒ–è¾“å…¥/è¾“å‡ºï¼‰ã€å¸¦è¶…æ—¶ä¸å›é€€çš„é‡è¯•æœºåˆ¶ã€ä»¥åŠè·¨å±‚æ•…éšœéš”ç¦»æœºåˆ¶ï¼Œæ˜¾è‘—é™ä½éšæœºæ€§ä¸é”™è¯¯ä¼ æ’­é£é™©ã€‚  \nğŸ”¸å…¨é¢æ”¯æŒå¼€æºå·¥å…·é“¾ï¼ˆå¦‚Qwen2.5-VLã€Whisper-v3ç­‰ï¼‰ï¼Œåœ¨ä¸ä¾èµ–å•†ä¸šAPIå‰æä¸‹è¾¾æˆæ¥è¿‘é—­æºç³»ç»Ÿçš„æ€§èƒ½æ°´å¹³ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸MiroFlowåœ¨GAIAã€BrowseComp-EN/ZHã€HLEã€xBench-DeepSearchåŠFutureXäº”å¤§åŸºå‡†ä¸Šå‡è¾¾SOTAï¼Œä¸”æ‰€æœ‰ç»“æœåŸºäºç»Ÿä¸€é…ç½®ã€æ— éœ€ä»»åŠ¡è°ƒä¼˜ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šæ¶ˆæ¯æ ‡å‡†åŒ–ä½¿GAIA-Valæ ‡å‡†å·®ä»2.43%é™è‡³1.21%ï¼Œé‡è¯•æœºåˆ¶å°†å‡†ç¡®ç‡æå‡2.9ä¸ªç™¾åˆ†ç‚¹ï¼ŒäºŒè€…ååŒä¿éšœå¤ç°æ€§ã€‚  \nğŸ”¸é‡æ¨ç†æ¨¡å¼åœ¨GAIA-Valä¸Šå°†GPT-5æ€§èƒ½ä»71.9%æå‡è‡³75.0%ï¼ˆ4æ¨¡å‹+å¤šæ ·åŒ–æç¤ºï¼‰ï¼ŒéªŒè¯å…¶å¯¹å¤æ‚ä»»åŠ¡çš„æœ‰æ•ˆå¢ç›Šã€‚  \nğŸ”¸å¤šæ™ºèƒ½ä½“æ¶æ„åœ¨BrowseComp/HLEä¸Šä¼˜äºå•æ™ºèƒ½ä½“ï¼Œä½†åœ¨GAIAä¸Šåé€Šäºå•æ™ºèƒ½ä½“ï¼Œæ­ç¤ºä»»åŠ¡åºåˆ—æ€§è¶Šå¼ºï¼Œä¸Šä¸‹æ–‡è¿è´¯æ€§è¶Šå…³é”®ã€‚  \nğŸ”¸å¼€æºå·¥å…·é›†é…ç½®ä¸‹ï¼ŒMiroFlowåœ¨GAIA-Valä¸Šä»…æ¯”é»˜è®¤å•†ç”¨å·¥å…·ä½1.6ä¸ªç™¾åˆ†ç‚¹ï¼Œè¯å®å…¶å¯¹å·¥å…·ç”Ÿæ€çš„å¼ºé€‚åº”æ€§ä¸æˆæœ¬å¯æ§æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºç³»ç»Ÿæ€§ç ´è§£å½“å‰å¼€æºæ™ºèƒ½ä½“çš„ä¸‰å¤§ç“¶é¢ˆï¼šç”¨ä»£ç†å›¾æ›¿ä»£ç¡¬ç¼–ç æµæ°´çº¿è§£å†³çµæ´»æ€§ä¸è¶³ï¼›ä»¥ç»“æ„åŒ–I/O+æ•…éšœéš”ç¦»+é‡è¯•æœºåˆ¶æ”»å…‹ç¨³å®šæ€§é¡½ç–¾ï¼›é€šè¿‡è½»é‡çº§å¼€æºå·¥å…·é›†æˆä¸ç»Ÿä¸€æ¡†æ¶è®¾è®¡ï¼Œå®è´¨æ€§é™ä½ç ”ç©¶é—¨æ§›ä¸éƒ¨ç½²æˆæœ¬ã€‚å…¶â€œå¯é…ç½®ã€å¯å¤ç°ã€å¯æ¯”è¾ƒâ€çš„è®¾è®¡ç†å¿µï¼Œä¸ºç¤¾åŒºæä¾›äº†çœŸæ­£å¯ç”¨çš„æ·±åº¦ç ”ç©¶åŸºåº§ã€‚\n    "
    },
    {
        "title": "MoDora: Tree-Based Semi-Structured Document Analysis System",
        "authors": [
            "Bangrui Xu",
            "Qihang Yao",
            "Zirui Tang",
            "Xuanhe Zhou",
            "Yeye He",
            "Shihan Yu",
            "Qianqian Xu",
            "Bin Wang",
            "Guoliang Li",
            "Conghui He",
            "Fan Wu"
        ],
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CL",
            "cs.DB",
            "cs.LG"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.23061v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23061v1",
        "summary": "Semi-structured documents integrate diverse interleaved data elements (e.g., tables, charts, hierarchical paragraphs) arranged in various and often irregular layouts. These documents are widely observed across domains and account for a large portion of real-world data. However, existing methods struggle to support natural language question answering over these documents due to three main technical challenges: (1) The elements extracted by techniques like OCR are often fragmented and stripped of their original semantic context, making them inadequate for analysis. (2) Existing approaches lack effective representations to capture hierarchical structures within documents (e.g., associating tables with nested chapter titles) and to preserve layout-specific distinctions (e.g., differentiating sidebars from main content). (3) Answering questions often requires retrieving and aligning relevant information scattered across multiple regions or pages, such as linking a descriptive paragraph to table cells located elsewhere in the document.\n  To address these issues, we propose MoDora, an LLM-powered system for semi-structured document analysis. First, we adopt a local-alignment aggregation strategy to convert OCR-parsed elements into layout-aware components, and conduct type-specific information extraction for components with hierarchical titles or non-text elements. Second, we design the Component-Correlation Tree (CCTree) to hierarchically organize components, explicitly modeling inter-component relations and layout distinctions through a bottom-up cascade summarization process. Finally, we propose a question-type-aware retrieval strategy that supports (1) layout-based grid partitioning for location-based retrieval and (2) LLM-guided pruning for semantic-based retrieval. Experiments show MoDora outperforms baselines by 5.97%-61.07% in accuracy. The code is at https://github.com/weAIDB/MoDora.",
        "tag": "RAG æ£€ç´¢ä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.23061v1ã€RAG æ£€ç´¢ä¼˜åŒ–-ä¸Šæµ·äº¤é€šå¤§å­¦ã€‘MoDora_ Tree-Based Semi-Structured Document Analysis System.pdf",
        "institution_status": "keep",
        "institution": "ä¸Šæµ·äº¤é€šå¤§å­¦ã€å¾®è½¯",
        "first_institution": "ä¸Šæµ·äº¤é€šå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šMoDora: Tree-Based Semi-Structured Document Analysis System\nğŸŒæ¥æºï¼šarXiv, 2602.23061v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ ‘çŠ¶ç»“æ„åŒ–æ–‡æ¡£åˆ†ææ¡†æ¶\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•æœ‰æ•ˆæ”¯æŒè‡ªç„¶è¯­è¨€é—®ç­”åœ¨å¸ƒå±€å¤æ‚ã€å…ƒç´ æ··æ‚çš„åŠç»“æ„åŒ–æ–‡æ¡£ä¸Šï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºMoDoraç³»ç»Ÿï¼Œé€šè¿‡ç»„ä»¶èšåˆã€ç»„ä»¶å…³è”æ ‘ï¼ˆCCTreeï¼‰å»ºæ¨¡ä¸é—®é¢˜ç±»å‹æ„ŸçŸ¥æ£€ç´¢ï¼Œæ˜¾è‘—æå‡è·¨å…ƒç´ ã€è·¨é¡µã€è·¨æ¨¡æ€é—®ç­”å‡†ç¡®ç‡ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸é‡‡ç”¨å±€éƒ¨å¯¹é½èšåˆç­–ç•¥ï¼Œå°†OCRç¢ç‰‡åŒ–å…ƒç´ æŒ‰è¯­ä¹‰ä¸ç©ºé—´é‚»è¿‘æ€§èšåˆæˆè‡ªåŒ…å«ç»„ä»¶ï¼ˆå¦‚æ ‡é¢˜+æ®µè½ã€å›¾è¡¨+æ ‡é¢˜ï¼‰ã€‚  \nğŸ”¸è®¾è®¡ç»„ä»¶å…³è”æ ‘ï¼ˆCCTreeï¼‰ï¼Œä»¥å±‚æ¬¡åŒ–æ–¹å¼ç»„ç»‡ç»„ä»¶ï¼Œæ˜¾å¼å»ºæ¨¡æ–‡æœ¬-æ–‡æœ¬ï¼ˆæ ‡é¢˜å±‚çº§ï¼‰ã€æ–‡æœ¬-éæ–‡æœ¬ï¼ˆæ®µè½ä¸å¯¹åº”è¡¨æ ¼ï¼‰ã€è¡¥å……å…ƒç´ ï¼ˆé¡µçœ‰/é¡µè„šï¼‰ä¸‰ç±»å…³ç³»ã€‚  \nğŸ”¸å¼•å…¥è‡ªåº•å‘ä¸Šçº§è”æ‘˜è¦æœºåˆ¶ï¼Œåœ¨æ ‘èŠ‚ç‚¹ä¸­æ³¨å…¥å­æ ‘è¯­ä¹‰æ‘˜è¦ï¼Œå¹¶ä¾æ®æ·±åº¦åŠ¨æ€æ§åˆ¶æ‘˜è¦å…³é”®è¯æ•°é‡ä»¥ç¼“è§£ä¿¡æ¯è¡°å‡ã€‚  \nğŸ”¸æ„å»ºé—®é¢˜ç±»å‹æ„ŸçŸ¥çš„åŒè·¯å¾„æ£€ç´¢ï¼šä½ç½®å‹é—®é¢˜é‡‡ç”¨ç½‘æ ¼åˆ’åˆ†+åæ ‡åŒ¹é…ï¼›è¯­ä¹‰å‹é—®é¢˜èåˆLLMå¼•å¯¼çš„èŠ‚ç‚¹ç­›é€‰ã€åµŒå…¥å›é€€æ£€ç´¢ä¸å¤šæ¨¡æ€åå‘éªŒè¯ã€‚  \nğŸ”¸åœ¨è¯æ®èšåˆé˜¶æ®µåŒæ­¥è¾“å…¥æ–‡æœ¬å†…å®¹ã€è£å‰ªå›¾åƒåŒºåŸŸï¼ˆå®šä½è¯æ®ï¼‰åŠæ ‘è·¯å¾„ç´¢å¼•ï¼ˆç»“æ„ä¸Šä¸‹æ–‡ï¼‰ï¼Œæ”¯æ’‘ç«¯åˆ°ç«¯ç­”æ¡ˆç”Ÿæˆã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸CCTreeç»“æ„å»ºæ¨¡ä½¿å±‚æ¬¡ç±»é—®é¢˜å‡†ç¡®ç‡æå‡è‡³76.73%ï¼Œå¤§å¹…é¢†å…ˆZenDBï¼ˆ52.83%ï¼‰å’ŒDocAgentï¼ˆ55.97%ï¼‰ï¼ŒéªŒè¯å…¶å¯¹æ–‡æ¡£åµŒå¥—ç»“æ„çš„åˆ»ç”»èƒ½åŠ›ã€‚  \nğŸ”¸ä½ç½®å‹é—®é¢˜ä¸ŠMoDoraè¾¾68.21%å‡†ç¡®ç‡ï¼Œä¼˜äºGPT-5ï¼ˆ47.02%ï¼‰ï¼Œè¯´æ˜ç½‘æ ¼æ˜ å°„ä¸å¸ƒå±€æ„ŸçŸ¥æ£€ç´¢å¯ç²¾å‡†å®šä½é¡µé¢åŒºåŸŸã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šç§»é™¤æ ‘ç»“æ„å¯¼è‡´å‡†ç¡®ç‡ä¸‹é™è¶…15%ï¼Œè¯å®å±‚æ¬¡åŒ–è¡¨ç¤ºå¯¹å¤šè·³æ¨ç†ä¸å¯æˆ–ç¼ºï¼›ç¼ºå¤±å®šä½è¯æ®ä½¿æ€§èƒ½ä¸‹é™5.07%ï¼Œå‡¸æ˜¾å›¾åƒåŒºåŸŸè¾“å…¥å¯¹éæ–‡æœ¬å…ƒç´ ç†è§£çš„å…³é”®ä½œç”¨ã€‚  \nğŸ”¸ç›¸æ¯”åŸºçº¿ï¼ŒMoDoraåœ¨æ··åˆå‹é—®é¢˜ï¼ˆå«è¡¨æ ¼/å›¾è¡¨ï¼‰ä¸Šä»ä¿æŒ68.00%æœ€é«˜å‡†ç¡®ç‡ï¼Œè€ŒTextRAGä¸ZenDBå› çº¯æ–‡æœ¬è½¬æ¢ä¸¢å¤±ç»“æ„ä¿¡æ¯ï¼Œå‡ ä¹æ— æ³•å¤„ç†æ­¤ç±»é—®é¢˜ã€‚  \nğŸ”¸APIæˆæœ¬åˆ†ææ˜¾ç¤ºï¼ŒMoDoraå•æŸ¥è¯¢èŠ±è´¹0.025ç¾å…ƒï¼Œç²¾åº¦æ¯”GPT-5é«˜16.24%ï¼Œæˆæœ¬ä»…ä¸ºå…¶2.5å€ï¼Œå®ç°ç²¾åº¦ä¸æ•ˆç‡çš„å®ç”¨å¹³è¡¡ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºå°†æ–‡æ¡£è§£æã€ç»“æ„å»ºæ¨¡ä¸æ£€ç´¢æ¨ç†è§£è€¦ä¸ºä¸‰ä¸ªæ­£äº¤ä½†ååŒçš„æ¨¡å—ï¼šç»„ä»¶åŒ–å°è£…è§£å†³OCRè¯­ä¹‰æ–­è£‚é—®é¢˜ï¼›CCTreeé¦–æ¬¡ç»Ÿä¸€å»ºæ¨¡è·¨æ¨¡æ€ç»„ä»¶å…³ç³»ä¸å¸ƒå±€åŒºåˆ†ï¼›é—®é¢˜é©±åŠ¨çš„åŒæ¨¡æ£€ç´¢æœºåˆ¶å…¼é¡¾ä½ç½®ç²¾ç¡®æ€§ä¸è¯­ä¹‰é²æ£’æ€§ã€‚å…¶æ ¸å¿ƒæ€æƒ³ä¸æ˜¯å †å å¤§æ¨¡å‹ï¼Œè€Œæ˜¯ç”¨ç»“æ„åŒ–å…ˆéªŒé™ä½LLMæ¨ç†è´Ÿæ‹…ï¼Œä¸ºåŠç»“æ„åŒ–æ–‡æ¡£ç†è§£æä¾›äº†å¯è§£é‡Šã€å¯æ‰©å±•çš„æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios",
        "authors": [
            "Zhiheng Song",
            "Jingshuai Zhang",
            "Chuan Qin",
            "Chao Wang",
            "Chao Chen",
            "Longfei Xu",
            "Kaikui Liu",
            "Xiangxiang Chu",
            "Hengshu Zhu"
        ],
        "categories": [
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22638v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22638v1",
        "summary": "Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making. However, systematic evaluation in real-world mobility settings is hindered by diverse routing demands, non-deterministic mapping services, and limited reproducibility. In this study, we introduce MobilityBench, a scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap and covers a broad spectrum of route-planning intents across multiple cities worldwide. To enable reproducible, end-to-end evaluation, we design a deterministic API-replay sandbox that eliminates environmental variance from live services. We further propose a multi-dimensional evaluation protocol centered on outcome validity, complemented by assessments of instruction understanding, planning, tool use, and efficiency. Using MobilityBench, we evaluate multiple LLM-based route-planning agents across diverse real-world mobility scenarios and provide an in-depth analysis of their behaviors and performance. Our findings reveal that current models perform competently on Basic information retrieval and Route Planning tasks, yet struggle considerably with Preference-Constrained Route Planning, underscoring significant room for improvement in personalized mobility applications. We publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench .",
        "tag": "è¯„ä¼°åŸºå‡†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22638v1ã€è¯„ä¼°åŸºå‡†-ä¸­å›½ç§‘å­¦é™¢ã€‘MobilityBench_ A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios.pdf",
        "institution_status": "keep",
        "institution": "ä¸­å›½ç§‘å­¦é™¢ã€é˜¿é‡Œ",
        "first_institution": "ä¸­å›½ç§‘å­¦é™¢",
        "institution_category": "å…¶ä»–æœºæ„",
        "note": "ğŸ“–æ ‡é¢˜ï¼šMobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios\nğŸŒæ¥æºï¼šarXiv, 2602.22638v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºçœŸå®å‡ºè¡Œåœºæ™¯è¯„æµ‹åŸºå‡†\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ç³»ç»Ÿã€å¯å¤ç°åœ°è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„è·¯å¾„è§„åˆ’æ™ºèƒ½ä½“åœ¨å¤æ‚ç°å®å‡ºè¡Œåœºæ™¯ä¸­çš„ç»¼åˆèƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºäº†MobilityBenchâ€”â€”é¦–ä¸ªåŸºäºå¤§è§„æ¨¡åŒ¿åçœŸå®ç”¨æˆ·æŸ¥è¯¢æ„å»ºã€æ”¯æŒç«¯åˆ°ç«¯å¯å¤ç°è¯„æµ‹çš„è·¯çº¿è§„åˆ’æ™ºèƒ½ä½“åŸºå‡†ï¼Œæ¶µç›–å¤šåŸå¸‚ã€å¤šæ¨¡æ€ã€å¤šçº¦æŸçš„çœŸå®å‡ºè¡Œä»»åŠ¡ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸é‡‡ç”¨â€œepisode-centricâ€è®¾è®¡ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«è‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€ç¡®å®šæ€§APIå“åº”å¿«ç…§å’Œç»“æ„åŒ–çœŸå€¼æ ‡æ³¨ï¼Œç¡®ä¿ä»»åŠ¡è‡ªæ´½ä¸”æ— éœ€ç”¨æˆ·æ¾„æ¸…ã€‚  \nğŸ”¸æ„å»ºè¦†ç›–22å›½350+åŸå¸‚çš„10ä¸‡æ ·æœ¬æ•°æ®é›†ï¼ŒæŒ‰æ„å›¾åˆ’åˆ†ä¸ºå››å¤§ä»»åŠ¡æ—ï¼ˆåŸºç¡€ä¿¡æ¯æ£€ç´¢ã€è·¯å¾„ä¾èµ–æ£€ç´¢ã€åŸºç¡€è·¯å¾„è§„åˆ’ã€åå¥½çº¦æŸè·¯å¾„è§„åˆ’ï¼‰å…±11ç±»ç»†ç²’åº¦åœºæ™¯ã€‚  \nğŸ”¸è®¾è®¡ç¡®å®šæ€§APIé‡æ”¾æ²™ç®±ï¼Œç¼“å­˜å¹¶æ ‡å‡†åŒ–åœ°å›¾æœåŠ¡å“åº”ï¼ˆå¦‚è·¯å†µã€POIã€å¤©æ°”ï¼‰ï¼Œæ¶ˆé™¤å®æ—¶æœåŠ¡æ³¢åŠ¨å¸¦æ¥çš„ä¸å¯å¤ç°æ€§ã€‚  \nğŸ”¸æå‡ºå¤šç»´è¯„æµ‹åè®®ï¼Œä»æŒ‡ä»¤ç†è§£ï¼ˆæ„å›¾æ£€æµ‹ã€ä¿¡æ¯æŠ½å–ï¼‰ã€è§„åˆ’èƒ½åŠ›ï¼ˆä»»åŠ¡åˆ†è§£ï¼‰ã€å·¥å…·ä½¿ç”¨ï¼ˆå·¥å…·é€‰æ‹©ã€æ¨¡å¼åˆè§„ï¼‰ã€å†³ç­–è´¨é‡ï¼ˆäº¤ä»˜ç‡ã€æœ€ç»ˆé€šè¿‡ç‡ï¼‰åŠæ•ˆç‡ï¼ˆè¾“å…¥/è¾“å‡ºtokenï¼‰äº”ä¸ªç»´åº¦è¿›è¡Œç»†ç²’åº¦è¯Šæ–­ã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸å½“å‰LLMè·¯å¾„è§„åˆ’æ™ºèƒ½ä½“åœ¨åŸºç¡€ä¿¡æ¯æ£€ç´¢ä¸ç‚¹å¯¹ç‚¹è·¯å¾„è§„åˆ’ä¸Šè¡¨ç°è‰¯å¥½ï¼ˆFPRè¾¾60%â€“70%ï¼‰ï¼Œä½†åœ¨åå¥½çº¦æŸè·¯å¾„è§„åˆ’ï¼ˆå¦‚é¿é«˜é€Ÿã€é™æ¢ä¹˜ï¼‰ä¸Šæ˜¾è‘—ä¸è¶³ï¼Œæš´éœ²ä¸ªæ€§åŒ–å‡ºè¡Œæ”¯æŒèƒ½åŠ›çŸ­æ¿ã€‚  \nğŸ”¸Plan-and-Executeæ¡†æ¶åœ¨é€»è¾‘å¼ºçº¦æŸä»»åŠ¡ä¸­æ›´ç¨³å®šï¼Œè€ŒReActå› é—­ç¯åé¦ˆæœºåˆ¶åœ¨æ•´ä½“æˆåŠŸç‡ä¸Šç•¥ä¼˜ï¼Œä½†æ¨ç†å¼€é”€é«˜ï¼ˆå¹³å‡è¾“å…¥tokené«˜35.4%ï¼‰ã€‚  \nğŸ”¸é—­æºæ¨¡å‹ï¼ˆClaude/Geminiï¼‰åœ¨æŒ‡ä»¤ç†è§£ä¸Šé¢†å…ˆï¼Œä½†å¼€æºå¤§æ¨¡å‹ï¼ˆQwen235B-A22Bã€DeepSeek-V3.2-Expï¼‰å·²æ¥è¿‘å…¶æ°´å¹³ï¼Œä¸”å…·å¤‡æ›´é«˜æ€§ä»·æ¯”ã€‚  \nğŸ”¸å¯ç”¨â€œThinkingâ€æ¨ç†æ¨¡å¼å¯æå‡æœ€ç»ˆé€šè¿‡ç‡ï¼ˆæœ€é«˜+5.98%ï¼‰ï¼Œä½†å¸¦æ¥æ˜¾è‘—å»¶è¿Ÿä¸tokenå¼€é”€ï¼Œåˆ¶çº¦å®æ—¶éƒ¨ç½²å¯è¡Œæ€§ã€‚  \n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†çœŸå®ä¸–ç•Œå‡ºè¡Œå¤æ‚æ€§ï¼ˆå¤šæ¨¡æ€ã€åŠ¨æ€çº¦æŸã€é•¿å°¾æ„å›¾ï¼‰ä¸ä¸¥æ ¼å¯å¤ç°æ€§ï¼ˆæ²™ç®±é‡æ”¾ã€ç»“æ„åŒ–çœŸå€¼ï¼‰ç»“åˆï¼Œå¡«è¡¥äº†é¢†åŸŸè¯„æµ‹ç©ºç™½ï¼›å…¶å¤šç»´è¯Šæ–­åè®®è¶…è¶Šä¼ ç»Ÿç«¯åˆ°ç«¯å‡†ç¡®ç‡ï¼Œä¸ºæ¨¡å‹èƒ½åŠ›å½’å› æä¾›æ–°èŒƒå¼ï¼›å…¬å¼€æ•°æ®ä¸å·¥å…·é“¾æå¤§ä¿ƒè¿›å…¬å¹³æ¯”è¾ƒä¸æŠ€æœ¯è¿­ä»£ã€‚\n    "
    },
    {
        "title": "Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search",
        "authors": [
            "Xun Huang",
            "Simeng Qin",
            "Xiaoshuang Jia",
            "Ranjie Duan",
            "Huanqian Yan",
            "Zhitao Zeng",
            "Fei Yang",
            "Yang Liu",
            "Xiaojun Jia"
        ],
        "categories": [
            "cs.AI",
            "cs.CR"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22983v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22983v1",
        "summary": "As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.",
        "tag": "å®‰å…¨å¯¹é½",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22983v1ã€å®‰å…¨å¯¹é½-å—æ´‹ç†å·¥å¤§å­¦ã€‘Obscure but Effective_ Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search.pdf",
        "institution_status": "keep",
        "institution": "å—æ´‹ç†å·¥å¤§å­¦ã€BraneMatrix AIã€å—äº¬ç†å·¥å¤§å­¦ã€ä¸œåŒ—å¤§å­¦ã€ä¸­å›½äººæ°‘å¤§å­¦ã€é˜¿é‡Œå·´å·´é›†å›¢ã€åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦ã€æ–°åŠ å¡å›½ç«‹å¤§å­¦ã€æµ™æ±Ÿå®éªŒå®¤",
        "first_institution": "å—æ´‹ç†å·¥å¤§å­¦",
        "institution_category": "å…¶ä»–æœºæ„",
        "note": "ğŸ“–æ ‡é¢˜ï¼šObscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search\nğŸŒæ¥æºï¼šarXiv, 2602.22983v1\n\nç¬”è®°æ ‡é¢˜ï¼šå¤å…¸ä¸­æ–‡æ¿€å‘é»‘ç›’è¶Šç‹±\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šä¸ºä½•å¤å…¸ä¸­æ–‡èƒ½æœ‰æ•ˆç»•è¿‡å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨å¯¹é½æœºåˆ¶å¹¶å®ç°é«˜æˆåŠŸç‡çš„é»‘ç›’è¶Šç‹±æ”»å‡»ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šé¦–æ¬¡ç³»ç»Ÿæ€§æå‡ºå°†å¤å…¸ä¸­æ–‡ä½œä¸ºå¯¹æŠ—æç¤ºç”Ÿæˆçš„æ–°è¯­è¨€ç»´åº¦ï¼Œæ„å»ºåŸºäºå…«ç»´ç­–ç•¥ç©ºé—´ä¸æœè‡ä»¿ç”Ÿä¼˜åŒ–çš„é»‘ç›’è¶Šç‹±æ¡†æ¶CC-BOSã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æå‡ºå¤å…¸ä¸­æ–‡è¯­å¢ƒä¸‹çš„å…«ç»´ç­–ç•¥ç©ºé—´ï¼Œæ¶µç›–è§’è‰²èº«ä»½ã€è¡Œä¸ºå¼•å¯¼ã€ä½œç”¨æœºåˆ¶ã€éšå–»æ˜ å°„ã€è¡¨è¾¾é£æ ¼ã€çŸ¥è¯†å…³è”ã€æƒ…å¢ƒè®¾å®šå’Œè§¦å‘æ¨¡å¼ï¼Œç»Ÿä¸€å»ºæ¨¡ç°æœ‰è¶Šç‹±ç­–ç•¥ä¸å¤æ–‡ç‰¹æ€§ã€‚  \nğŸ”¸è®¾è®¡æœè‡ä»¿ç”Ÿä¼˜åŒ–ç®—æ³•ï¼Œèåˆå—…è§‰æœç´¢ï¼ˆå±€éƒ¨æ‰°åŠ¨ï¼‰ã€è§†è§‰æœç´¢ï¼ˆå‘æœ€ä¼˜è§£å¸å¼•ï¼‰ä¸æŸ¯è¥¿å˜å¼‚ï¼ˆé€ƒé€¸å±€éƒ¨æœ€ä¼˜ï¼‰ï¼Œåœ¨ç¦»æ•£ç­–ç•¥ç©ºé—´ä¸­é«˜æ•ˆè¿­ä»£å¯»ä¼˜ã€‚  \nğŸ”¸æ„å»ºä¸¤é˜¶æ®µç¿»è¯‘æ¨¡å—ï¼ˆæ–‡è¨€â†’ç™½è¯â†’è‹±æ–‡ï¼‰ï¼Œç¼“è§£è¯­ä¹‰å‹ç¼©ä¸éšå–»æ­§ä¹‰ï¼Œä¿éšœè·¨è¯­è¨€å“åº”è¯„ä¼°çš„ä¸€è‡´æ€§ä¸å¯é æ€§ã€‚  \nğŸ”¸é‡‡ç”¨å“ˆå¸Œå»é‡ä¸æ—©åœæœºåˆ¶æå‡æœç´¢ç¨³å®šæ€§ï¼Œå¹¶ä»¥å…³é”®è¯åŒ¹é…ä¸æ„å›¾ä¸€è‡´æ€§åŒæŒ‡æ ‡åŠ æƒå®šä¹‰é€‚åº”åº¦å‡½æ•°ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸CC-BOSåœ¨6ä¸ªä¸»æµé»‘ç›’æ¨¡å‹ä¸Šå‡è¾¾100%æ”»å‡»æˆåŠŸç‡ï¼Œæ˜¾è‘—è¶…è¶ŠPAIRã€TAPã€ICRTç­‰SOTAæ–¹æ³•ï¼Œä¸”å¹³å‡æ¯’æ€§å¾—åˆ†æ›´é«˜ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šå¤å…¸ä¸­æ–‡åŸºåº•ï¼ˆ+18%â†’60%ï¼‰ã€å…«ç»´ç­–ç•¥ï¼ˆ+60%â†’100%ï¼‰ä¸ä»¿ç”Ÿä¼˜åŒ–ä¸‰è€…ç¼ºä¸€ä¸å¯ï¼ŒååŒæå‡è¶Šç‹±æ•ˆèƒ½ã€‚  \nğŸ”¸ç¿»è¯‘æ¨¡å—ä½¿ASRä»90%æå‡è‡³100%ï¼ŒéªŒè¯å…¶å¯¹è¯„ä¼°å‡†ç¡®æ€§çš„å…³é”®ä½œç”¨ï¼›åœ¨Llama-Guard-3é˜²å¾¡ä¸‹ä»ä¿æŒ40%æˆåŠŸç‡ï¼Œå±•ç°å¼ºé²æ£’æ€§ã€‚  \nğŸ”¸è·¨æ¨¡å‹è¿ç§»å®éªŒæ˜¾ç¤ºï¼Œç”Ÿæˆçš„å¤å…¸ä¸­æ–‡æç¤ºåœ¨ä¸åŒLLMé—´å…·æœ‰é«˜è¾¾96%çš„è½¬ç§»æˆåŠŸç‡ï¼Œè¯æ˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚  \nğŸ”¸æ‹‰ä¸æ–‡ä¸æ¢µæ–‡æ‰©å±•å®éªŒè¡¨æ˜è¯¥æ¼æ´æºäºâ€œé«˜ç†è§£åŠ›â€”ä½å®‰å…¨å¯¹é½â€çš„ç»“æ„æ€§å¤±é…ï¼Œéå¤å…¸ä¸­æ–‡ç‰¹æœ‰ï¼Œå…·è¯­è¨€æ™®é€‚æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡åˆ›æ–°æ€§åœ¨äºå°†è¯­è¨€å­¦ç‰¹æ€§ï¼ˆå¤å…¸ä¸­æ–‡çš„å‡ç»ƒæ€§ã€å¤šä¹‰æ€§ã€éšå–»æ€§ï¼‰è½¬åŒ–ä¸ºå¯è®¡ç®—çš„å¯¹æŠ—ç»´åº¦ï¼Œçªç ´ä»¥å¾€ä»…ä¾èµ–ç°ä»£è¯­è¨€æˆ–æ¨¡æ¿å·¥ç¨‹çš„å±€é™ï¼›å…¶å…«ç»´ç­–ç•¥ç©ºé—´è®¾è®¡å…¼å…·ç†è®ºæŠ½è±¡æ€§ä¸å·¥ç¨‹å¯æ“ä½œæ€§ï¼Œä¸ºå¤šæ¨¡æ€ã€è·¨æ–‡åŒ–AIå®‰å…¨ç ”ç©¶å¼€è¾Ÿæ–°è·¯å¾„ï¼›ä½†éœ€è­¦æƒ•è¯¥æŠ€æœ¯è¢«æ»¥ç”¨é£é™©ï¼Œå…¶æ ¸å¿ƒä»·å€¼åœ¨äºæš´éœ²å¯¹é½ç›²åŒºï¼Œæ¨åŠ¨æ„å»ºæ›´é²æ£’çš„è·¨è¯­è¨€å®‰å…¨æŠ¤æ ã€‚\n    "
    },
    {
        "title": "OmniGAIA: Towards Native Omni-Modal AI Agents",
        "authors": [
            "Xiaoxi Li",
            "Wenxiang Jiao",
            "Jiarui Jin",
            "Shijian Wang",
            "Guanting Dong",
            "Jiajie Jin",
            "Hao Wang",
            "Yinuo Wang",
            "Ji-Rong Wen",
            "Yuan Lu",
            "Zhicheng Dou"
        ],
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG",
            "cs.MM"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22897v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22897v1",
        "summary": "Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.",
        "tag": "å¤šæ¨¡æ€è¯„ä¼°åŸºå‡†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22897v1ã€å¤šæ¨¡æ€è¯„ä¼°åŸºå‡†-ä¸­å›½äººæ°‘å¤§å­¦ã€‘OmniGAIA_ Towards Native Omni-Modal AI Agents.pdf",
        "institution_status": "keep",
        "institution": "ä¸­å›½äººæ°‘å¤§å­¦ã€å°çº¢ä¹¦",
        "first_institution": "ä¸­å›½äººæ°‘å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šOmniGAIA: Towards Native Omni-Modal AI Agents\nğŸŒæ¥æºï¼šarXiv, 2602.22897v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºåŸç”Ÿå…¨æ¨¡æ€æ™ºèƒ½ä½“\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è¯„ä¼°å’Œæå‡AIæ¨¡å‹åœ¨è§†é¢‘ã€å›¾åƒã€éŸ³é¢‘ä¸‰æ¨¡æ€èåˆä¸‹çš„é•¿ç¨‹æ¨ç†ä¸å¤šæ­¥å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºé¦–ä¸ªé¢å‘åŸç”Ÿå…¨æ¨¡æ€æ™ºèƒ½ä½“çš„åŸºå‡†OmniGAIAåŠé…å¥—æ™ºèƒ½ä½“OmniAtlasï¼Œç³»ç»Ÿè§£å†³è·¨æ¨¡æ€æ·±åº¦æ¨ç†ä¸ä¸»åŠ¨æ„ŸçŸ¥ä¸‹çš„å·¥å…·ååŒéš¾é¢˜ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºOmniGAIAåŸºå‡†ï¼šåŸºäºå…¨æ¨¡æ€äº‹ä»¶å›¾æ–¹æ³•ï¼Œä»çœŸå®éŸ³è§†é¢‘ä¸å›¾æ–‡æ•°æ®ä¸­æŒ–æ˜ç»†ç²’åº¦ä¿¡å·ï¼Œå»ºæ¨¡è·¨æ¨¡æ€å®ä½“ä¸äº‹ä»¶å…³ç³»ï¼Œå¹¶é€šè¿‡ä¸»åŠ¨æ‰©å±•ä¸èŠ‚ç‚¹æ¨¡ç³ŠåŒ–ç”Ÿæˆéœ€å¤šè·³æ¨ç†ä¸å·¥å…·éªŒè¯çš„å¼€æ”¾å‹é—®ç­”ä»»åŠ¡ã€‚  \nğŸ”¸è®¾è®¡OmniAtlasæ™ºèƒ½ä½“ï¼šé‡‡ç”¨å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰èŒƒå¼ï¼Œæ”¯æŒâ€œæŒ‰éœ€æ„ŸçŸ¥â€â€”â€”å¯åŠ¨æ€è°ƒç”¨read_video/read_audio/read_imageç­‰æ“ä½œç²¾å‡†è·å–å…³é”®ç‰‡æ®µï¼Œé¿å…å…¨å±€ä¸‹é‡‡æ ·å¯¼è‡´çš„ä¿¡æ¯æŸå¤±ã€‚  \nğŸ”¸æå‡ºä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šå…ˆé€šè¿‡å›æº¯å¼•å¯¼çš„æ ‘æ¢ç´¢åˆæˆé«˜è´¨é‡å·¥å…·è°ƒç”¨è½¨è¿¹ï¼Œå†ç»“åˆè½¨è¿¹çº§ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸ç»†ç²’åº¦é”™è¯¯ä¿®æ­£æ–¹æ³•OmniDPOï¼Œèšç„¦ä¿®æ­£æ„ŸçŸ¥ã€æ¨ç†ã€å·¥å…·ä½¿ç”¨ç­‰æ¨¡å—çº§é”™è¯¯ã€‚  \nğŸ”¸å»ºç«‹ä¸¥æ ¼è´¨é‡ç®¡æ§æµç¨‹ï¼šèåˆå¤§æ¨¡å‹åˆç­›ï¼ˆåˆ¤æ–­é—®é¢˜è‡ªç„¶æ€§ã€æ¨¡æ€å¿…è¦æ€§ã€ç­”æ¡ˆå”¯ä¸€æ€§ï¼‰ã€éš¾åº¦å¢å¼ºä¸äººå·¥ä¸‰é‡æ ¡éªŒï¼Œç¡®ä¿ä»»åŠ¡å¯è§£ã€æ— æ­§ä¹‰ã€å…·æŒ‘æˆ˜æ€§ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸OmniGAIAæå…·æŒ‘æˆ˜æ€§ï¼šæœ€å¼ºé—­æºæ¨¡å‹Gemini-3-Proä»…è¾¾62.5 Pass@1ï¼Œè€Œæœ€ä½³å¼€æºåŸºçº¿Qwen3-Omniä»…ä¸º13.3ï¼Œå‡¸æ˜¾å½“å‰å¼€æºæ¨¡å‹åœ¨å…¨æ¨¡æ€ä»£ç†èƒ½åŠ›ä¸Šçš„å·¨å¤§å·®è·ã€‚  \nğŸ”¸å·¥å…·è°ƒç”¨æ˜¯æˆè´¥å…³é”®ï¼šå¤±è´¥æ¡ˆä¾‹ä¸­â€œæ— æ•ˆå·¥å…·è°ƒç”¨â€ä¸â€œæ¨ç†é”™è¯¯â€å æ¯”æœ€é«˜ï¼ˆè¾¾35%â€“96%ï¼‰ï¼Œä¸”ç¡¬ä»»åŠ¡ä¸­äºŒè€…å‘ˆçº§è”å¤±æ•ˆï¼›å•çº¯å¢åŠ è°ƒç”¨æ¬¡æ•°ä¸æå‡æˆåŠŸç‡ï¼Œä½æ•ˆâ€œå·¥å…·æŠ–åŠ¨â€æ™®éå­˜åœ¨ã€‚  \nğŸ”¸åŸç”Ÿæ„ŸçŸ¥ä¸å¯æ›¿ä»£ï¼šæ¶ˆèå®éªŒè¯æ˜ï¼Œå¯¹å¼ºæ¨¡å‹ï¼ˆå¦‚Gemini-3-Flashï¼‰ï¼ŒåŸç”Ÿå¤šæ¨¡æ€è¾“å…¥æ€§èƒ½æœ€ä¼˜ã€æˆæœ¬æœ€ä½ï¼›å¯¹å¼±æ¨¡å‹ï¼Œå·¥å…·è¾…åŠ©æ„ŸçŸ¥ä»…èƒ½æå‡ç®€å•ä»»åŠ¡è¡¨ç°ï¼Œæ— æ³•å¼¥è¡¥é•¿ç¨‹è·¨æ¨¡æ€æ¨ç†ç¼ºé™·ã€‚  \nğŸ”¸OmniAtlasæ˜¾è‘—ææ•ˆï¼šåœ¨Qwen3-Omniä¸Šå°†Pass@1ä»13.3æå‡è‡³20.8ï¼Œå·¥å…·è¯¯ç”¨ç‡ä¸‹é™21.7ä¸ªç™¾åˆ†ç‚¹ï¼ŒéªŒè¯å…¶è®­ç»ƒèŒƒå¼å¯¹è§£é”å¼€æºæ¨¡å‹ä»£ç†æ½œåŠ›çš„æœ‰æ•ˆæ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ¨äºé¦–æ¬¡å°†â€œåŸç”Ÿå…¨æ¨¡æ€â€ä¸â€œä»£ç†å¼å·¥å…·ååŒâ€æ·±åº¦è€¦åˆï¼šä¸ä»…æ„å»ºäº†é¦–ä¸ªè¦†ç›–è§†é¢‘/å›¾åƒ/éŸ³é¢‘ã€å¼ºè°ƒå¤šè·³éªŒè¯ä¸å¼€æ”¾ç­”æ¡ˆçš„ä¸¥è‹›åŸºå‡†ï¼Œæ›´æå‡ºä¸»åŠ¨æ„ŸçŸ¥æœºåˆ¶ä¸ç»†ç²’åº¦çº é”™è®­ç»ƒæ¡†æ¶ï¼Œç›´å‡»å½“å‰æ¨¡å‹åœ¨è¯æ®é”šå®šã€å‡è®¾æ£€éªŒä¸è·¨æ¨¡æ€éªŒè¯ç­‰æ ¸å¿ƒç¯èŠ‚çš„è–„å¼±ç‚¹ï¼Œä¸ºä¸‹ä¸€ä»£é€šç”¨AIåŠ©æ‰‹æä¾›äº†å¯å¤ç°ã€å¯æ¼”è¿›çš„æŠ€æœ¯è·¯å¾„ã€‚\n    "
    },
    {
        "title": "PRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training",
        "authors": [
            "Yanyi Li",
            "Yimu Zhang",
            "Cong Fang"
        ],
        "categories": [
            "cs.LG"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.23111v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23111v1",
        "summary": "Activations have become the primary memory bottleneck in large-batch LLM training. However, existing compression methods fail to exploit the spectral structure of activations, resulting in slow convergence or limited compression. To address this, we bridge the relationship between the algorithm's fast convergence and the requirements for subspace projection, and show that an effective compression should yield an unbiased estimate of the original activation with low variance. We propose Principal-Random Subspace for LLM Activation Compression (PRAC), which novelly decomposes activations into two components: a principal subspace captured via SVD to retain dominant information, and a random subspace sampled from the orthogonal complement to approximate the tail. By introducing a precise scaling factor, we prove that PRAC yields an unbiased gradient estimator with minimum variance under certain conditions. Extensive experiments on pre-training and fine-tuning tasks demonstrate that PRAC achieves up to 36% total memory reduction with negligible performance degradation and minimal computational cost.",
        "tag": "è®­ç»ƒæ˜¾å­˜ä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.23111v1ã€è®­ç»ƒæ˜¾å­˜ä¼˜åŒ–-åŒ—äº¬å¤§å­¦ã€‘PRAC_ Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training.pdf",
        "institution_status": "keep",
        "institution": "åŒ—äº¬å¤§å­¦",
        "first_institution": "åŒ—äº¬å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šPRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training\nğŸŒæ¥æºï¼šarXiv, 2602.23111v1\n\nç¬”è®°æ ‡é¢˜ï¼šèåˆä¸»éšæœºå­ç©ºé—´å‹ç¼©æ¿€æ´»\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨å¤§æ‰¹æ¬¡LLMè®­ç»ƒä¸­é«˜æ•ˆå‹ç¼©æ¿€æ´»å€¼ï¼Œä½¿å…¶åœ¨æ˜¾è‘—é™ä½å†…å­˜çš„åŒæ—¶ä¸æŸå®³æ”¶æ•›é€Ÿåº¦å’Œæ¨¡å‹æ€§èƒ½ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºPRACæ–¹æ³•ï¼Œé¦–æ¬¡å°†æ¿€æ´»çš„è°±ç»“æ„ï¼ˆä¸»æˆåˆ†+é•¿å°¾ï¼‰æ˜¾å¼å»ºæ¨¡ï¼Œç†è®ºè¯æ˜å…¶åœ¨é€€åŒ–æ¡ä»¶ä¸‹å¯å®ç°æ— åä¸”æœ€å°æ–¹å·®çš„æ¢¯åº¦ä¼°è®¡ï¼Œè¾¾æˆæœ€é«˜36%æ€»å†…å­˜ç¼©å‡ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸åŸºäºä¼˜åŒ–æ”¶æ•›ç†è®ºï¼ŒæŒ‡å‡ºæœ‰æ•ˆæ¿€æ´»å‹ç¼©éœ€åŒæ—¶æ»¡è¶³æ— åæ€§ä¸ä½æ–¹å·®ï¼Œè€Œéä»…è¿½æ±‚é‡å»ºç²¾åº¦ã€‚  \nğŸ”¸è§‚å¯Ÿåˆ°LLMæ¿€æ´»å…·æœ‰â€œä¸»å¯¼å¥‡å¼‚å€¼+ç¼“æ…¢è¡°å‡é•¿å°¾â€çš„é€€åŒ–ç»“æ„ï¼Œæ®æ­¤æå‡ºåŒå­ç©ºé—´åˆ†è§£ï¼šSVDæå–ä¸»å­ç©ºé—´ä¿ç•™å¼ºä¿¡å·ï¼Œæ­£äº¤è¡¥ç©ºé—´å†…å‡åŒ€é‡‡æ ·éšæœºå­ç©ºé—´é€¼è¿‘é•¿å°¾ã€‚  \nğŸ”¸å¼•å…¥ç²¾ç¡®ç¼©æ”¾å› å­k=(nâˆ’râ‚)/râ‚‚ï¼Œä½¿éšæœºåˆ†é‡èƒ½é‡è¡¥å¿è¢«æˆªæ–­çš„å°¾éƒ¨èƒ½é‡ï¼Œä¸¥æ ¼ä¿è¯é‡å»ºä¸æ¢¯åº¦ä¼°è®¡çš„æ— åæ€§ã€‚  \nğŸ”¸è®¾è®¡åŠ¨æ€æ›´æ–°ç­–ç•¥ï¼šä¸»/éšæœºå­ç©ºé—´æŒ‰å›ºå®šæ­¥æ•°æƒ°æ€§æ›´æ–°ï¼›è·¨å±‚å…±äº«æŠ•å½±çŸ©é˜µï¼›çº¿æ€§å±‚ä¸éçº¿æ€§å±‚é‡‡ç”¨å·®å¼‚åŒ–ç§©é…ç½®ï¼Œå…¼é¡¾æ•ˆç‡ä¸ç¨³å®šæ€§ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸PRACåœ¨LLaMA/GPTç³»åˆ—é¢„è®­ç»ƒä¸­å®ç°æœ€é«˜36%æ€»å†…å­˜ä¸‹é™ï¼ˆå¦‚LLaMA-1Bä»94.5GBé™è‡³60.48GBï¼‰ï¼ŒéªŒè¯æŸå¤±ä¸åŸºçº¿å‡ ä¹æ— å·®å¼‚ã€‚  \nğŸ”¸ç›¸æ¯”çº¯ä¸»æˆåˆ†ï¼ˆPACï¼‰åæœŸæ”¶æ•›åœæ»ã€çº¯éšæœºï¼ˆRACï¼‰åˆæœŸé«˜æ–¹å·®éœ‡è¡ï¼ŒPRACå…¨ç¨‹ä¿æŒç¨³å®šå¿«é€Ÿæ”¶æ•›ï¼ŒéªŒè¯åŒå­ç©ºé—´ååŒä¼˜åŠ¿ã€‚  \nğŸ”¸åœ¨RoBERTaå¾®è°ƒä»»åŠ¡ä¸­ï¼ŒPRACä¸ä»…å†…å­˜å‡å°‘38%ï¼Œéƒ¨åˆ†ä»»åŠ¡æŒ‡æ ‡ç”šè‡³è¶…è¶Šå…¨å‚æ•°å¾®è°ƒï¼Œè¡¨æ˜éšæœºåˆ†é‡å…·æœ‰æ­£åˆ™åŒ–æ•ˆåº”ã€‚  \nğŸ”¸ä¸GaLoreã€RSOç­‰åŸºçº¿ç›¸æ¯”ï¼ŒPRACåœ¨åŒç­‰å†…å­˜ä¸‹æ€§èƒ½æ›´ä¼˜ï¼›ä¸å…ˆè¿›ä¼˜åŒ–å™¨ï¼ˆMuonã€Adam-miniï¼‰æ­£äº¤å…¼å®¹ï¼Œå¯å åŠ èŠ‚çœçº¦27%â€“32%å†…å­˜ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºå°†æ¿€æ´»çš„å†…åœ¨è°±ç»“æ„ï¼ˆä½ç§©+é•¿å°¾é€€åŒ–ï¼‰è½¬åŒ–ä¸ºå¯è¯æ˜æœ€ä¼˜çš„å‹ç¼©èŒƒå¼ï¼šä¸»å­ç©ºé—´ä¿éšœä¿¡æ¯ä¿çœŸï¼Œéšæœºå­ç©ºé—´ä»¥æœ€å°æ–¹å·®è¦†ç›–æ®‹å·®ï¼Œç¼©æ”¾å› å­å®ç°ç†è®ºæœ€ä¼˜æƒè¡¡ã€‚å…¶è´¡çŒ®ä¸ä»…æ˜¯å·¥ç¨‹æŠ€å·§ï¼Œæ›´æ˜¯é¦–æ¬¡ä¸ºæ¿€æ´»å‹ç¼©å»ºç«‹äº†â€œæ— å+æœ€å°æ–¹å·®â€çš„æ”¶æ•›æ€§ç†è®ºåŸºçŸ³ã€‚\n    "
    },
    {
        "title": "ParamMem: Augmenting Language Agents with Parametric Reflective Memory",
        "authors": [
            "Tianjun Yao",
            "Yongqiang Chen",
            "Yujia Zheng",
            "Pan Li",
            "Zhiqiang Shen",
            "Kun Zhang"
        ],
        "categories": [
            "cs.LG",
            "cs.MA"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.23320v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23320v1",
        "summary": "Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. Building on this module, we propose ParamAgent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. Extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. Further analysis reveals that ParamMem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential of ParamMem as an effective component for enhancing language agents.",
        "tag": "Agent è®°å¿†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.23320v1ã€Agent è®°å¿†-å¡å†…åŸºæ¢…éš†å¤§å­¦ã€‘ParamMem_ Augmenting Language Agents with Parametric Reflective Memory.pdf",
        "institution_status": "keep",
        "institution": "å¡å†…åŸºæ¢…éš†å¤§å­¦ã€Mohamed bin Zayed University of Artificial Intelligenceã€Georgia Institute of Technology",
        "first_institution": "å¡å†…åŸºæ¢…éš†å¤§å­¦",
        "institution_category": "å›½å¤–å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šParamMem: Augmenting Language Agents with Parametric Reflective Memory\nğŸŒæ¥æºï¼šarXiv, 2602.23320v1\n\nç¬”è®°æ ‡é¢˜ï¼šå‚æ•°åŒ–è®°å¿†å¢å¼ºåæ€å¤šæ ·æ€§\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è¿›ä¸€æ­¥æ‰©å¤§è¯­è¨€ä»£ç†çš„åæ€å¤šæ ·æ€§ä»¥æå‡æ¨ç†æ€§èƒ½ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºParamMemâ€”â€”ä¸€ç§å°†è·¨æ ·æœ¬åæ€æ¨¡å¼ç¼–ç è¿›æ¨¡å‹å‚æ•°çš„è½»é‡çº§å‚æ•°åŒ–è®°å¿†æ¨¡å—ï¼Œé€šè¿‡æ¸©åº¦æ§åˆ¶é‡‡æ ·ç”Ÿæˆå¤šæ ·åŒ–åæ€ä¿¡å·ï¼Œæ˜¾è‘—æå‡è¯­è¨€ä»£ç†çš„æ¨ç†èƒ½åŠ›ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºè¾…åŠ©åæ€æ•°æ®é›†D={(x_i, rg_i)}ï¼Œå…¶ä¸­rg_iç”±å¼ºLLMï¼ˆå¦‚GPT-4o-miniï¼‰å¯¹è¾“å…¥x_iç”Ÿæˆçš„ç»“æ„åŒ–åæ€ï¼ˆç¼–ç¨‹ä»»åŠ¡å«é”™è¯¯æšä¸¾ã€æ•°å­¦ä»»åŠ¡å«æ¨å¯¼é™·é˜±ã€å¤šè·³QAå«è¯­ä¹‰åˆ†è§£å•å…ƒï¼‰æ„æˆã€‚  \nğŸ”¸é‡‡ç”¨LoRAé«˜æ•ˆå¾®è°ƒè½»é‡LLMï¼ˆå¦‚Llama-3.1-8Bï¼‰ä½œä¸ºParamMemæ¨¡å—M_gï¼Œä½¿å…¶éšå¼å­¦ä¹ è·¨æ ·æœ¬åæ€è§„å¾‹ï¼Œè€Œéä¾èµ–æç¤ºæ¨¡æ¿æˆ–æ£€ç´¢ç›¸ä¼¼æ ·æœ¬ã€‚  \nğŸ”¸åœ¨åå°„æ¡†æ¶ä¸­ï¼Œæ¯è½®è¿­ä»£é™¤ä½¿ç”¨å†å²è‡ªåæ€r_{1:kâˆ’1}å¤–ï¼Œé¢å¤–é‡‡æ ·ParamMemè¾“å‡ºrg_kï¼ˆæ¸©åº¦T=0.2é¦–è½®ã€T=1.0åç»­è½®ï¼‰ï¼Œä¸actoræ¨¡å‹è”åˆæ¡ä»¶ç”Ÿæˆy_kã€‚  \nğŸ”¸æå‡ºParamAgentï¼ˆèåˆæ—¶åºè®°å¿†+å‚æ•°åŒ–è®°å¿†ï¼‰å’ŒParamAgent-plusï¼ˆä¸‰é‡è®°å¿†ï¼šæ—¶åº+è·¨æ ·æœ¬+å‚æ•°åŒ–ï¼‰ï¼Œç»Ÿä¸€å»ºæ¨¡ä¸åŒè®°å¿†æºçš„äº’è¡¥æ€§ã€‚  \nğŸ”¸æ”¯æŒæ— éœ€å¤–éƒ¨å¼ºæ¨¡å‹çš„è‡ªæ”¹è¿›ï¼šç”¨åŸºæ¨¡å‹è‡ªèº«ç”Ÿæˆçš„æ•°æ®è®­ç»ƒParamMemï¼Œä»èƒ½æŒç»­æå‡æ€§èƒ½ï¼›ä¸”å°æ¨¡å‹ParamMemå¯æœ‰æ•ˆå¢å¼ºå¤§æ¨¡å‹agentï¼ˆå¼±åˆ°å¼ºè¿ç§»ï¼‰ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åæ€å¤šæ ·æ€§ï¼ˆå¹³å‡æˆå¯¹ä½™å¼¦è·ç¦»ï¼‰ä¸ä»»åŠ¡å‡†ç¡®ç‡å‘ˆå¼ºæ­£ç›¸å…³ï¼ˆå¹³å‡r=0.76ï¼‰ï¼ŒéªŒè¯æå‡å¤šæ ·æ€§æ˜¯å…³é”®ä¼˜åŒ–æ–¹å‘ã€‚  \nğŸ”¸ParamMemæ˜¾è‘—å¢åŠ åæ€èšç±»æ•°ï¼ˆK*=39 vs Reflexionçš„32ï¼‰ä¸è½®å»“ç³»æ•°ï¼Œè¯æ˜å…¶å¼•å…¥äº†ç‹¬ç«‹äºæ—¶åº/è·¨æ ·æœ¬è®°å¿†çš„æ–°é¢–è¯­ä¹‰å¤šæ ·æ€§å±‚ã€‚  \nğŸ”¸å¤šæ ·åæ€æ‰©å±•äº†é”™è¯¯è¯Šæ–­çš„å‡è®¾ç©ºé—´ï¼Œä½¿agentæ›´å¯èƒ½æ•è·æ­£ç¡®ä¿®æ­£çº¿ç´¢ï¼Œå°¤å…¶åœ¨Reflexion/DoTå¤±è´¥æ¡ˆä¾‹ä¸­ä½“ç°æ˜æ˜¾ã€‚  \nğŸ”¸ä»…éœ€çº¦500ä¸ªK-meansé‡‡æ ·çš„å¤šæ ·æ ·æœ¬å³å¯è¾¾åˆ°ä¼˜å¼‚æ€§èƒ½ï¼ŒParamAgent-plusç”¨500æ ·æœ¬å³è¶…è¶Šç”¨8000æ ·æœ¬è®­ç»ƒçš„ParamAgentï¼Œå‡¸æ˜¾æ ·æœ¬æ•ˆç‡ã€‚  \nğŸ”¸ParamMemæ”¯æŒå¼±åˆ°å¼ºè¿ç§»ï¼š8B ParamMemå¯æå‡70BåŸºæ¨¡å‹æ€§èƒ½ï¼›ä¸”æ— éœ€å¼ºæ¨¡å‹æ ‡æ³¨ï¼Œä»…ç”¨åŸºæ¨¡å‹è‡ªäº§æ•°æ®å³å¯å®ç°è‡ªæˆ‘æ”¹è¿›ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºè·³å‡ºâ€œæç¤ºå·¥ç¨‹â€ä¸â€œæ£€ç´¢å¢å¼ºâ€çš„ä¼ ç»ŸèŒƒå¼ï¼Œé¦–æ¬¡å°†åæ€å¤šæ ·æ€§å»ºæ¨¡ä¸ºå¯å­¦ä¹ çš„å‚æ•°åŒ–è®°å¿†ï¼Œä»¥è½»é‡å¾®è°ƒæ–¹å¼å†…åŒ–è·¨æ ·æœ¬å…±æ€§æ¨¡å¼ã€‚å…¶æ ¸å¿ƒæ´è§æ˜¯ï¼šå¤šæ ·æ€§æœ¬è´¨æ˜¯æ³›åŒ–èƒ½åŠ›ï¼Œè€Œéç®€å•æ£€ç´¢æˆ–éšæœºæ‰°åŠ¨ï¼›ParamMemé€šè¿‡å‚æ•°åŒ–ç¼–ç å®ç°æ¨¡å¼æ’å€¼ä¸å¤–æ¨ï¼Œå…¼å…·é«˜æ•ˆæ€§ã€å¯è¿ç§»æ€§ä¸è‡ªæŒæ€§ï¼Œä¸ºæ„å»ºå¯æŒç»­è¿›åŒ–çš„è¯­è¨€ä»£ç†æä¾›äº†æ–°åŸºç¡€è®¾æ–½ã€‚\n    "
    },
    {
        "title": "RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format",
        "authors": [
            "Zhehao Huang",
            "Yuhang Liu",
            "Baijiong Lin",
            "Yixin Lou",
            "Zhengbao He",
            "Hanling Tian",
            "Tao Li",
            "Xiaolin Huang"
        ],
        "categories": [
            "cs.LG",
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22538v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22538v1",
        "summary": "Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that naive merges are fragile because they overlook the output format mismatch between LRMs (with explicit thinking and response segments) and ITMs (answers-only). We introduce RAIN-Merging (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM's structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning & general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agent settings.",
        "tag": "æ¨¡å‹åˆå¹¶",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22538v1ã€æ¨¡å‹åˆå¹¶-ä¸Šæµ·äº¤é€šå¤§å­¦ã€‘RAIN-Merging_ A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format.pdf",
        "institution_status": "keep",
        "institution": "ä¸Šæµ·äº¤é€šå¤§å­¦ã€é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰",
        "first_institution": "ä¸Šæµ·äº¤é€šå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šRAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format\nğŸŒæ¥æºï¼šarXiv, 2602.22538v1\n\nç¬”è®°æ ‡é¢˜ï¼šRAIN-Mergingæå‡æŒ‡ä»¤éµå¾ª  \n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸ç ´åå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰ç»“æ„åŒ–æ€ç»´æ ¼å¼å’Œæ¨ç†èƒ½åŠ›çš„å‰æä¸‹ï¼Œæœ‰æ•ˆå¢å¼ºå…¶å¯¹å¤æ‚æŒ‡ä»¤çš„éµå¾ªèƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºRAIN-Mergingâ€”â€”ä¸€ç§æ— éœ€æ¢¯åº¦ã€åˆ†ä¸¤é˜¶æ®µçš„æ¨¡å‹èåˆæ–¹æ³•ï¼Œé¦–æ¬¡å®ç°æŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¸æ˜¾å¼æ€ç»´æ ¼å¼çš„ååŒä¿ç•™ä¸å¢å¼ºã€‚  \n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸åˆ†æLRMä¸æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼ˆITMï¼‰ä»»åŠ¡å‘é‡åœ¨å…³é”®æ¨¡å—çš„ä¸»å­ç©ºé—´è¿‘ä¹æ­£äº¤ï¼Œä¸ºè½»é‡èåˆæä¾›ç†è®ºä¾æ®ã€‚  \nğŸ”¸å‘ç°ç›´æ¥èåˆä¼šç ´åLRMç‰¹æœ‰çš„â€œ<think>â€¦</think>â€æ€ç»´åˆ†æ®µè¾“å‡ºç»“æ„ï¼Œå¯¼è‡´æ ¼å¼é”™ä¹±ä¸çº¦æŸè¿åã€‚  \nğŸ”¸ç¬¬ä¸€é˜¶æ®µï¼šåœ¨å°è§„æ¨¡æ¨ç†æ ¡å‡†é›†ä¸Šï¼Œå°†ITMä»»åŠ¡å‘é‡æŠ•å½±è‡³æ€ç»´ç‰¹æ®Štokenå‰å‘ç‰¹å¾çš„é›¶ç©ºé—´ï¼Œå¼ºåˆ¶ä¿æŒæ€ç»´æ®µåˆ†å¸ƒä¸å˜ã€‚  \nğŸ”¸ç¬¬äºŒé˜¶æ®µï¼šåœ¨å°è§„æ¨¡æŒ‡ä»¤æ ¡å‡†é›†ä¸Šï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶é‡åŒ–å„æ¨¡å—å¯¹æŒ‡ä»¤æ®µçš„å…³æ³¨åº¦ï¼ˆalignmentï¼‰ä¸æ³„éœ²åº¦ï¼ˆleakageï¼‰ï¼Œæ¨å¯¼å‡ºå±‚/å¤´è‡ªé€‚åº”ç¼©æ”¾ç³»æ•°ã€‚  \nğŸ”¸é€šè¿‡äºŒé˜¶æ³°å‹’è¿‘ä¼¼ä¸å¯¹è§’Hessianä¼°è®¡ï¼Œåœ¨çº¯å‰å‘ä¼ æ’­ä¸‹é«˜æ•ˆæ±‚è§£æœ€ä¼˜åˆå¹¶ç³»æ•°ï¼Œå…¨ç¨‹æ— éœ€åå‘ä¼ æ’­ã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸RAIN-Mergingåœ¨4ä¸ªæŒ‡ä»¤éµå¾ªåŸºå‡†ä¸Šå¹³å‡æå‡+3.99%ï¼ŒåŒæ—¶åœ¨9ä¸ªæ¨ç†ä¸é€šç”¨èƒ½åŠ›åŸºå‡†ä¸Šå¹³å‡æå‡+4.56%ï¼Œæ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šå»é™¤ç¬¬ä¸€é˜¶æ®µä¼šå¯¼è‡´æ¨ç†èƒ½åŠ›ä¸‹é™2.47%ï¼Œå»é™¤ç¬¬äºŒé˜¶æ®µåˆ™æŒ‡ä»¤éµå¾ªæ€§èƒ½é™ä½1.53%ï¼Œä¸¤é˜¶æ®µäº’è¡¥ä¸”ç¼ºä¸€ä¸å¯ã€‚  \nğŸ”¸é›¶ç©ºé—´æŠ•å½±ä½¿æ€ç»´æ®µKLæ•£åº¦ä»0.1224é™è‡³0.0065ï¼Œ</think>ç¼ºå¤±ç‡ä»6.4%é™è‡³0%ï¼Œç¡®ä¿è¯æ€æ ¼å¼å®Œæ•´æ€§ã€‚  \nğŸ”¸æŒ‡ä»¤æ³¨æ„åŠ›åˆ†æ•°åœ¨å„å±‚å‡æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨æµ…å±‚æ³¨æ„åŠ›å¤´ä¸­ç³»æ•°è¾¾ä¸Šé™ï¼ŒéªŒè¯äº†æ¨¡å—å·®å¼‚åŒ–å“åº”å‡è®¾ã€‚  \nğŸ”¸åœ¨ALFWorldä¸WebShopç­‰ä»£ç†åœºæ™¯ä¸­ï¼Œæ€§èƒ½è¶…è¶ŠåŸå§‹LRMä¸ITMï¼Œè¯æ˜å…¶åœ¨å¤šè½®äº¤äº’ä»»åŠ¡ä¸­çš„å®ç”¨ä»·å€¼ã€‚  \n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†â€œæ€ç»´æ ¼å¼â€å»ºæ¨¡ä¸ºå¯çº¦æŸçš„å‰å‘ç‰¹å¾å­ç©ºé—´ï¼Œå¹¶ä»¥é›¶ç©ºé—´æŠ•å½±å®ç°ç»“æ„å¼ºä¿ï¼›åŒæ—¶é¦–åˆ›æŒ‡ä»¤æ³¨æ„åŠ›å¼•å¯¼çš„ç»†ç²’åº¦åˆå¹¶ç³»æ•°å­¦ä¹ èŒƒå¼ï¼Œå°†æŠ½è±¡çš„æŒ‡ä»¤éµå¾ªå…·è±¡ä¸ºå¯æµ‹é‡ã€å¯ä¼˜åŒ–çš„æ³¨æ„åŠ›è¡Œä¸ºã€‚å…¶æ ¸å¿ƒæ´è§åœ¨äºï¼šæŒ‡ä»¤éµå¾ªä¸æ˜¯è¦†ç›–æ¨ç†ï¼Œè€Œæ˜¯ç²¾å‡†è°ƒæ§æ¨ç†è¿‡ç¨‹ä¸­çš„æŒ‡ä»¤æ„ŸçŸ¥é€šè·¯â€”â€”è¿™ä¸€æ€æƒ³ä¸ºåç»­å¯æ§æ¨ç†ã€å¯ä¿¡ä»£ç†ç³»ç»Ÿæä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue",
        "authors": [
            "Ning Gao",
            "Wei Zhang",
            "Yuqin Dai",
            "Ling Shi",
            "Ziyin Wang",
            "Yujie Wang",
            "Wei He",
            "Jinpeng Wang",
            "Chaozheng Wang"
        ],
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22697v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22697v1",
        "summary": "The rapid evolution of Large Language Models (LLMs) has accelerated the transition from conversational chatbots to general agents. However, effectively balancing empathetic communication with budget-aware decision-making remains an open challenge. Since existing methods fail to capture these complex strategic trade-offs, we propose InteractCS-RL, a framework that reframes task-oriented dialogue as a multi-granularity reinforcement learning process. Specifically, we first establish a User-centric Interaction Framework to provide a high-fidelity training gym, enabling agents to dynamically explore diverse strategies with persona-driven users. Then, we introduce Cost-aware Multi-turn Policy Optimization (CMPO) with a hybrid advantage estimation strategy. By integrating generative process credits and employing a PID-Lagrangian cost controller, CMPO effectively guides the policy to explore Pareto boundary between user reward and global cost constraints. Extensive experiments on customized real business scenarios demonstrate that InteractCS-RL significantly outperform other baselines across three evaluation dimensions. Further evaluation on tool-agent-user interaction benchmarks verify InteractCS-RL robustness across diverse domains.",
        "tag": "å¼ºåŒ–å­¦ä¹ ",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22697v1ã€å¼ºåŒ–å­¦ä¹ -ç¾å›¢ã€‘Reinforcing Real-world Service Agents_ Balancing Utility and Cost in Task-oriented Dialogue.pdf",
        "institution_status": "keep",
        "institution": "ç¾å›¢",
        "first_institution": "ç¾å›¢",
        "institution_category": "å›½å†…å·¥ä¸šç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šReinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue\nğŸŒæ¥æºï¼šarXiv, 2602.22697v1\n\nç¬”è®°æ ‡é¢˜ï¼šå¤šç²’åº¦æˆæœ¬æ„ŸçŸ¥å¯¹è¯å¼ºåŒ–å­¦ä¹ \n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨çœŸå®å®¢æœå¯¹è¯ä¸­åŒæ—¶ä¼˜åŒ–ç”¨æˆ·æ»¡æ„åº¦ä¸è¿è¥æˆæœ¬ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºInteractCS-RLæ¡†æ¶ï¼Œé¦–æ¬¡å°†ä»»åŠ¡å‹å¯¹è¯å»ºæ¨¡ä¸ºå…¼é¡¾ä¼šè¯è¿‡ç¨‹è´¨é‡ã€ç»ˆç«¯ç»“æœæ•ˆç”¨ä¸å…¨å±€æˆæœ¬çº¦æŸçš„å¤šç²’åº¦å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œå¹¶å®ç°å¸•ç´¯æ‰˜æœ€ä¼˜å¹³è¡¡ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºç”¨æˆ·ä¸­å¿ƒäº¤äº’æ¡†æ¶ï¼Œé€šè¿‡å†…åœ¨äººæ ¼ç‰¹è´¨ï¼ˆæ²Ÿé€šé£æ ¼ã€ä¿¡æ¯æŠ«éœ²å€¾å‘ã€é—®é¢˜è§£å†³ç­–ç•¥ã€æƒ…æ„Ÿç¨³å®šæ€§ï¼‰ä¸å¤–åœ¨éœ€æ±‚ç±»å‹ï¼ˆåˆšæ€§è¿½å¿ã€åé¦ˆå¯¼å‘ç­‰ï¼‰åŒç»´åº¦å»ºæ¨¡ï¼Œé©±åŠ¨é«˜ä¿çœŸåŠ¨æ€ç”¨æˆ·æ¨¡æ‹Ÿã€‚  \nğŸ”¸è®¾è®¡æˆæœ¬æ„ŸçŸ¥å¤šè½®ç­–ç•¥ä¼˜åŒ–ï¼ˆCMPOï¼‰ï¼Œèåˆä¸‰å±‚ä¼˜åŠ¿ä¼°è®¡ï¼šä¼šè¯çº§ç»“æœæ•ˆç”¨ï¼ˆç”¨æˆ·æ»¡æ„åº¦ï¼‰ã€å›åˆçº§ç”Ÿæˆè¿‡ç¨‹ä¿¡ç”¨ï¼ˆåŸºäºåŸåˆ™çš„GenRMè¯„åˆ†ï¼‰ã€PIDè°ƒæ§çš„æ‹‰æ ¼æœ—æ—¥æˆæœ¬ç½šé¡¹ã€‚  \nğŸ”¸é‡‡ç”¨åˆ†å±‚å¥–åŠ±æœºåˆ¶ï¼Œå°†ä¸šåŠ¡è§„èŒƒè½¬åŒ–ä¸ºå¯å­¦ä¹ çš„ç»†ç²’åº¦è¯„ä¼°åŸåˆ™ï¼ˆå¦‚é˜¶æ®µé€‚é…æ€§ã€äººæ ¼å¼•å¯¼æ€§ï¼‰ï¼Œç”±å¤§æ¨¡å‹ä½œä¸ºè£åˆ¤è¿›è¡Œç¦»æ•£åŒ–æ‰“åˆ†å¹¶åŠ æƒèšåˆã€‚  \nğŸ”¸å¼•å…¥PIDæ§åˆ¶å™¨åŠ¨æ€è°ƒèŠ‚æ‹‰æ ¼æœ—æ—¥ä¹˜å­Î»ï¼Œå®æ—¶æ ¡æ­£ç¬æ—¶è¿è§„ä¸é•¿æœŸåå·®ï¼Œä½¿æˆæœ¬çº¦æŸç¨³å®šæ”¶æ•›äºé¢„è®¾é˜ˆå€¼ï¼ˆå¦‚ä»£é‡‘åˆ¸å‘æ”¾ç‡â‰¤30%ï¼‰ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨é£Ÿå“é…é€çº çº·åœºæ™¯ä¸­ï¼ŒInteractCS-RLæ˜¾è‘—è¶…è¶ŠSFTåŠPPO/GRPOç­‰åŸºçº¿ï¼šç”¨æˆ·æ»¡æ„åº¦æå‡è¶…40%ï¼Œå®Œæˆç‡è¾¾100%ï¼Œä»£é‡‘åˆ¸ç‡ç²¾å‡†æ§åˆ¶åœ¨30.8%ï¼ŒéªŒè¯æˆæœ¬å¯æ§æ€§ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šç§»é™¤PIDæ§åˆ¶å¯¼è‡´ä»£é‡‘åˆ¸ç‡éœ‡è¡è‡³34.5%ï¼›å›ºå®šÎ»åˆ™è¿‡åº¦æŠ‘åˆ¶è¡Œä¸ºï¼ˆ24.6%ï¼‰ï¼ŒæŸå®³æ»¡æ„åº¦ï¼›ä»…ä¾èµ–è¿‡ç¨‹å¥–åŠ±ä¼šå¯¼è‡´æˆæœ¬å¤±æ§ï¼ˆ41.2%ï¼‰ã€‚  \nğŸ”¸è·¨åŸŸæµ‹è¯•Ï„2-benchæ˜¾ç¤ºï¼Œå…¶Pass@1å¹³å‡æå‡5.6%ï¼ŒDBç‡ä¸åŠ¨ä½œå¥–åŠ±åŒæ­¥ä¸Šå‡ï¼Œè¯æ˜æˆæœ¬æ„è¯†è®­ç»ƒèƒ½æ³›åŒ–è‡³é›¶å”®ã€èˆªç©ºç­‰æ–°é¢†åŸŸï¼Œå¢å¼ºå·¥å…·è°ƒç”¨ä¸é€»è¾‘ä¸€è‡´æ€§ã€‚  \nğŸ”¸æ¡ˆä¾‹åˆ†ææ­ç¤ºï¼šç›¸æ¯”SFTæ¨¡å‹é™·å…¥é‡å¤è‡´æ­‰æˆ–æœºæ¢°è¿½é—®ï¼ŒInteractCS-RLèƒ½ä¾æ®ç”¨æˆ·äººæ ¼åŠ¨æ€åˆ‡æ¢ç­–ç•¥â€”â€”å¯¹ä¸åˆä½œç”¨æˆ·åšå®ˆæµç¨‹ã€å¯¹çµæ´»ç”¨æˆ·ä¸»åŠ¨è¡¥å¿ã€å¯¹åˆä½œç”¨æˆ·é«˜æ•ˆé—­ç¯ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡åˆ›æ–°æ€§åœ¨äºçªç ´ä¼ ç»Ÿå¯¹è¯ç³»ç»Ÿå•ç›®æ ‡ä¼˜åŒ–èŒƒå¼ï¼Œå°†â€œå…±æƒ…è¡¨è¾¾â€ä¸â€œé¢„ç®—çºªå¾‹â€ç»Ÿä¸€ä¸ºå¯ååŒå­¦ä¹ çš„å¤šç²’åº¦ç›®æ ‡ï¼›å…¶äººæ ¼é©±åŠ¨çš„ä»¿çœŸç¯å¢ƒä¸PID-Lagrangianæˆæœ¬è°ƒæ§æœºåˆ¶ï¼Œä¸ºçœŸå®æœåŠ¡åœºæ™¯ä¸­LLMä»£ç†çš„ç¨³å¥éƒ¨ç½²æä¾›äº†å¯å¤ç°ã€å¯è§£é‡Šã€å¯çº¦æŸçš„æŠ€æœ¯è·¯å¾„ã€‚\n    "
    },
    {
        "title": "Rejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference",
        "authors": [
            "Yushi Ye",
            "Feng Hong",
            "Huangjie Zheng",
            "Xu Chen",
            "Zhiyong Chen",
            "Yanfeng Wang",
            "Jiangchao Yao"
        ],
        "categories": [
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22868v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22868v1",
        "summary": "Diffusion Large Language Models (DLLMs) promise fast non-autoregressive inference but suffer a severe quality-speed trade-off in parallel decoding. This stems from the ''combinatorial contradiction'' phenomenon, where parallel tokens form semantically inconsistent combinations. We address this by integrating continuous representations into the discrete decoding process, as they preserve rich inter-position dependency. We propose ReMix (Rejection Mixing), a framework that introduces a novel Continuous Mixing State as an intermediate between the initial masked state and the final decoded token state. This intermediate state allows a token's representation to be iteratively refined in a continuous space, resolving mutual conflicts with other tokens before collapsing into a final discrete sample. Furthermore, a rejection rule reverts uncertain representations from the continuous state back to the masked state for reprocessing, ensuring stability and preventing error propagation. ReMix thus mitigates combinatorial contradictions by enabling continuous-space refinement during discrete diffusion decoding. Extensive experiments demonstrate that ReMix, as a training-free method, achieves a $2-8 \\times$ inference speedup without any quality degradation.",
        "tag": "é«˜æ•ˆæ¨ç†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22868v1ã€é«˜æ•ˆæ¨ç†-ä¸Šæµ·äº¤é€šå¤§å­¦ã€‘Rejection Mixing_ Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference.pdf",
        "institution_status": "keep",
        "institution": "ä¸Šæµ·äº¤é€šå¤§å­¦",
        "first_institution": "ä¸Šæµ·äº¤é€šå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šRejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference\nğŸŒæ¥æºï¼šarXiv, 2602.22868v1\n\nç¬”è®°æ ‡é¢˜ï¼šè¿ç»­æ··åˆç¼“è§£ç»„åˆçŸ›ç›¾\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸ç‰ºç‰²ç”Ÿæˆè´¨é‡çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆDLLMï¼‰çš„å¹¶è¡Œè§£ç é€Ÿåº¦ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºReMixæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è¿ç»­æ··åˆçŠ¶æ€ä¸æ‹’ç»æœºåˆ¶ï¼Œåœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå°†DLLMæ¨ç†é€Ÿåº¦æå‡2â€“8å€ä¸”ä¸é™ä½ç”šè‡³æå‡è¾“å‡ºè´¨é‡ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è¯†åˆ«æ ¸å¿ƒç“¶é¢ˆä¸ºâ€œç»„åˆçŸ›ç›¾â€â€”â€”å¹¶è¡Œé‡‡æ ·æ—¶å„ä½ç½®ç‹¬ç«‹ç”Ÿæˆç¦»æ•£tokenï¼Œå¯¼è‡´è¯­ä¹‰å†²çªï¼ˆå¦‚â€œFull Pairâ€è€Œéâ€œFull Houseâ€ï¼‰ã€‚  \nğŸ”¸è®¾è®¡ä¸‰æ€åŠ¨æ€è§£ç æµç¨‹ï¼šä»æ©ç æ€ï¼ˆMï¼‰ç»è¿ç»­æ··åˆæ€ï¼ˆCï¼‰æœ€ç»ˆåç¼©è‡³ç¦»æ•£è¯å…ƒæ€ï¼ˆTï¼‰ï¼Œä½¿tokenè¡¨å¾å¯åœ¨è¿ç»­ç©ºé—´ä¸­è¿­ä»£ååŒä¼˜åŒ–ã€‚  \nğŸ”¸æå‡ºæ··åˆè§„åˆ™ï¼ˆMâ†’CâŸ³ï¼‰ï¼šå°†è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒçº¿æ€§åŠ æƒæ˜ å°„ä¸ºåµŒå…¥æ›´æ–°ï¼Œä¿ç•™è¯­ä¹‰ä¾èµ–ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”top-pç­–ç•¥ç¨³å®šä½ç½®ä¿¡åº¦ä½ç½®ã€‚  \nğŸ”¸å¼•å…¥æ‹’ç»è§„åˆ™ï¼ˆCâ†’Mï¼‰ï¼šå½“è¿ç»­æ­¥é—´è¾“å‡ºåˆ†å¸ƒJSæ•£åº¦è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œå°†ä¸ç¨³å®šä½ç½®é‡ç½®ä¸º[MASK]ï¼Œé˜²æ­¢é”™è¯¯ä¼ æ’­ï¼Œå¢å¼ºé²æ£’æ€§ã€‚  \nğŸ”¸å…¨ç¨‹æ— éœ€æ¨¡å‹å¾®è°ƒæˆ–é¢å¤–è®­ç»ƒï¼Œä»…ä¿®æ”¹è§£ç é€»è¾‘ï¼Œå…¼å®¹ç°æœ‰DLLMæ¶æ„ï¼ˆå¦‚LLaDAã€MMaDAï¼‰ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨8ä¸ªè¯­è¨€ä»»åŠ¡ï¼ˆGSM8Kã€ARC-Cç­‰ï¼‰ä¸Šï¼ŒReMixå¹³å‡å‡å°‘150â€“205æ­¥è§£ç ï¼Œå®ç°2.4Ã—â€“4.6Ã—ç«¯åˆ°ç«¯åŠ é€Ÿï¼ŒåŒæ—¶å‡†ç¡®ç‡å…¨é¢æå‡ï¼ˆæœ€é«˜+14.05%ï¼‰ã€‚  \nğŸ”¸åœ¨6ä¸ªå¤šæ¨¡æ€ä»»åŠ¡ï¼ˆFlickr30kã€MathVistaç­‰ï¼‰ä¸­åŒæ ·æœ‰æ•ˆï¼Œé€Ÿåº¦æå‡è¾¾3.75Ã—â€“7.52Ã—ï¼ŒCaptioningä¸å›¾è¡¨ç†è§£ä»»åŠ¡å¢ç›Šæœ€æ˜¾è‘—ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šç§»é™¤è¿ç»­æ··åˆæ€ä¼šé€€åŒ–ä¸ºæ™®é€šç½®ä¿¡åº¦è§£ç ï¼Œæ€§èƒ½æ˜æ˜¾ä¸‹é™ï¼›æ··åˆç³»æ•°Î²ä¸æ‹’ç»é˜ˆå€¼Ï„_rejå­˜åœ¨æœ€ä¼˜åŒºé—´ï¼Œè¿‡é«˜æˆ–è¿‡ä½å‡æŸå®³ç²¾åº¦ã€‚  \nğŸ”¸æ¡ˆä¾‹åˆ†ææ˜¾ç¤ºï¼ŒReMixèƒ½åœ¨æ›´å°‘æ­¥æ•°å†…é¿å…æ—©æœŸé”™è¯¯ï¼ˆå¦‚GSM8Kä¸­æ­£ç¡®ç´¯ç§¯è®¡ç®—è‡³23ï¼‰ï¼Œè€ŒåŸºçº¿å› é”™è¯¯tokenå›ºåŒ–å¯¼è‡´åç»­æ¨ç†å´©æºƒã€‚  \nğŸ”¸è®¡ç®—å¼€é”€æä½ï¼ŒReMixä¸“å±æ“ä½œä»…å æ€»è€—æ—¶9.12%ï¼Œæ— é¢å¤–å‰å‘ä¼ æ’­ï¼Œé«˜æ•ˆé€‚é…å®é™…éƒ¨ç½²ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†è¿ç»­è¡¨å¾å¼•å…¥ç¦»æ•£æ‰©æ•£è§£ç é—­ç¯ï¼Œä»¥è½»é‡çº§çŠ¶æ€æœºè®¾è®¡ç ´è§£â€œç»„åˆçŸ›ç›¾â€è¿™ä¸€æ ¹æœ¬æ€§éš¾é¢˜ï¼›å…¶æ‹’ç»æœºåˆ¶å·§å¦™å¹³è¡¡äº†è¿ç»­ä¼˜åŒ–çš„çµæ´»æ€§ä¸ç¦»æ•£è®­ç»ƒçš„å…ˆéªŒçº¦æŸï¼Œä½“ç°äº†å¯¹æ‰©æ•£èŒƒå¼æœ¬è´¨çš„æ·±åˆ»ç†è§£ã€‚\n    "
    },
    {
        "title": "Replacing Multi-Step Assembly of Data Preparation Pipelines with One-Step LLM Pipeline Generation for Table QA",
        "authors": [
            "Fengyu Li",
            "Junhao Zhu",
            "Kaishi Song",
            "Lu Chen",
            "Zhongming Yao",
            "Tianyi Li",
            "Christian S. Jensen"
        ],
        "categories": [
            "cs.DB",
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22721v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22721v1",
        "summary": "Table Question Answering (TQA) aims to answer natural language questions over structured tables. Large Language Models (LLMs) enable promising solutions to this problem, with operator-centric solutions that generate table manipulation pipelines in a multi-step manner offering state-of-the-art performance. However, these solutions rely on multiple LLM calls, resulting in prohibitive latencies and computational costs.\n  We propose Operation-R1, the first framework that trains lightweight LLMs (e.g., Qwen-4B/1.7B) via a novel variant of reinforcement learning with verifiable rewards to produce high-quality data-preparation pipelines for TQA in a single inference step. To train such an LLM, we first introduce a self-supervised rewarding mechanism to automatically obtain fine-grained pipeline-wise supervision signals for LLM training. We also propose variance-aware group resampling to mitigate training instability. To further enhance robustness of pipeline generation, we develop two complementary mechanisms: operation merge, which filters spurious operations through multi-candidate consensus, and adaptive rollback, which offers runtime protection against information loss in data transformation. Experiments on two benchmark datasets show that, with the same LLM backbone, Operation-R1 achieves average absolute accuracy gains of 9.55 and 6.08 percentage points over multi-step preparation baselines, with 79\\% table compression and a 2.2$\\times$ reduction in monetary cost.",
        "tag": "å¼ºåŒ–å­¦ä¹ ",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22721v1ã€å¼ºåŒ–å­¦ä¹ -æµ™æ±Ÿå¤§å­¦ã€‘Replacing Multi-Step Assembly of Data Preparation Pipelines with One-Step LLM Pipeline Generation for Table QA.pdf",
        "institution_status": "keep",
        "institution": "æµ™æ±Ÿå¤§å­¦ã€ä¸œåŒ—å¤§å­¦ã€å¥¥å°”å ¡å¤§å­¦",
        "first_institution": "æµ™æ±Ÿå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šReplacing Multi-Step Assembly of Data Preparation Pipelines with One-Step LLM Pipeline Generation for Table QA\nğŸŒæ¥æºï¼šarXiv, 2602.22721v1\n\nç¬”è®°æ ‡é¢˜ï¼šå•æ­¥ç”Ÿæˆè¡¨æ ¼é¢„å¤„ç†ç®¡é“\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¿è¯è¡¨æ ¼é—®ç­”ï¼ˆTQAï¼‰ç²¾åº¦çš„å‰æä¸‹ï¼Œå°†ä¼ ç»Ÿå¤šæ­¥è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆæ•°æ®å‡†å¤‡ç®¡é“çš„æ–¹å¼ï¼Œæ›¿æ¢ä¸ºä»…éœ€ä¸€æ¬¡è½»é‡çº§æ¨¡å‹æ¨ç†çš„é«˜æ•ˆæ–¹æ¡ˆï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºäº†Operation-R1æ¡†æ¶ï¼Œé¦–æ¬¡åˆ©ç”¨å¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è®­ç»ƒè½»é‡çº§LLMï¼ˆå¦‚Qwen-1.7B/4Bï¼‰ï¼Œå®ç°é¢å‘TQAçš„é«˜è´¨é‡æ•°æ®å‡†å¤‡ç®¡é“çš„å•æ­¥ç”Ÿæˆã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æå‡ºORPOç®—æ³•â€”â€”ä¸€ç§æ“ä½œç²’åº¦çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œå°†æ•°æ®å‡†å¤‡å»ºæ¨¡ä¸ºåºåˆ—åŒ–æ“ä½œç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å¼•å…¥ç»†ç²’åº¦å¥–åŠ±æœºåˆ¶ã€‚  \nğŸ”¸è®¾è®¡è‡ªç›‘ç£ç®¡é“å¥–åŠ±æœºåˆ¶ï¼šåŸºäºâ€œå•å…ƒæ ¼èšç„¦QAâ€å‡è®¾ï¼Œé€æ“ä½œéªŒè¯æ˜¯å¦ä¿ç•™ç­”æ¡ˆå•å…ƒæ ¼ï¼ˆæ­£ç¡®æ€§å¥–åŠ±ï¼‰å¹¶è¡¡é‡å‹ç¼©ç‡ï¼ˆæ•ˆç‡å¥–åŠ±ï¼‰ï¼Œé¿å…ä¾èµ–äººå·¥æ ‡æ³¨ç®¡é“ã€‚  \nğŸ”¸æå‡ºæ–¹å·®æ„ŸçŸ¥çš„ç»„é‡é‡‡æ ·ï¼ˆVGRï¼‰ç­–ç•¥ï¼šåŠ¨æ€è¿‡æ»¤ä½æ–¹å·®æˆ–ä½è´¨é‡å“åº”ç»„ï¼Œç¼“è§£ç»†ç²’åº¦å¥–åŠ±ä¸‹RLè®­ç»ƒçš„ä¸ç¨³å®šæ€§ä¸ä¿¡å·åç¼©é—®é¢˜ã€‚  \nğŸ”¸æ„å»ºè¿è¡Œæ—¶é²æ£’æœºåˆ¶ï¼šOperation Mergeé€šè¿‡æ“ä½œå­—å…¸æ ‘å¯¹å¤šå€™é€‰ç®¡é“æŠ•ç¥¨èåˆï¼Œè¿‡æ»¤ä¸ç¨³å®šæ“ä½œï¼›Adaptive Rollbackåœ¨QAå¤±è´¥æ—¶é€çº§å›é€€è‡³æ›´åŸå§‹è¡¨æ ¼ï¼Œé˜²æ­¢ä¿¡æ¯ä¸¢å¤±ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨WikiTQå’ŒTabFactä¸Šï¼ŒOperation-R1-4Bç›¸æ¯”å¤šæ­¥åŸºçº¿å¹³å‡æå‡9.55%å’Œ6.08%ç»å¯¹å‡†ç¡®ç‡ï¼ŒåŒæ—¶å®ç°79%è¡¨æ ¼å‹ç¼©ä¸2.2å€æˆæœ¬ä¸‹é™ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šç§»é™¤ORPOè®­ç»ƒã€Operation Mergeæˆ–Adaptive Rollbackåˆ†åˆ«å¯¼è‡´5.77ã€7.36ã€5.02ä¸ªç™¾åˆ†ç‚¹æ€§èƒ½ä¸‹é™ï¼Œä¸‰è€…å‡ä¸å¯æˆ–ç¼ºã€‚  \nğŸ”¸VGRç­–ç•¥æ˜¾è‘—æå‡è®­ç»ƒç¨³å®šæ€§ï¼šå»é™¤åæ—©æœŸå¥–åŠ±éª¤é™ä¸”æ¢å¤ç¼“æ…¢ï¼Œè€Œå®Œæ•´VGRä½¿å¥–åŠ±æ›²çº¿æ›´å¹³æ»‘ã€æ”¶æ•›æ›´é«˜ã€‚  \nğŸ”¸Operatoråˆ†å¸ƒåˆ†æè¡¨æ˜ï¼šFilterä¸Selectæ“ä½œå¸¦æ¥æœ€å¤§å¢ç›Šï¼ˆè¶…17%ï¼‰ï¼Œå°è¯å…¶æœ‰æ•ˆè§£å†³â€œå¤§æµ·æé’ˆâ€é—®é¢˜ï¼›é•¿æ“ä½œé“¾å¯¹åº”å¤æ‚é—®é¢˜ï¼ŒOperation-R1å¢ç›Šéšé“¾é•¿å¢åŠ è€Œæ‰©å¤§ï¼Œä½“ç°å¼ºè¯­ä¹‰å¯¹é½èƒ½åŠ›ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†RLVRä»ç«¯åˆ°ç«¯é—®ç­”è¿ç§»è‡³å‰ç½®æ“ä½œç”Ÿæˆç¯èŠ‚ï¼Œä»¥â€œå¯éªŒè¯å•å…ƒæ ¼ä¿ç•™â€æ›¿ä»£æ¨¡ç³Šçš„æœ€ç»ˆç­”æ¡ˆè¯„ä¼°ï¼Œå®ç°äº†ç›‘ç£ä¿¡å·çš„ç»†ç²’åº¦ã€æ— æ ‡æ³¨ã€ä½æˆæœ¬ï¼›å…¶â€œæ“ä½œç©ºé—´çº¦æŸ+å•æ­¥ç”Ÿæˆ+è½»é‡æ¨¡å‹â€çš„èŒƒå¼ï¼Œä¸ºç»“æ„åŒ–æ•°æ®ç†è§£æä¾›äº†æ›´é«˜æ•ˆã€å¯éƒ¨ç½²çš„æ–°è·¯å¾„ã€‚\n    "
    },
    {
        "title": "S2O: Early Stopping for Sparse Attention via Online Permutation",
        "authors": [
            "Yu Zhang",
            "Songwei Liu",
            "Chenqian Yan",
            "Sheng Lin",
            "Beichen Ning",
            "Fangmin Chen",
            "Xing Wang"
        ],
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22575v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22575v1",
        "summary": "Attention scales quadratically with sequence length, fundamentally limiting long-context inference. Existing block-granularity sparsification can reduce latency, but coarse blocks impose an intrinsic sparsity ceiling, making further improvements difficult even with carefully engineered designs. We present S2O, which performs early stopping for sparse attention via online permutation. Inspired by virtual-to-physical address mapping in memory systems, S2O revisits and factorizes FlashAttention execution, enabling inference to load non-contiguous tokens rather than a contiguous span in the original order. Motivated by fine-grained structures in attention heatmaps, we transform explicit permutation into an online, index-guided, discrete loading policy; with extremely lightweight preprocessing and index-remapping overhead, it concentrates importance on a small set of high-priority blocks. Building on this importance-guided online permutation for loading, S2O further introduces an early-stopping rule: computation proceeds from high to low importance; once the current block score falls below a threshold, S2O terminates early and skips the remaining low-contribution blocks, thereby increasing effective sparsity and reducing computation under a controlled error budget.\n  As a result, S2O substantially raises the practical sparsity ceiling. On Llama-3.1-8B under a 128K context, S2O reduces single-operator MSE by 3.82$\\times$ at matched sparsity, and reduces prefill compute density by 3.31$\\times$ at matched MSE; meanwhile, it preserves end-to-end accuracy and achieves 7.51$\\times$ attention and 3.81$\\times$ end-to-end speedups.",
        "tag": "æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22575v1ã€æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–-å­—èŠ‚è·³åŠ¨ã€‘S2O_ Early Stopping for Sparse Attention via Online Permutation.pdf",
        "institution_status": "keep",
        "institution": "å­—èŠ‚è·³åŠ¨ã€å¦é—¨å¤§å­¦",
        "first_institution": "å­—èŠ‚è·³åŠ¨",
        "institution_category": "å›½å†…å·¥ä¸šç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šS2O: Early Stopping for Sparse Attention via Online Permutation\nğŸŒæ¥æºï¼šarXiv, 2602.22575v1\n\nç¬”è®°æ ‡é¢˜ï¼šåœ¨çº¿ç½®æ¢å®ç°ç¨€ç–æ³¨æ„åŠ›æ—©åœ\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•çªç ´å—ç²’åº¦ç¨€ç–æ³¨æ„åŠ›çš„å›ºæœ‰ç¨€ç–ä¸Šé™ï¼Œæå‡é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸‹ç¨€ç–æ³¨æ„åŠ›çš„ç²¾åº¦ä¸æ•ˆç‡ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºS2Oæ–¹æ³•ï¼Œé€šè¿‡è½»é‡çº§åœ¨çº¿ç½®æ¢ä¸é‡è¦æ€§é©±åŠ¨çš„æ—©åœæœºåˆ¶ï¼Œåœ¨ä¸ç‰©ç†é‡æ’å¼ é‡çš„å‰æä¸‹æ˜¾è‘—æå‡æœ‰æ•ˆç¨€ç–åº¦ï¼ŒåŒæ—¶æ§åˆ¶è¯¯å·®é¢„ç®—ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å—å†…å­˜è™šæ‹Ÿåœ°å€æ˜ å°„å¯å‘ï¼Œå°†FlashAttentionæ‰§è¡Œè§£è€¦ä¸ºé€»è¾‘ç´¢å¼•åŠ è½½ä¸ç‰©ç†å—è®¡ç®—ï¼Œå®ç°â€œéè¿ç»­ä½†é«˜æ•ˆâ€çš„tokenåŠ è½½ã€‚  \nğŸ”¸åŸºäºæ³¨æ„åŠ›çƒ­å›¾ä¸­æ™®éå­˜åœ¨çš„ç»†ç²’åº¦æ¡çº¹ç»“æ„ï¼ˆæ°´å¹³/å‚ç›´/æ–œå‘ï¼‰ï¼Œè®¾è®¡æ¡çº¹ç²’åº¦å‡å€¼æ± åŒ–ä¿¡å·ï¼Œåœ¨æ®µå†…å¯¹Qè¿›è¡Œè½»é‡æ’åºï¼ˆQpermï¼‰ï¼Œå¹¶ç”¨æ®µä»£è¡¨æŸ¥è¯¢å¯¹å†å²KVåšå…¨å±€å› æœå‰ç¼€æ’åºï¼ˆKVpermï¼‰ã€‚  \nğŸ”¸å¼•å…¥åæ ‡è°ƒåº¦çš„åœ¨çº¿ç½®æ¢åŠ è½½ç­–ç•¥ï¼šä»…ç»´æŠ¤ä¸¤ä¸ªè½»é‡ç´¢å¼•æ•°ç»„ï¼Œè¿è¡Œæ—¶é€šè¿‡gatheræ“ä½œæŒ‰ç½®æ¢é¡ºåºåŠ è½½Q/K/V tileï¼Œä¿æŒåŸå§‹å†…å­˜å¸ƒå±€ä¸FlashAttentionè®¡ç®—æµæ°´çº¿ä¸å˜ã€‚  \nğŸ”¸æå‡ºå•è°ƒå¢ç›Šæ—©åœè§„åˆ™ï¼šæŒ‰KVpermé¡ºåºé€å—å¤„ç†å†å²å‰ç¼€ï¼ŒåŠ¨æ€ç›‘æ§å½’ä¸€åŒ–è´¨é‡â„“çš„è¾¹é™…å¢é‡âˆ†â„“ï¼›å½“âˆ†â„“ä½äºé˜ˆå€¼Ï„Â·â„“æ—¶ç«‹å³ç»ˆæ­¢ï¼Œè·³è¿‡ä½è´¡çŒ®å—ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨128Kä¸Šä¸‹æ–‡Llama-3.1-8Bä¸Šï¼ŒS2Oåœ¨ç›¸åŒç¨€ç–ç‡ä¸‹å°†å•ç®—å­MSEé™ä½3.82å€ï¼Œæˆ–åœ¨ç›¸åŒMSEä¸‹å°†è®¡ç®—å¯†åº¦é™ä½3.31å€ã€‚  \nğŸ”¸å®ç°7.51Ã—æ³¨æ„åŠ›ç®—å­åŠ é€Ÿä¸3.81Ã—ç«¯åˆ°ç«¯é¢„å¡«å……åŠ é€Ÿï¼Œä¸”é¢„å¤„ç†å¼€é”€å æ¯”æå°ï¼ˆ<10%ï¼‰ï¼Œä¸»è¦æ¥è‡ªä¸€æ¬¡è½»é‡æ’åºã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¡¨æ˜ï¼Œæ—©åœé˜ˆå€¼Ï„æ˜¯ç²¾åº¦-é€Ÿåº¦æƒè¡¡çš„ä¸»å¯¼å› ç´ ï¼Œè€Œæ®µé•¿Så½±å“è¾ƒå°ï¼›å¯ç”¨Qç«¯æ®µå†…ç½®æ¢å¯è¿›ä¸€æ­¥æå‡æ—©åœæœ‰æ•ˆæ€§ã€‚  \nğŸ”¸çƒ­å›¾å¯è§†åŒ–è¯å®ï¼šç›¸æ¯”å±€éƒ¨ç½®æ¢ï¼ˆPBSï¼‰ç­‰æ–¹æ³•ï¼ŒS2Oèƒ½æ›´æœ‰æ•ˆåœ°å°†æ³¨æ„åŠ›è´¨é‡èšæ‹¢è‡³å·¦ä¸ŠåŒºåŸŸï¼Œæ˜¾è‘—å‡å°‘å—å†…å†—ä½™è®¡ç®—ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†ç³»ç»Ÿçº§æ€æƒ³ï¼ˆè™šæ‹Ÿåœ°å€æ˜ å°„ï¼‰è¿ç§»åˆ°æ³¨æ„åŠ›ä¼˜åŒ–ä¸­ï¼Œä»¥â€œç´¢å¼•å³è°ƒåº¦â€æ›¿ä»£â€œé‡æ’å³ä¼˜åŒ–â€ï¼Œè§„é¿äº†ç‰©ç†ç½®æ¢çš„é«˜å¼€é”€ï¼›å…¶æ¡çº¹æ„ŸçŸ¥çš„é‡è¦æ€§å»ºæ¨¡ä¸åœ¨çº¿æ—©åœæœºåˆ¶ååŒçªç ´äº†å—ç²’åº¦ç“¶é¢ˆï¼Œä¸ºé•¿ä¸Šä¸‹æ–‡ç¨€ç–åŒ–æä¾›äº†å…¼å…·ç†è®ºæ´å¯Ÿä¸å·¥ç¨‹è½åœ°çš„æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning",
        "authors": [
            "Amita Kamath",
            "Jack Hessel",
            "Khyathi Chandu",
            "Jena D. Hwang",
            "Kai-Wei Chang",
            "Ranjay Krishna"
        ],
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.23351v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23351v1",
        "summary": "The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., \"at the game today!\" is a more likely caption than \"a photo of 37 people standing behind a field\". We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.",
        "tag": "è®­ç»ƒæ•°æ®åå·®",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.23351v1ã€è®­ç»ƒæ•°æ®åå·®-åç››é¡¿å¤§å­¦ã€‘Scale Can't Overcome Pragmatics_ The Impact of Reporting Bias on Vision-Language Reasoning.pdf",
        "institution_status": "keep",
        "institution": "åç››é¡¿å¤§å­¦ã€åŠ å·å¤§å­¦æ´›æ‰çŸ¶åˆ†æ ¡ã€Samaya AIã€Mistral AIã€AllenAI",
        "first_institution": "åç››é¡¿å¤§å­¦",
        "institution_category": "å›½å¤–å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šScale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning\nğŸŒæ¥æºï¼šarXiv, 2602.23351v1\n\nç¬”è®°æ ‡é¢˜ï¼šæŠ¥å‘Šåå·®é˜»ç¢è§†è§‰è¯­è¨€æ¨ç†\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šä¸ºä»€ä¹ˆå½“å‰å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´ã€æ—¶é—´ã€å¦å®šå’Œè®¡æ•°ç­‰åŸºç¡€æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°è¿œé€Šäºäººç±»ï¼Œå³ä½¿æ•°æ®å’Œæ¨¡å‹è§„æ¨¡æŒç»­æ‰©å¤§ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šé¦–æ¬¡ç³»ç»Ÿæ­ç¤ºå¹¶éªŒè¯äº†â€œæŠ¥å‘Šåå·®â€â€”â€”äººç±»åœ¨æè¿°å›¾åƒæ—¶ç³»ç»Ÿæ€§çœç•¥éšå«æ¨ç†ä¿¡æ¯â€”â€”æ˜¯å¯¼è‡´VLMæ¨ç†èƒ½åŠ›ç¼ºå¤±çš„æ ¹æœ¬åŸå› ï¼Œå¹¶è¯æ˜å•çº¯æ‰©å¤§è§„æ¨¡æ— æ³•å…‹æœè¯¥åå·®ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸åŸºäºè¯­ç”¨å­¦ï¼ˆGriceä¼šè¯å‡†åˆ™ï¼‰å’Œè®¤çŸ¥è¯­è¨€å­¦ç†è®ºï¼Œæå‡ºå››ç±»è¢«æ™®éçœç•¥çš„æ¨ç†ç±»å‹ï¼šç©ºé—´å…³ç³»ï¼ˆå¦‚â€œå·¦/å³â€ï¼‰ã€æ—¶é—´é¡ºåºï¼ˆå¦‚â€œä¹‹å‰/ä¹‹åâ€ï¼‰ã€å¦å®šè¡¨è¾¾ï¼ˆå¦‚â€œæ²¡æœ‰â€¦â€¦â€ï¼‰å’Œç²¾ç¡®è®¡æ•°ã€‚  \nğŸ”¸é€šè¿‡å…³é”®è¯æ£€ç´¢ä¸äººå·¥æ ¡éªŒï¼Œåœ¨LAIONã€LLaVA-1.5ã€Molmoç­‰ä¸‰å¤§å¼€æºå¤šæ¨¡æ€æ•°æ®é›†ä¸Šå®è¯å››ç±»æ¨ç†è¡¨è¾¾çš„å‡ºç°ç‡æä½ï¼ˆå¦‚LAIONä¸­ç©ºé—´å…³ç³»ä»…çº¦0.1%ï¼‰ï¼Œè¯å®æŠ¥å‘Šåå·®æ™®éå­˜åœ¨ã€‚  \nğŸ”¸æ„å»ºå››ä¸ªé’ˆå¯¹æ€§åŸºå‡†ï¼ˆSpatial/Counting/Negation/Temporalï¼‰ï¼Œç»Ÿä¸€é‡‡ç”¨å¤šé€‰é¢˜å›¾åƒ-æ–‡æœ¬åŒ¹é…èŒƒå¼ï¼Œè¦†ç›–å¯¹æ¯”å¼ä¸ç”Ÿæˆå¼VLMï¼Œå®ç°è·¨æ¨¡å‹å…¬å¹³è¯„ä¼°ã€‚  \nğŸ”¸è®¾è®¡å—æ§ç”¨æˆ·å®éªŒï¼šå›ºå®šCOCOå›¾åƒé›†ï¼Œå¯¹æ¯”ä¸åŒæ ‡æ³¨æŒ‡ä»¤ï¼ˆCOCOåŸæŒ‡ä»¤ã€LLaVAã€PixMoåŠæœ¬æ–‡æ–°æŒ‡ä»¤ï¼‰å¯¹å››ç±»æ¨ç†è¡¨è¾¾äº§å‡ºç‡çš„å½±å“ï¼ŒéªŒè¯æŒ‡ä»¤å¯å®šå‘ç¼“è§£åå·®ã€‚  \nğŸ”¸å¼€å±•å¾®è°ƒå®éªŒï¼Œå°†æŒ‰æ–°æŒ‡ä»¤ç”Ÿæˆçš„é«˜æ¨ç†å¯†åº¦æ•°æ®ï¼ˆ39%è®¡æ•°ï¼‰æ³¨å…¥LLaVA-1.5è®­ç»ƒï¼Œè¯å®å…¶æ˜¾è‘—æå‡æ¨¡å‹è®¡æ•°æ€§èƒ½ï¼ŒéªŒè¯æ•°æ®å¹²é¢„çš„æœ‰æ•ˆæ€§ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸æ‰€æœ‰ä¸»æµVLMï¼ˆåŒ…æ‹¬CLIPç³»åˆ—ã€LLaVAã€Molmoã€Qwen-VLåŠé—­æºGPT-4o/Geminiï¼‰åœ¨å››ç±»æ¨ç†ä»»åŠ¡ä¸Šå¹³å‡è½åäººç±»æ€§èƒ½è¾¾54åˆ†ï¼Œå°¤å…¶å¦å®šä»»åŠ¡æ¥è¿‘éšæœºæ°´å¹³ã€‚  \nğŸ”¸æ¨¡å‹ä¸æ•°æ®è§„æ¨¡æ‰©å¤§ï¼ˆå‚æ•°é‡ã€æ•°æ®é‡ã€å¤šè¯­è¨€ç¿»è¯‘ï¼‰å‡æœªå¸¦æ¥æ¨ç†èƒ½åŠ›çš„â€œæ¶Œç°â€ï¼Œç¼©æ”¾å®šå¾‹æ˜¾ç¤ºæŸå¤±å‡ ä¹ä¸éšè®¡ç®—é‡å¢åŠ è€Œä¸‹é™ï¼Œè¯æ˜åå·®æœ¬è´¨æ˜¯æ•°æ®ç”Ÿæˆæœºåˆ¶é—®é¢˜è€Œéè§„æ¨¡ä¸è¶³ã€‚  \nğŸ”¸æŠ¥å‘Šåå·®åŒæ ·å­˜åœ¨äºLLMåˆæˆæ•°æ®ï¼ˆå¦‚LLaVA-1.5ä¸­GPT-4ç”Ÿæˆéƒ¨åˆ†ï¼‰ï¼Œè¯´æ˜è¯­è¨€æ¨¡å‹ç»§æ‰¿å¹¶æ”¾å¤§äº†äººç±»åå·®ï¼ŒæŒ‡ä»¤è®¾è®¡å¯¹åˆæˆæ•°æ®åŒæ ·å…³é”®ã€‚  \nğŸ”¸æ ‡æ³¨æŒ‡ä»¤ç›´æ¥å½±å“æ¨ç†è¡¨è¾¾å¯†åº¦ï¼šæœ¬æ–‡æ–°æŒ‡ä»¤ä½¿å¦å®šä¸æ—¶é—´è¡¨è¾¾ç‡ä»0%è·ƒå‡è‡³52%å’Œ44%ï¼Œè€Œä»…å»¶é•¿å­—æ•°ï¼ˆ50è¯ï¼‰æ— æ³•æ¿€å‘è¢«é»˜è®¤å¿½ç•¥çš„æ¨ç†ç±»å‹ã€‚  \nğŸ”¸è®­ç»ƒæ•°æ®ä¸­æ¨ç†æ¦‚å¿µçš„çœŸå®è¦†ç›–ç‡ä¸æ¨¡å‹æ€§èƒ½å‘ˆå¼ºæ­£ç›¸å…³ï¼Œä¸”å¾®è°ƒå®éªŒè¯æ˜ï¼Œæå‡è¦†ç›–ç‡å¯ç›´æ¥æ”¹å–„æ¨¡å‹èƒ½åŠ›ï¼Œç¡®è®¤åå·®æ˜¯å¯å¹²é¢„çš„æ•°æ®è´¨é‡é—®é¢˜ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºå°†è¯­è¨€å­¦ä¸­çš„â€œæŠ¥å‘Šåå·®â€ç†è®ºé¦–æ¬¡æ·±åº¦è¿ç§»åˆ°å¤šæ¨¡æ€é¢†åŸŸï¼Œä¸ä»…æŒ‡å‡ºé—®é¢˜ï¼Œæ›´é€šè¿‡è·¨å­¦ç§‘ç†è®ºå»ºæ¨¡ã€å¤šç»´åº¦å®è¯æ£€éªŒä¸å¯å¤ç°å¹²é¢„å®éªŒï¼Œç¡®ç«‹äº†â€œæ•°æ®ç”Ÿæˆæœºåˆ¶ç¼ºé™·â€æ¯”â€œæ¨¡å‹æ¶æ„é™åˆ¶â€æ›´æ ¹æœ¬çš„å½’å› ï¼›å…¶æ ¸å¿ƒæ´è§â€”â€”â€œäººç±»äº¤æµçš„ç»æµæ€§åŸåˆ™å¤©ç„¶æ’æ–¥æ¨ç†å†—ä½™ï¼Œå› æ­¤å¿…é¡»ä¸»åŠ¨è®¾è®¡æ•°æ®é‡‡é›†è§„åˆ™â€â€”â€”ä¸ºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å‘å±•æä¾›äº†èŒƒå¼çº§å¯ç¤ºï¼šé«˜è´¨é‡æ¨ç†èƒ½åŠ›æ— æ³•é è§„æ¨¡å †ç Œï¼Œè€Œéœ€åœ¨æ•°æ®æºå¤´åµŒå…¥è®¤çŸ¥æ„è¯†ã€‚\n    "
    },
    {
        "title": "Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training",
        "authors": [
            "Tianle Xia",
            "Ming Xu",
            "Lingxiang Hu",
            "Yiding Sun",
            "Wenwei Li",
            "Linfang Shang",
            "Liqun Liu",
            "Peng Shu",
            "Huan Yu",
            "Jie Jiang"
        ],
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22576v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22576v1",
        "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, yet traditional single-round retrieval struggles with complex multi-step reasoning. Agentic RAG addresses this by enabling LLMs to dynamically decide when and what to retrieve, but current RL-based training methods suffer from sparse outcome rewards that discard intermediate signals and low sample efficiency where failed samples contribute nothing. We propose Search-P1, a framework that introduces path-centric reward shaping for agentic RAG training, comprising two key components: (1) Path-Centric Reward, which evaluates the structural quality of reasoning trajectories through order-agnostic step coverage and soft scoring that extracts learning signals even from failed samples, and (2) Dual-Track Path Scoring with offline-generated reference planners that assesses paths from both self-consistency and reference-alignment perspectives. Experiments on multiple QA benchmarks demonstrate that Search-P1 achieves significant improvements over Search-R1 and other strong baselines, with an average accuracy gain of 7.7 points.",
        "tag": "RAG æ£€ç´¢ä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22576v1ã€RAG æ£€ç´¢ä¼˜åŒ–-è…¾è®¯ã€‘Search-P1_ Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training.pdf",
        "institution_status": "keep",
        "institution": "è…¾è®¯",
        "first_institution": "è…¾è®¯",
        "institution_category": "å›½å†…å·¥ä¸šç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šSearch-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training\nğŸŒæ¥æºï¼šarXiv, 2602.22576v1\n\nç¬”è®°æ ‡é¢˜ï¼šè·¯å¾„ä¸­å¿ƒå¥–åŠ±å¡‘å½¢\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è§£å†³åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ™ºèƒ½ä½“RAGè®­ç»ƒä¸­å¥–åŠ±ç¨€ç–ã€æ ·æœ¬æ•ˆç‡ä½å’Œæ”¶æ•›æ…¢çš„é—®é¢˜ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºSEARCH-P1æ¡†æ¶ï¼Œé€šè¿‡è·¯å¾„ä¸­å¿ƒå¥–åŠ±å¡‘å½¢ï¼Œä»æ¨ç†è½¨è¿¹ç»“æ„è´¨é‡ä¸­æå–å¯†é›†ã€ç»†ç²’åº¦ã€å®¹é”™çš„å­¦ä¹ ä¿¡å·ï¼Œæ˜¾è‘—æå‡è®­ç»ƒç¨³å®šæ€§ä¸æ•ˆç‡ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å°†éšå¼æ¨ç†è®¡åˆ’æ˜¾å¼åŒ–ä¸ºç‹¬ç«‹â€œè§„åˆ’æ­¥â€ï¼Œä½¿è·¯å¾„ç»“æ„å¯è§‚æµ‹ã€å¯è¯„ä¼°ã€‚  \nğŸ”¸è®¾è®¡åŒè½¨è·¯å¾„è¯„åˆ†æœºåˆ¶ï¼šè‡ªä¸€è‡´æ€§è½¨ï¼ˆè¯„ä¼°æ¨¡å‹æ˜¯å¦å¿ å®æ‰§è¡Œè‡ªèº«è§„åˆ’ï¼‰ä¸å‚è€ƒå¯¹é½è½¨ï¼ˆåŸºäºç¦»çº¿ç”Ÿæˆçš„é«˜è´¨é‡å‚è€ƒè·¯å¾„è¿›è¡Œé¡ºåºæ— å…³åŒ¹é…ï¼‰ã€‚  \nğŸ”¸å¼•å…¥è½¯ç»“æœè¯„åˆ†ï¼Œåœ¨ç­”æ¡ˆé”™è¯¯æ—¶ä»ä¾æ®ç­”æ¡ˆéƒ¨åˆ†æ­£ç¡®æ€§ä¸æ¨ç†è´¨é‡ç»™äºˆæ¢¯åº¦ä¿¡å·ï¼Œå˜é›¶å¥–åŠ±æ ·æœ¬ä¸ºæœ‰æ•ˆè®­ç»ƒæ•°æ®ã€‚  \nğŸ”¸èåˆæ ¼å¼å¥–åŠ±ï¼ˆé¼“åŠ±ç»“æ„åŒ–è¾“å‡ºï¼‰ã€è·¯å¾„å¥–åŠ±ä¸è½¯ç»“æœå¥–åŠ±ï¼Œæ„å»ºå¤šç›®æ ‡åŠ æƒæ€»å¥–åŠ±å‡½æ•°ï¼Œå¹³è¡¡å‡†ç¡®æ€§ä¸æ¨ç†è´¨é‡ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åŒè½¨è·¯å¾„è¯„åˆ†ç¼ºä¸€ä¸å¯ï¼šç§»é™¤ä»»ä¸€è½¨é“å‡å¯¼è‡´å¹³å‡å‡†ç¡®ç‡ä¸‹é™3â€“5ä¸ªç™¾åˆ†ç‚¹ï¼ŒéªŒè¯äºŒè€…äº’è¡¥æ€§ã€‚  \nğŸ”¸è½¯ç»“æœè¯„åˆ†å¯¹å¤æ‚ä»»åŠ¡å¢ç›Šæ›´æ˜¾è‘—ï¼šåœ¨å¤šè·³QAä¸Šæå‡+3.5%ï¼Œåœ¨å·¥ä¸šå¹¿å‘ŠQAï¼ˆAD-QAï¼‰ä¸Šè¾¾+8.8%ï¼Œè¯´æ˜è·¯å¾„è´¨é‡ä¿¡å·åœ¨é«˜éš¾åº¦åœºæ™¯ä¸­ä»·å€¼æ›´é«˜ã€‚  \nğŸ”¸è·¯å¾„å¥–åŠ±æƒé‡å­˜åœ¨â€œç”œç‚¹åŒºâ€ï¼ˆÎ»p=0.3ï¼‰ï¼šè¿‡ä½åˆ™ç›‘ç£ä¸è¶³ï¼Œè¿‡é«˜åˆ™å¼•å‘å¥–åŠ±è¿‡æ‹Ÿåˆï¼ˆè·¯å¾„åˆ†å‡ä½†å‡†ç¡®ç‡é™ï¼‰ã€‚  \nğŸ”¸SEARCH-P1è®­ç»ƒæ”¶æ•›é€Ÿåº¦æå‡è¶…2å€ï¼š60æ­¥å³è¾¾Search-R1 150æ­¥åçš„æ€§èƒ½ï¼Œä¸”æ¨ç†è½®æ¬¡æ›´å°‘ã€æ›´ç¨³å®šï¼Œå°¤å…¶åœ¨å¤±è´¥æ¡ˆä¾‹ä¸­è½®æ¬¡æ³¢åŠ¨æ˜¾è‘—é™ä½ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†å¼ºåŒ–å­¦ä¹ çš„ç›‘ç£ç²’åº¦ä»â€œæœ€ç»ˆç­”æ¡ˆâ€ä¸‹æ²‰è‡³â€œå®Œæ•´æ¨ç†è·¯å¾„â€ï¼Œé€šè¿‡æ˜¾å¼è§„åˆ’å»ºæ¨¡ã€åŒè§†è§’è½¨è¿¹è¯„ä¼°ä¸è½¯æ€§ç»“æœåé¦ˆï¼Œç³»ç»Ÿæ€§ç ´è§£äº†agentic RAGè®­ç»ƒä¸­çš„ä¸‰å¤§é¡½ç–¾ï¼›å…¶è·¯å¾„ä¸­å¿ƒæ€æƒ³ä¸ä¾èµ–ç‰¹å®šæ¨¡å‹æˆ–RLç®—æ³•ï¼Œå…·å¤‡å¼ºæ³›åŒ–æ€§ä¸å·¥ç¨‹è½åœ°æ½œåŠ›ã€‚\n    "
    },
    {
        "title": "SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning",
        "authors": [
            "Sanjay Kariyappa",
            "G. Edward Suh"
        ],
        "categories": [
            "cs.AI",
            "cs.LG"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22603v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22603v1",
        "summary": "Long-running agentic tasks, such as deep research, require multi-hop reasoning over information distributed across multiple webpages and documents. In such tasks, the LLM context is dominated by tokens from external retrieval, causing memory usage to grow rapidly and limiting decode performance. While several KV cache compression techniques exist for long-context inputs, we find that existing heuristics fail to support multi-step reasoning models effectively. We address this challenge with SideQuest -- a novel approach that leverages the Large Reasoning Model (LRM) itself to perform KV cache compression by reasoning about the usefulness of tokens in its context. To prevent the tokens associated with this management process from polluting the model's memory, we frame KV cache compression as an auxiliary task executed in parallel to the main reasoning task. Our evaluations, using a model trained with just 215 samples, show that SideQuest reduces peak token usage by up to 65% on agentic tasks with minimal degradation in accuracy, outperforming heuristic-based KV cache compression techniques.",
        "tag": "Agent è®°å¿†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22603v1ã€Agent è®°å¿†-è‹±ä¼Ÿè¾¾ã€‘SideQuest_ Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning.pdf",
        "institution_status": "keep",
        "institution": "è‹±ä¼Ÿè¾¾",
        "first_institution": "è‹±ä¼Ÿè¾¾",
        "institution_category": "å›½å¤–å·¥ä¸šç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šSideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning\nğŸŒæ¥æºï¼šarXiv, 2602.22603v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ¨¡å‹é©±åŠ¨çš„KVç¼“å­˜ç®¡ç†\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨é•¿å‘¨æœŸæ™ºèƒ½ä½“å¤šæ­¥æ¨ç†ä¸­å®ç°é«˜æ•ˆã€è¯­ä¹‰æ„ŸçŸ¥çš„KVç¼“å­˜å‹ç¼©ï¼Œé¿å…é™æ€å¯å‘å¼æ–¹æ³•å¯¼è‡´çš„å…³é”®ä¿¡æ¯è¿‡æ—©ä¸¢å¼ƒï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºSideQuestæ¡†æ¶ï¼Œé¦–æ¬¡å°†KVç¼“å­˜ç®¡ç†å»ºæ¨¡ä¸ºå¤§æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰è‡ªä¸»æ‰§è¡Œçš„å¹¶è¡Œè¾…åŠ©ä»»åŠ¡ï¼Œé€šè¿‡æ¨¡å‹è‡ªèº«è¯­ä¹‰ç†è§£åŠ¨æ€è¯†åˆ«å¹¶æ¸…é™¤å¤±æ•ˆå·¥å…·å“åº”ï¼Œæ˜¾è‘—é™ä½å†…å­˜å¼€é”€ä¸”å‡ ä¹ä¸æŸæ¨ç†ç²¾åº¦ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡å¹¶è¡ŒåŒçº¿ç¨‹æ¶æ„ï¼šä¸»æ¨ç†çº¿ç¨‹æ‰§è¡ŒReActæµç¨‹ï¼Œè¾…åŠ©çº¿ç¨‹å®šæœŸåœ¨å…±äº«ä¸Šä¸‹æ–‡ä¸Šè¿è¡Œï¼Œä¸“è´£å†…å­˜ç®¡ç†ï¼Œé¿å…ç®¡ç†tokenæ±¡æŸ“ä¸»æ³¨æ„åŠ›çª—å£ã€‚  \nğŸ”¸å¼•å…¥è§¦å‘å¼æ¨¡å‹ steeringï¼šåœ¨è¾…åŠ©çº¿ç¨‹è¾“å…¥å‰æ·»åŠ â€œMemory management modeâ€æç¤ºï¼Œå¹¶ç»“åˆä»»åŠ¡ç‰¹å®šå¾®è°ƒï¼Œä½¿æ¨¡å‹ç²¾å‡†åˆ‡æ¢è‡³ç¼“å­˜æ¸…ç†æ¨¡å¼ã€‚  \nğŸ”¸åŸºäºå›æº¯åˆ†ææ„å»ºè®­ç»ƒæ•°æ®ï¼šå¯¹æ­£ç¡®æ¨ç†è½¨è¿¹æ ‡æ³¨å„å·¥å…·è¾“å‡ºçš„â€œæœ€åä½¿ç”¨è½®æ¬¡â€ï¼Œç”Ÿæˆå¸¦æ©ç çš„æ¨¡æ‹Ÿå‹ç¼©åœºæ™¯åŠå¯¹åº”åˆ é™¤æŒ‡ä»¤ï¼Œè”åˆè’¸é¦æŸå¤±ä¸äº¤å‰ç†µæŸå¤±è”åˆä¼˜åŒ–ã€‚  \nğŸ”¸èšç„¦å·¥å…·å“åº”çº§ç»†ç²’åº¦æ¸…ç†ï¼šä»…é’ˆå¯¹æµè§ˆå™¨å·¥å…·è¿”å›çš„å¸¦æ¸¸æ ‡æ ‡è¯†ï¼ˆå¦‚[Cursor 0]ï¼‰å†…å®¹è¿›è¡Œå¯¹è±¡çº§é©±é€ï¼Œå…¼é¡¾å¯è§£é‡Šæ€§ä¸å·¥ç¨‹å¯è¡Œæ€§ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸SideQueståœ¨FRAMESå’ŒBrowseCompåŸºå‡†ä¸Šå°†å³°å€¼tokenç”¨é‡é™ä½56â€“65%ï¼ŒKVç¼“å­˜è¯»å–é‡å‡å°‘53â€“71%ï¼Œè€Œå‡†ç¡®ç‡ä»…ä¸‹é™æœ€å¤š2%ï¼ˆFRAMESï¼‰å’Œ5%ï¼ˆBrowseCompï¼‰ï¼Œè¿œä¼˜äºH2Oã€SnapKVç­‰å¯å‘å¼æ–¹æ³•ã€‚  \nğŸ”¸å›ºå®štokené¢„ç®—çš„å¯å‘å¼æ–¹æ³•åœ¨ä»»åŠ¡éš¾åº¦åˆ†å¸ƒå®½æ³›æ—¶è¡¨ç°è„†å¼±â€”â€”ç®€å•ä»»åŠ¡æµªè´¹å†…å­˜ï¼Œå¤æ‚ä»»åŠ¡å› ç¡¬æˆªæ–­è€Œå´©æºƒï¼›SideQuestè‡ªé€‚åº”æŒ‰éœ€æ¸…ç†ï¼Œæ— éœ€äººå·¥è®¾å®šé¢„ç®—ã€‚  \nğŸ”¸SideQuestæ˜¾è‘—æå‡ç”Ÿäº§æœåŠ¡æ€§èƒ½ï¼šåœ¨å•H100 GPUä¸Šï¼Œç³»ç»Ÿååé‡æå‡83.9%ï¼Œå³°å€¼KVç¼“å­˜å ç”¨ä¸‹é™53.9%ï¼Œæ€»åŸºå‡†æµ‹è¯•è€—æ—¶å‡å°‘36.8%ã€‚  \nğŸ”¸å…¶é²æ£’æ€§æ›´å¼ºï¼šéå®Œæˆç‡ï¼ˆå«è§£æå¤±è´¥ã€ä¸Šä¸‹æ–‡è¶…é™ã€æ­»å¾ªç¯ï¼‰ä¸æœªå‹ç¼©åŸºçº¿ç›¸å½“ï¼Œè€Œå¯å‘å¼æ–¹æ³•å¸¸å› è¯¯åˆ å…³é”®tokenå¯¼è‡´è¯­æ³•æˆ–é€»è¾‘æ–­è£‚ï¼Œäº§ç”Ÿå¤§é‡ä¸å¯è§£æå“åº”ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†ä¼ ç»Ÿè§†ä¸ºç³»ç»Ÿçº§çº¦æŸçš„KVç¼“å­˜ç®¡ç†ï¼Œå‡ç»´ä¸ºæ¨¡å‹è‡ªèº«çš„å¯å­¦ä¹ æ¨ç†èƒ½åŠ›â€”â€”ä¸æ˜¯ç”¨å¤–éƒ¨è§„åˆ™â€œå‰ªæâ€ï¼Œè€Œæ˜¯è®©æ¨¡å‹â€œç†è§£ä½•æ—¶è¯¥é—å¿˜â€ã€‚å…¶å¹¶è¡Œè½»é‡æ¶æ„ã€å›æº¯å¼æ•°æ®åˆæˆä¸æ¸¸æ ‡çº§æ“ä½œè®¾è®¡ï¼Œå…¼å…·ç†è®ºæ·±åº¦ä¸å·¥ç¨‹è½åœ°æ€§ï¼Œä¸ºé•¿ç¨‹æ™ºèƒ½ä½“çš„é«˜æ•ˆæ¨ç†æä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs",
        "authors": [
            "Guanting Ye",
            "Qiyan Zhao",
            "Wenhao Yu",
            "Liangyu Yuan",
            "Mingkai Li",
            "Xiaofeng Zhang",
            "Jianmin Ji",
            "Yanyong Zhang",
            "Qing Jiang",
            "Ka-Veng Yuen"
        ],
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22716v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22716v1",
        "summary": "3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. The vanilla RoPE formulation fails to preserve essential three-dimensional spatial structures when encoding 3D tokens, and its relative distance computation overlooks angular dependencies, hindering the model's ability to capture directional variations in visual representations. To overcome these limitations, we introduce Spherical Coordinate-based Positional Embedding (SoPE). Our method maps point-cloud token indices into a 3D spherical coordinate space, enabling unified modeling of spatial locations and directional angles. This formulation preserves the inherent geometric structure of point-cloud data, enhances spatial awareness, and yields more consistent and expressive geometric representations for multimodal learning. In addition, we introduce a multi-scale frequency mixing strategy to fuse feature information across different frequency domains. Experimental results on multiple 3D scene benchmarks validate the effectiveness of our approach, while real-world deployment experiments further demonstrate its strong generalization capability.",
        "tag": "ä½ç½®ç¼–ç ä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22716v1ã€ä½ç½®ç¼–ç ä¼˜åŒ–-æ¾³é—¨å¤§å­¦ã€‘SoPE_ Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs.pdf",
        "institution_status": "keep",
        "institution": "æ¾³é—¨å¤§å­¦ã€ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦ã€ä¸Šæµ·äº¤é€šå¤§å­¦ã€åˆè‚¥å·¥ä¸šå¤§å­¦ã€æ–°åŠ å¡å›½ç«‹å¤§å­¦",
        "first_institution": "æ¾³é—¨å¤§å­¦",
        "institution_category": "å…¶ä»–æœºæ„",
        "note": "ğŸ“–æ ‡é¢˜ï¼šSoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs\nğŸŒæ¥æºï¼šarXiv, 2602.22716v1\n\nç¬”è®°æ ‡é¢˜ï¼šSoPEæå‡3Dç©ºé—´æ„ŸçŸ¥\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•æ”¹è¿›3Då¤§è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰å¯¹ç‚¹äº‘å‡ ä½•ç»“æ„ä¸æ–¹å‘ä¿¡æ¯å»ºæ¨¡èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºåŸºäºçƒåæ ‡ç³»çš„ä½ç½®ç¼–ç SoPEï¼Œå°†ç‚¹äº‘tokenæ˜ å°„è‡³(t, r, Î¸, Ï†)å››ç»´ç©ºé—´ï¼Œè”åˆå»ºæ¨¡æ—¶åºã€æ·±åº¦ä¸æ–¹å‘ï¼Œå¹¶å¼•å…¥å¤šå°ºåº¦é¢‘ç‡æ··åˆç­–ç•¥ï¼Œæ˜¾è‘—å¢å¼º3Dç©ºé—´ä¸æ–¹å‘æ„ŸçŸ¥èƒ½åŠ›ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å°†ç‚¹äº‘tokençš„ç¬›å¡å°”åæ ‡(x,y,z)é‡æ˜ å°„ä¸ºçƒåæ ‡(r,Î¸,Ï†)ï¼Œå¹¶ä¿ç•™åŸå§‹æ—¶åºç´¢å¼•tï¼Œæ„å»ºå‡ ä½•æ„ŸçŸ¥çš„å››ç»´ä½ç½®ç´¢å¼•(t,r,Î¸,Ï†)ã€‚  \nğŸ”¸æ‰©å±•RoPEç›¸å¯¹ä½ç½®è®¡ç®—ï¼Œåˆ†åˆ«å¯¹tã€rã€Î¸ã€Ï†å››ä¸ªç»´åº¦ç‹¬ç«‹å»ºæ¨¡ç›¸å¯¹ä½ç§»Î”tã€Î”rã€Î”Î¸ã€Î”Ï†ï¼Œä½¿æ³¨æ„åŠ›æœºåˆ¶èƒ½æ˜¾å¼æ•æ‰ç©ºé—´è·ç¦»ä¸è§’åº¦å˜åŒ–ã€‚  \nğŸ”¸è®¾è®¡å››ç»´é¢‘ç‡åˆ†é…ç­–ç•¥ï¼ˆt:r:Î¸:Ï†=24:2:3:3ï¼‰ï¼Œå°†ä½é¢‘æ®µåˆ†é…ç»™æ—¶åºtä»¥ä¿éšœé•¿ç¨‹ä¸€è‡´æ€§ï¼Œé«˜é¢‘æ®µåˆ†é…ç»™çƒåæ ‡åˆ†é‡ä»¥å¢å¼ºç»†ç²’åº¦ç©ºé—´/æ–¹å‘åˆ†è¾¨åŠ›ã€‚  \nğŸ”¸æå‡ºå¤šå°ºåº¦ç›¸ä½æ··åˆç­–ç•¥ï¼Œå¯¹æ¯ä¸ªåæ ‡åˆ†é‡åº”ç”¨çº¿æ€§ã€å¯¹æ•°å‹ç¼©å’Œå‘¨æœŸæ€§ä¸‰ç§å˜æ¢ï¼Œèåˆä¸åŒå°ºåº¦çš„ä½ç½®å…ˆéªŒï¼Œå…¼é¡¾å±€éƒ¨ç»†èŠ‚ä¸å…¨å±€ç»“æ„ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸SoPEåœ¨Structured3Då’ŒARKitScenesç­‰åŸºå‡†ä¸ŠæŒç»­æå‡å¸ƒå±€ä¼°è®¡ä¸3Dç›®æ ‡æ£€æµ‹æ€§èƒ½ï¼ŒIoUæŒ‡æ ‡å…¨é¢è¶…è¶ŠSpatialLMåŠRoPE-3Dç­‰åŸºçº¿ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šçƒåæ ‡é‡å‚æ•°åŒ–ã€éå‡åŒ€é¢‘ç‡åˆ†é…ã€å¤šå°ºåº¦æ··åˆä¸‰è€…å‡å¸¦æ¥æ˜¾è‘—å¢ç›Šï¼Œå…¶ä¸­çƒåæ ‡å»ºæ¨¡æ˜¯æå‡æ–¹å‘æ„ŸçŸ¥çš„æ ¸å¿ƒã€‚  \nğŸ”¸å¯è§†åŒ–æ˜¾ç¤ºSoPEç”Ÿæˆæ›´å‡è¡¡ã€å…¨å±€åŒ–çš„è·¨æ¨¡æ€æ³¨æ„åŠ›å›¾ï¼Œæœ‰æ•ˆç¼“è§£ä¼ ç»ŸRoPEå¯¼è‡´çš„â€œçƒ­ç‚¹é›†ä¸­â€ä¸ç©ºé—´æ„ŸçŸ¥åå·®é—®é¢˜ã€‚  \nğŸ”¸çœŸå®æœºå™¨äººéƒ¨ç½²éªŒè¯SoPEå¯æ”¯æ’‘ç«¯åˆ°ç«¯åœºæ™¯ç†è§£ä¸ä»»åŠ¡è§„åˆ’ï¼Œæ˜¾è‘—æå‡å°ç‰©ä½“æ£€æµ‹é²æ£’æ€§åŠå¤šè§†è§’é¢„æµ‹ä¸€è‡´æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†ç‰©ç†ç©ºé—´å‡ ä½•å…ˆéªŒï¼ˆçƒåæ ‡ï¼‰æ·±åº¦èå…¥ä½ç½®ç¼–ç è®¾è®¡ï¼Œçªç ´äº†RoPEçº¯åºåˆ—å»ºæ¨¡çš„å±€é™ï¼›å…¶å››ç»´è§£è€¦+å¤šå°ºåº¦æ··åˆçš„æ¶æ„å…¼å…·ç†è®ºåˆç†æ€§ä¸å·¥ç¨‹å®ç”¨æ€§ï¼Œä¸º3Då¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„ä½ç½®å»ºæ¨¡æä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents",
        "authors": [
            "Zhou Xu",
            "Bowen Zhou",
            "Qi Wang",
            "Shuwen Feng",
            "Jingyu Xiao"
        ],
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.23235v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23235v1",
        "summary": "Pure-vision GUI agents provide universal interaction capabilities but suffer from severe efficiency bottlenecks due to the massive spatiotemporal redundancy inherent in high-resolution screenshots and historical trajectories. We identify two critical misalignments in existing compression paradigms: the temporal mismatch, where uniform history encoding diverges from the agent's \"fading memory\" attention pattern, and the spatial topology conflict, where unstructured pruning compromises the grid integrity required for precise coordinate grounding, inducing spatial hallucinations. To address these challenges, we introduce GUIPruner, a training-free framework tailored for high-resolution GUI navigation. It synergizes Temporal-Adaptive Resolution (TAR), which eliminates historical redundancy via decay-based resizing, and Stratified Structure-aware Pruning (SSP), which prioritizes interactive foregrounds and semantic anchors while safeguarding global layout. Extensive evaluations across diverse benchmarks demonstrate that GUIPruner consistently achieves state-of-the-art performance, effectively preventing the collapse observed in large-scale models under high compression. Notably, on Qwen2-VL-2B, our method delivers a 3.4x reduction in FLOPs and a 3.3x speedup in vision encoding latency while retaining over 94% of the original performance, enabling real-time, high-precision navigation with minimal resource consumption.",
        "tag": "Agent æ¶æ„ä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.23235v1ã€Agent æ¶æ„ä¼˜åŒ–-æ¸…åå¤§å­¦ã€‘Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents.pdf",
        "institution_status": "keep",
        "institution": "æ¸…åå¤§å­¦ã€é¦™æ¸¯ä¸­æ–‡å¤§å­¦",
        "first_institution": "æ¸…åå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šSpatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents\nğŸŒæ¥æºï¼šarXiv, 2602.23235v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ—¶ç©ºæ„ŸçŸ¥çš„GUIè§†è§‰å‹ç¼©\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸æŸå®³é«˜ç²¾åº¦åæ ‡å®šä½èƒ½åŠ›çš„å‰æä¸‹ï¼Œé«˜æ•ˆå‹ç¼©GUIä»£ç†ä¸­é«˜åˆ†è¾¨ç‡æˆªå›¾ä¸å†å²è½¨è¿¹å¸¦æ¥çš„æ—¶ç©ºå†—ä½™ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºæ— éœ€è®­ç»ƒçš„GUIPruneræ¡†æ¶ï¼Œé€šè¿‡æ—¶åºè‡ªé€‚åº”åˆ†è¾¨ç‡è°ƒæ•´ä¸åˆ†å±‚ç»“æ„æ„ŸçŸ¥å‰ªæï¼Œå…¼é¡¾æ•ˆç‡æå‡ä¸ç©ºé—´æ‹“æ‰‘å®Œæ•´æ€§ï¼Œå®ç°é«˜ä¿çœŸã€ä½å¼€é”€çš„GUIå¯¼èˆªã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è¯†åˆ«ä¸¤å¤§ç“¶é¢ˆï¼šå†å²ç»´åº¦å­˜åœ¨â€œæ—¶é—´è¡°å‡â€ç°è±¡ï¼ˆè¿‘æœŸå¸§éœ€é«˜åˆ†è¾¨ç‡ï¼Œè¿œæœŸä»…éœ€è¯­ä¹‰è½®å»“ï¼‰ï¼Œå½“å‰å¸§å­˜åœ¨â€œç¨€ç–-æ‹“æ‰‘å†²çªâ€ï¼ˆèƒŒæ™¯ token å æ¯”è¶…60%ï¼Œä½†éƒ¨åˆ†è¾¹ç•ŒåŒºåŸŸæ˜¯å…³é”®è¯­ä¹‰é”šç‚¹ï¼Œä¸å¯éšæ„è£å‰ªï¼‰ã€‚  \nğŸ”¸è®¾è®¡Temporal-Adaptive Resolutionï¼ˆTARï¼‰ï¼šä»¥å…¨å±€ token é¢„ç®—ä¸ºçº¦æŸï¼ŒæŒ‰çº¿æ€§è¡°å‡åˆ†é…å„å†å²å¸§åˆ†è¾¨ç‡ï¼Œå¯¹è¿œæœŸå¸§ç›´æ¥é™é‡‡æ ·ï¼Œä»æºå¤´å‰Šå‡è§†è§‰ç¼–ç è®¡ç®—é‡ã€‚  \nğŸ”¸è®¾è®¡Stratified Structure-aware Pruningï¼ˆSSPï¼‰ï¼šåœ¨æµ…å±‚LLMä¸­åˆ†ä¸‰çº§ä¿ç•™ tokenâ€”â€”ä¼˜å…ˆä¿ç•™äº¤äº’å‰æ™¯ï¼ˆæŒ‰é’®/è¾“å…¥æ¡†ï¼‰ã€æ¬¡é€‰è¯­ä¹‰æ˜¾è‘—èƒŒæ™¯ï¼ˆåŸºäºæ³¨æ„åŠ›æ’åºï¼‰ã€æœ€åç”¨å‡åŒ€ç½‘æ ¼é‡‡æ ·ï¼ˆUGSï¼‰è¡¥å…¨å…¨å±€å¸ƒå±€ï¼Œå¼ºåˆ¶ç»´æŒ2Dç©ºé—´ç»“æ„ã€‚  \nğŸ”¸é‡‡ç”¨çº¯æ¨ç†æ—¶ã€é›¶å‚æ•°æ›´æ–°çš„å³æ’å³ç”¨è®¾è®¡ï¼Œå…¼å®¹ä»»æ„é¢„è®­ç»ƒå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆå¦‚Qwen2-VLç³»åˆ—ï¼‰ï¼Œæ— éœ€å¾®è°ƒæˆ–é‡è®­ç»ƒã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨Mind2Webç­‰é«˜åˆ†è¾¨ç‡ç¨€ç–åœºæ™¯ä¸‹ï¼ŒGUIPruneræ˜¾è‘—ä¼˜äºDivPruneã€CDPrunerç­‰SOTAæ–¹æ³•ï¼Œé¿å…å…¶å› æ¿€è¿›é¢„LLMå‰ªæå¯¼è‡´çš„æ€§èƒ½å´©å¡Œï¼ˆå¦‚7Bæ¨¡å‹ä»34.7%â†’7.7%ï¼‰ã€‚  \nğŸ”¸TARæ¨¡å—åœ¨40%å†å²ä¿ç•™ç‡ä¸‹å³å¯æ¥è¿‘æ— æŸæ€§èƒ½ï¼Œåœ¨Mind2Webä¸Šç”šè‡³å°å¹…è¶…è¶ŠåŸå§‹æ¨¡å‹ï¼ŒéªŒè¯â€œè¡°å‡è®°å¿†â€æœºåˆ¶å¯æ»¤é™¤å¹²æ‰°å™ªå£°ã€‚  \nğŸ”¸SSPæ¨¡å—åœ¨45%å½“å‰å¸§ä¿ç•™ç‡ä¸‹ä»ä¿æŒ87.3%ç›¸å¯¹æ€§èƒ½ï¼Œè€Œéšæœºé‡‡æ ·ä»…è¾¾86.1%ï¼Œè¯æ˜å‡åŒ€ç½‘æ ¼é‡‡æ ·å¯¹é˜²æ­¢ç©ºé—´å¹»è§‰å…·æœ‰ä¸å¯æ›¿ä»£ä½œç”¨ã€‚  \nğŸ”¸åœ¨Qwen2-VL-2Bä¸Šå®ç°3.4Ã— FLOPsä¸‹é™ä¸3.3Ã—è§†è§‰ç¼–ç åŠ é€Ÿï¼ŒGPUæ˜¾å­˜å‹é™è‡³5.9GBï¼Œæ”¯æŒè¾¹ç¼˜è®¾å¤‡å®æ—¶è¿è¡Œã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†äººç±»å·¥ä½œè®°å¿†çš„â€œè¿‘å› æ•ˆåº”â€å»ºæ¨¡ä¸ºæ—¶åºè¡°å‡ç­–ç•¥ï¼Œå¹¶å°†GUIå¼ºç»“æ„åŒ–å…ˆéªŒï¼ˆå¦‚ç½‘æ ¼å¸ƒå±€ã€è¾¹ç•Œé”šç‚¹ï¼‰æ˜¾å¼åµŒå…¥å‰ªæè¿‡ç¨‹ï¼Œçªç ´äº†é€šç”¨å›¾åƒtokenå‹ç¼©æ–¹æ³•åœ¨åæ ‡æ•æ„Ÿä»»åŠ¡ä¸­çš„å±€é™ï¼›å…¶è§£è€¦å¼æ—¶ç©ºå¤„ç†èŒƒå¼ä¸ºå¤šæ¨¡æ€ä»£ç†çš„è½»é‡åŒ–æä¾›äº†å¯è¿ç§»çš„æ–¹æ³•è®ºå¯ç¤ºã€‚\n    "
    },
    {
        "title": "Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation",
        "authors": [
            "Zihang Xu",
            "Haozhi Xie",
            "Ziqi Miao",
            "Wuxuan Gong",
            "Chen Qian",
            "Lijun Li"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22556v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22556v1",
        "summary": "Large reasoning models (LRMs) achieve strong performance through extended reasoning traces, but they often exhibit overthinking behavior for low-complexity queries. Existing efforts to mitigate this issue are fundamentally limited by unstable accuracy-efficiency trade-offs and poor robustness to heterogeneous reasoning behaviors. To address these challenges, we propose a two-stage framework for stable adaptive thinking in LRMs. The framework first applies Hybrid Fine-Tuning to expose the model to both thinking and no-thinking behaviors, establishing well-conditioned initialization. It then performs adaptive reinforcement learning with Correctness-Preserving Advantage Shaping (CPAS) to avoid suppressing correct long-chain reasoning, and Length-Aware Gradient Regulation (LAGR) to stabilize optimization under severe reasoning-length heterogeneity. Extensive experiments on Qwen2.5-1.5B and 7B show consistent improvements over strong baselines, achieving up to +3.7/+3.6 accuracy points while reducing generated tokens by 40.6%/43.9%. Further analyses across varying problem difficulties and out-of-distribution tasks confirm the robustness and generalization of our approach.",
        "tag": "å¿«æ…¢æ€è€ƒ",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22556v1ã€å¿«æ…¢æ€è€ƒ-ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ã€‘Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation.pdf",
        "institution_status": "keep",
        "institution": "ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ã€åŒ—äº¬é‚®ç”µå¤§å­¦ã€ä¸­å›½äººæ°‘å¤§å­¦",
        "first_institution": "ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šStable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation\nğŸŒæ¥æºï¼šarXiv, 2602.22556v1\n\nç¬”è®°æ ‡é¢˜ï¼šç¨³å®šè‡ªé€‚åº”æ¨ç†æ¡†æ¶\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨å¤§å‹æ¨ç†æ¨¡å‹ä¸­å®ç°æ—¢é«˜æ•ˆåˆå‡†ç¡®çš„è‡ªé€‚åº”æ€è€ƒï¼Œé¿å…ç®€å•é—®é¢˜è¿‡æ€è€ƒã€å¤æ‚é—®é¢˜æ¬ æ€è€ƒï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºé¦–ä¸ªå…¼é¡¾è®­ç»ƒç¨³å®šæ€§ä¸éš¾åº¦æ„ŸçŸ¥èƒ½åŠ›çš„ä¸¤é˜¶æ®µè‡ªé€‚åº”æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŠ¿å¡‘å½¢ä¸é•¿åº¦æ„ŸçŸ¥æ¢¯åº¦è°ƒæ§è§£å†³ç²¾åº¦-æ•ˆç‡æƒè¡¡å¤±ç¨³å’Œæ¨ç†é“¾é•¿å¼‚è´¨æ€§å¯¼è‡´çš„ä¼˜åŒ–å´©æºƒé—®é¢˜ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨æ··åˆç›‘ç£å¾®è°ƒï¼ˆHFTï¼‰ï¼Œç»Ÿä¸€æ³¨å…¥â€œæ€è€ƒâ€ä¸â€œä¸æ€è€ƒâ€ä¸¤ç§è¡Œä¸ºæ¨¡å¼ï¼Œä¸ºåç»­å¼ºåŒ–å­¦ä¹ æä¾›è‰¯å¥½åˆå§‹åŒ–ã€‚  \nğŸ”¸ç¬¬äºŒé˜¶æ®µæ„å»ºæ”¹è¿›å‹GRPOå¼ºåŒ–å­¦ä¹ æµç¨‹ï¼Œå¼•å…¥æ­£ç¡®æ€§ä¿æŒä¼˜åŠ¿å¡‘å½¢ï¼ˆCPASï¼‰ï¼Œå¯¹æ­£ç¡®çŸ­é“¾é¢å¤–æ¿€åŠ±ã€å¯¹æ­£ç¡®é•¿é“¾ä¸æƒ©ç½šï¼Œé˜²æ­¢æ¢ç´¢èƒ½åŠ›é€€åŒ–ã€‚  \nğŸ”¸è®¾è®¡é•¿åº¦æ„ŸçŸ¥æ¢¯åº¦è°ƒèŠ‚ï¼ˆLAGRï¼‰ï¼Œä¾æ®å“åº”é•¿åº¦åŠ¨æ€é‡åŠ æƒæ¢¯åº¦ï¼Œå¹¶æ˜¾å¼å¢å¼ºæ§åˆ¶ä»¤ç‰Œæ¢¯åº¦ï¼Œç¼“è§£é•¿é“¾ç¨€é‡Šæ•ˆåº”ã€‚  \nğŸ”¸ä½¿ç”¨æ˜¾å¼æ§åˆ¶ä»¤ç‰Œï¼ˆ/think /no_thinkï¼‰å®ç°å•æ¨¡å‹å†…æ¨¡å¼è‡ªä¸»å†³ç­–ï¼Œæ— éœ€å¤–éƒ¨è·¯ç”±æˆ–å¤šæ¨¡å‹éƒ¨ç½²ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨Qwen2.5-1.5B/7Bä¸Šï¼Œç›¸æ¯”æœ€å¼ºåŸºçº¿ï¼Œå‡†ç¡®ç‡æå‡3.7/3.6åˆ†ï¼Œç”Ÿæˆtokenå‡å°‘40.6%/43.9%ï¼ŒéªŒè¯æœ‰æ•ˆæ€§ä¸æ³›åŒ–æ€§ã€‚  \nğŸ”¸è·¨éš¾åº¦åˆ†ææ˜¾ç¤ºï¼šç®€å•é¢˜ï¼ˆMATH-500ï¼‰77.6%é€‰/no_thinkï¼Œéš¾é¢˜ï¼ˆAIMEï¼‰æ€è€ƒæ¯”ä¾‹æ˜¾è‘—ä¸Šå‡ï¼Œè¯æ˜éš¾åº¦æ„ŸçŸ¥å†³ç­–èƒ½åŠ›ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜CPASåŠ é€Ÿæ—©æœŸæ¢ç´¢å¹¶æå‡å³°å€¼æ€§èƒ½ï¼ŒLAGRä¸­Î²=0.4ä¸Î»=10æ—¶å–å¾—æœ€ä¼˜ç²¾åº¦-æ•ˆç‡å¹³è¡¡ã€‚  \nğŸ”¸åœ¨OODæ•°æ®é›†GPQAä¸Šä»æå‡å‡†ç¡®ç‡è‡³50.4%ï¼ˆ+2.9ï¼‰ã€tokenå‡å°‘51.0%ï¼Œè¯å®å¼ºæ³›åŒ–èƒ½åŠ›ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†â€œç¨³å®šæ€§â€ä½œä¸ºè‡ªé€‚åº”æ¨ç†çš„æ ¸å¿ƒè®¾è®¡ç›®æ ‡ï¼Œè€Œéä»…è¿½æ±‚æŒ‡æ ‡æå‡ï¼›CPASä»å¥–åŠ±ä¿¡å·å±‚é¢ä¿éšœé•¿é“¾åˆç†æ€§ï¼ŒLAGRä»ä¼˜åŒ–æœºåˆ¶å±‚é¢åº”å¯¹é•¿åº¦å¼‚è´¨æ€§ï¼ŒäºŒè€…ååŒæ„æˆåŸç†æ¸…æ™°ã€å¯è§£é‡Šæ€§å¼ºçš„ç³»ç»Ÿæ€§è§£æ³•ï¼›æ§åˆ¶ä»¤ç‰Œ+å•æ¨¡å‹æ¶æ„å¤§å¹…é™ä½éƒ¨ç½²é—¨æ§›ï¼Œå…·å¤‡å®é™…è½åœ°ä»·å€¼ã€‚\n    "
    },
    {
        "title": "Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance",
        "authors": [
            "Weida Liang",
            "Yiyou Sun",
            "Shuyuan Nan",
            "Chuang Li",
            "Dawn Song",
            "Kenji Kawaguchi"
        ],
        "categories": [
            "cs.AI",
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22583v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22583v1",
        "summary": "Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strategy usage-whether a reasoning strategy appears in successful solutions-and strategy executability-whether the strategy remains effective when instantiated as guidance for a target model. Through a controlled analysis of paired human-written and model-generated solutions, we identify a systematic dissociation between usage and executability: human- and model-derived strategies differ in structured, domain-dependent ways, leading to complementary strengths and consistent source-dependent reversals under guidance. Building on this diagnosis, we propose Selective Strategy Retrieval (SSR), a test-time framework that explicitly models executability by selectively retrieving and combining strategies using empirical, multi-route, source-aware signals. Across multiple mathematical reasoning benchmarks, SSR yields reliable and consistent improvements over direct solving, in-context learning, and single-source guidance, improving accuracy by up to $+13$ points on AIME25 and $+5$ points on Apex for compact reasoning models. Code and benchmark are publicly available at: https://github.com/lwd17/strategy-execute-pipeline.",
        "tag": "æ¨ç†ç­–ç•¥ä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22583v1ã€æ¨ç†ç­–ç•¥ä¼˜åŒ–-æ–°åŠ å¡å›½ç«‹å¤§å­¦ã€‘Strategy Executability in Mathematical Reasoning_ Leveraging Human-Model Differences for Effective Guidance.pdf",
        "institution_status": "keep",
        "institution": "æ–°åŠ å¡å›½ç«‹å¤§å­¦ã€åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡",
        "first_institution": "æ–°åŠ å¡å›½ç«‹å¤§å­¦",
        "institution_category": "å›½å¤–å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šStrategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance\nğŸŒæ¥æºï¼šarXiv, 2602.22583v1\n\nç¬”è®°æ ‡é¢˜ï¼šç­–ç•¥å¯æ‰§è¡Œæ€§å»ºæ¨¡\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šä¸ºä»€ä¹ˆæ­£ç¡®ä¸”ç›¸å…³çš„é—®é¢˜æ±‚è§£ç­–ç•¥åœ¨ä½œä¸ºæ¨ç†å¼•å¯¼æ—¶ä»å¸¸å¤±æ•ˆï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºâ€œç­–ç•¥å¯æ‰§è¡Œæ€§â€æ–°æ¦‚å¿µï¼Œæ­ç¤ºç­–ç•¥åœ¨äººç±»è§£æ³•ä¸­å‡ºç°ä¸åœ¨æ¨¡å‹ä¸ŠæˆåŠŸå¼•å¯¼ä¹‹é—´çš„ç³»ç»Ÿæ€§è„±èŠ‚ï¼Œå¹¶æ®æ­¤è®¾è®¡è½»é‡çº§æµ‹è¯•æ—¶æ¡†æ¶SSRã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å®šä¹‰ç­–ç•¥å¯æ‰§è¡Œæ€§ä¸ºç­–ç•¥åœ¨å›ºå®šæç¤ºä¸è§£ç æ¡ä»¶ä¸‹æå‡ç›®æ ‡æ¨¡å‹æ­£ç¡®ç‡çš„èƒ½åŠ›ï¼ŒåŒºåˆ«äºå•çº¯ç»Ÿè®¡ç­–ç•¥åœ¨æˆåŠŸè§£æ³•ä¸­çš„å‡ºç°é¢‘ç‡ï¼ˆå³ç­–ç•¥ä½¿ç”¨ï¼‰ã€‚  \nğŸ”¸æ„å»ºHM-ReasoningBenché…å¯¹æ•°æ®é›†ï¼Œç³»ç»Ÿåˆ†æäººç±»ä¸æ¨¡å‹è§£æ³•åœ¨ç­–ç•¥ä½¿ç”¨ä¸Šçš„ç»“æ„åŒ–å·®å¼‚ï¼šäººç±»åå¥½ç»“æ„/å‡ ä½•ç±»ç­–ç•¥ï¼ˆå¦‚è¾…åŠ©çº¿ã€å¯¹ç§°æ€§ï¼‰ï¼Œæ¨¡å‹å€¾å‘ç¨‹åº/ä»£æ•°ç±»ç­–ç•¥ï¼ˆå¦‚åæ ‡è®¾å®šã€åˆ†æƒ…å†µè®¨è®ºï¼‰ã€‚  \nğŸ”¸å‘ç°ç­–ç•¥æ¥æºä¸é¢†åŸŸå¼ºè€¦åˆï¼šäººç±»ç­–ç•¥åœ¨å‡ ä½•/ç»„åˆé¢˜ä¸­æ›´å¯æ‰§è¡Œï¼Œæ¨¡å‹ç­–ç•¥åœ¨ä»£æ•°/æ•°è®ºé¢˜ä¸­æ›´å¯é ï¼Œå•ä¸€æ¥æºå¼•å¯¼æ˜“å¤±è´¥ã€‚  \nğŸ”¸æå‡ºSelective Strategy Retrievalï¼ˆSSRï¼‰ï¼šåŸºäºå¼‚æ„ç­–ç•¥çŸ¥è¯†å›¾å»ºæ¨¡ï¼Œèåˆä¸‰è·¯æ£€ç´¢â€”â€”ç±»åˆ«çº§è§„å¾‹ï¼ˆRoute Aï¼‰ã€é—®é¢˜çº§è¿ç§»ï¼ˆRoute Bï¼‰ã€è¯­ä¹‰å›é€€ï¼ˆRoute Cï¼‰ï¼Œå¹¶ç”¨Beta-Binomialæ ¡å‡†çš„å¯æ‰§è¡Œæ€§é¢„æµ‹å™¨æ’åºç­–ç•¥ã€‚  \nğŸ”¸SSRçº¯æµ‹è¯•æ—¶è¿è¡Œï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹æˆ–è®­ç»ƒæ•°æ®ï¼Œä»…ä»¥æŠ½è±¡ç­–ç•¥æç¤ºï¼ˆéæ­¥éª¤ç»†èŠ‚ï¼‰å¼•å¯¼ï¼Œä¿æŒçµæ´»æ€§åŒæ—¶åŒ¹é…æ¨¡å‹æ“ä½œä¼˜åŠ¿ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸ç­–ç•¥ä½¿ç”¨é¢‘ç‡ä¸èƒ½é¢„æµ‹å…¶å¯æ‰§è¡Œæ€§ï¼šé«˜é¢‘äººç±»ç­–ç•¥ï¼ˆå¦‚è§’è¿½é€ï¼‰åœ¨æ¨¡å‹ä¸Šå¸¸ä¸å¯é ï¼Œè€Œä½é¢‘æ¨¡å‹ç­–ç•¥ï¼ˆå¦‚åæ ‡è®¾å®šï¼‰åè€Œæ›´æ˜“æ‰§è¡Œã€‚  \nğŸ”¸å¯æ‰§è¡Œæ€§å…·æœ‰å¼ºæºä¾èµ–æ€§ä¸é¢†åŸŸç‰¹å¼‚æ€§ï¼šåœ¨å‡ ä½•é¢˜ä¸­ï¼Œäººç±»ç­–ç•¥å¹³å‡å¯æ‰§è¡Œæ€§æ˜¾è‘—é«˜äºæ¨¡å‹ç­–ç•¥ï¼›åœ¨ä»£æ•°é¢˜ä¸­åˆ™ç›¸åã€‚  \nğŸ”¸SSRåœ¨å¤šä¸ªåŸºå‡†ï¼ˆHM-ReasoningBenchã€AIME25ã€APEXï¼‰ä¸Šç¨³å®šè¶…è¶Šç›´æ¥æ±‚è§£ã€ä¸Šä¸‹æ–‡å­¦ä¹ åŠå•æºç­–ç•¥å¼•å¯¼ï¼Œæœ€é«˜æå‡13ç‚¹å‡†ç¡®ç‡ï¼ˆAIME25ï¼‰ï¼Œä¸”åœ¨éš¾é¢˜ä¸Šå¢ç›Šæ›´å¤§ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ä¸‰è·¯æ£€ç´¢å‡å¿…è¦ï¼šç§»é™¤é—®é¢˜çº§è¿ç§»ï¼ˆRoute Bï¼‰å¯¼è‡´æœ€å¤§æ€§èƒ½ä¸‹é™ï¼Œå‡¸æ˜¾ç»†ç²’åº¦ä¸Šä¸‹æ–‡é€‚é…çš„å…³é”®ä½œç”¨ã€‚  \nğŸ”¸é”™è¯¯æ¨¡å¼åˆ†æè¡¨æ˜ï¼šäººç±»ç­–ç•¥ä¸»è¦ç¼“è§£ç»“æ„æ€§å¤±è´¥ï¼ˆå¦‚æ¼åˆ†è§£ã€ç¼ºä¸å˜é‡ï¼‰ï¼Œæ¨¡å‹ç­–ç•¥æ›´æ“…ä¿®æ­£ä»£æ•°è®¡ç®—é”™è¯¯ï¼ŒSSRäºŒè€…å…¼é¡¾ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†â€œç­–ç•¥æœ‰æ•ˆæ€§â€ä»é™æ€è¯­ä¹‰æ­£ç¡®æ€§è½¬å‘åŠ¨æ€æ“ä½œå¯è¡Œæ€§ï¼Œé¦–æ¬¡å°†äººç±»ä¸æ¨¡å‹çš„è®¤çŸ¥å·®å¼‚å»ºæ¨¡ä¸ºå¯æ‰§è¡Œæ€§ä¿¡å·æºã€‚å…¶æ–¹æ³•è®ºçªç ´ä½“ç°åœ¨ä¸‰æ–¹é¢ï¼šä¸€æ˜¯æå‡ºå¯æ‰§è¡Œæ€§è¿™ä¸€å¯æµ‹é‡ã€å¯æ ¡å‡†çš„æ“ä½œæ€§æŒ‡æ ‡ï¼›äºŒæ˜¯æ„å»ºé¦–ä¸ªå¸¦äººç±»/æ¨¡å‹åŒæºè§£æ³•çš„ç­–ç•¥çº§é…å¯¹åŸºå‡†HM-ReasoningBenchï¼›ä¸‰æ˜¯è®¾è®¡æºæ„ŸçŸ¥ã€å¤šè·¯å¾„ã€å›¾å¢å¼ºçš„è½»é‡çº§æ£€ç´¢æ¡†æ¶SSRï¼Œä¸å¢åŠ æ¨ç†å¼€é”€å´æ˜¾è‘—æå‡é²æ£’æ€§ã€‚è¯¥è§†è§’æœ‰æœ›æ¨åŠ¨æ¨ç†è¯„ä¼°ä»â€œç­”æ¡ˆå¯¹é”™â€è¿ˆå‘â€œä¸­é—´æŠ½è±¡æ˜¯å¦å¯ç”¨â€ã€‚\n    "
    },
    {
        "title": "Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching",
        "authors": [
            "Roy Miles",
            "Aysim Toker",
            "Andreea-Maria Oncescu",
            "Songcen Xu",
            "Jiankang Deng",
            "Ismail Elezi"
        ],
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22871v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22871v1",
        "summary": "Reasoning with large language models often benefits from generating multiple chains-of-thought, but existing aggregation strategies are typically trajectory-level (e.g., selecting the best trace or voting on the final answer), discarding useful intermediate work from partial or \"nearly correct\" attempts. We propose Stitching Noisy Diffusion Thoughts, a self-consistency framework that turns cheap diffusion-sampled reasoning into a reusable pool of step-level candidates. Given a problem, we (i) sample many diverse, low-cost reasoning trajectories using a masked diffusion language model, (ii) score every intermediate step with an off-the-shelf process reward model (PRM), and (iii) stitch these highest-quality steps across trajectories into a composite rationale. This rationale then conditions an autoregressive (AR) model (solver) to recompute only the final answer. This modular pipeline separates exploration (diffusion) from evaluation and solution synthesis, avoiding monolithic unified hybrids while preserving broad search. Across math reasoning benchmarks, we find that step-level recombination is most beneficial on harder problems, and ablations highlight the importance of the final AR solver in converting stitched but imperfect rationales into accurate answers. Using low-confidence diffusion sampling with parallel, independent rollouts, our training-free framework improves average accuracy by up to 23.8% across six math and coding tasks. At the same time, it achieves up to a 1.8x latency reduction relative to both traditional diffusion models (e.g., Dream, LLaDA) and unified architectures (e.g., TiDAR). Code is available at https://github.com/roymiles/diffusion-stitching.",
        "tag": "è¿‡ç¨‹å¥–åŠ±",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22871v1ã€è¿‡ç¨‹å¥–åŠ±-åä¸ºã€‘Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching.pdf",
        "institution_status": "keep",
        "institution": "åä¸º",
        "first_institution": "åä¸º",
        "institution_category": "å›½å†…å·¥ä¸šç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šTest-Time Scaling with Diffusion Language Models via Reward-Guided Stitching\nğŸŒæ¥æºï¼šarXiv, 2602.22871v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ‰©æ•£æ€ç»´æ‹¼æ¥æå‡æ¨ç†\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸å¢åŠ å»¶è¿Ÿçš„å‰æä¸‹ï¼Œæœ‰æ•ˆåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å¤šæ ·åŒ–ä½†å˜ˆæ‚çš„æ¨ç†è·¯å¾„æ¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†å‡†ç¡®ç‡ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºä¸€ç§è®­ç»ƒæ— å…³ã€æ¨¡å—åŒ–çš„â€œæ‰©æ•£æ€ç»´æ‹¼æ¥â€æ¡†æ¶ï¼Œé€šè¿‡æ­¥éª¤çº§å¥–åŠ±å¼•å¯¼ç­›é€‰ä¸é‡ç»„ï¼Œæ˜¾è‘—æå‡æ•°å­¦ä¸ä»£ç ä»»åŠ¡çš„å‡†ç¡®ç‡-å»¶è¿Ÿå¸•ç´¯æ‰˜å‰æ²¿ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸ä½¿ç”¨ä½ç½®ä¿¡åº¦æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹å¹¶è¡Œé‡‡æ ·å¤šæ¡æ¨ç†è·¯å¾„ï¼Œå…¼é¡¾å¤šæ ·æ€§ä¸ä½æˆæœ¬æ¢ç´¢ã€‚  \nğŸ”¸å¼•å…¥ç°æˆçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å¯¹æ¯æ¡è·¯å¾„ä¸­æ¯ä¸ªä¸­é—´æ­¥éª¤ç‹¬ç«‹æ‰“åˆ†ï¼Œæ„å»ºå…¨å±€é«˜è´¨é‡æ­¥éª¤æ± ã€‚  \nğŸ”¸åŸºäºç½®ä¿¡åº¦é˜ˆå€¼ä¸æœ€ä¼˜è½¨è¿¹é”šå®šç­–ç•¥ï¼Œè·¨è·¯å¾„æ‹¼æ¥é«˜åˆ†æ­¥éª¤å½¢æˆå¤åˆæ¨ç†é“¾ï¼Œå¹¶æ·»åŠ ç½®ä¿¡åº¦æ ‡æ³¨ã€‚  \nğŸ”¸ç”¨è½»é‡è‡ªå›å½’æ±‚è§£å™¨ä»¥åŸå§‹é—®é¢˜å’Œæ‹¼æ¥é“¾ä¸ºæ¡ä»¶ï¼Œä»…é‡æ–°ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œå®ç°çº é”™ä¸æ•´åˆã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸æ­¥éª¤çº§æ‹¼æ¥åœ¨éš¾é¢˜ä¸Šå¢ç›Šæ›´æ˜¾è‘—ï¼Œè¡¨æ˜ä¸­é—´æ­£ç¡®å­æ¨å¯¼æ¯”å®Œæ•´è·¯å¾„æ›´å…·å¤ç”¨ä»·å€¼ã€‚  \nğŸ”¸ARæ±‚è§£å™¨ä¸å¯æˆ–ç¼ºï¼šå³ä½¿æ‹¼æ¥é“¾å­˜åœ¨å†—ä½™æˆ–çŸ›ç›¾ï¼Œå…¶ä»èƒ½é€šè¿‡é‡è®¡ç®—æ¢å¤é«˜ç²¾åº¦ç­”æ¡ˆã€‚  \nğŸ”¸é™ä½æ‰©æ•£é‡‡æ ·ç½®ä¿¡åº¦å¯å¤§å¹…å‡å°‘æ¨ç†æ­¥æ•°ï¼ˆæœ€é«˜é™24.4%ï¼‰ï¼Œè€Œç²¾åº¦å‡ ä¹ä¸å˜ï¼ŒéªŒè¯å™ªå£°é²æ£’æ€§ã€‚  \nğŸ”¸å¹¶è¡Œç”Ÿæˆ+é›†ä¸­æ‹¼æ¥çš„è®¾è®¡ä½¿ç³»ç»Ÿçº¿æ€§æ‰©å±•ï¼ŒNå¢å¤§ä¸å¢åŠ ç«¯åˆ°ç«¯å»¶è¿Ÿï¼Œä¸”åœ¨4æ¡è·¯å¾„æ—¶å·²è¾¾æ€§èƒ½é¥±å’Œã€‚  \nğŸ”¸è¯¥æ–¹æ³•åœ¨6ä¸ªæ•°å­¦ä¸ç¼–ç¨‹åŸºå‡†ä¸Šå¹³å‡å‡†ç¡®ç‡æå‡è¾¾23.8%ï¼Œç«¯åˆ°ç«¯å»¶è¿Ÿæ¯”TiDARç­‰ç»Ÿä¸€æ¶æ„ä½1.8å€ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºæ‰“ç ´â€œè½¨è¿¹å³å•å…ƒâ€çš„å›ºæœ‰èŒƒå¼ï¼Œå°†æ¨ç†è§£è€¦ä¸ºæ¢ç´¢ï¼ˆæ‰©æ•£ï¼‰ã€è¯„ä¼°ï¼ˆPRMï¼‰ã€åˆæˆï¼ˆARï¼‰ä¸‰é˜¶æ®µï¼›å…¶æ ¸å¿ƒæ´è§æ˜¯â€œå±€éƒ¨æ­£ç¡®æ€§å¯è·¨è·¯å¾„è¿ç§»â€ï¼Œé€šè¿‡ç»†ç²’åº¦è¯„åˆ†ä¸æ‹¼æ¥ï¼Œä»¥æå°å¼€é”€æ¿€æ´»äº†è¢«ä¼ ç»Ÿèšåˆç­–ç•¥ä¸¢å¼ƒçš„å¤§é‡ä¸­é—´çŸ¥è¯†ï¼Œä¸ºæµ‹è¯•æ—¶é«˜æ•ˆç¼©æ”¾æä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions",
        "authors": [
            "Yue Xu",
            "Qian Chen",
            "Zizhan Ma",
            "Dongrui Liu",
            "Wenxuan Wang",
            "Xiting Wang",
            "Li Xiong",
            "Wenjie Wang"
        ],
        "categories": [
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22680v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22680v1",
        "summary": "Large language models have enabled agents that reason, plan, and interact with tools and environments to accomplish complex tasks. As these agents operate over extended interaction horizons, their effectiveness increasingly depends on adapting behavior to individual users and maintaining continuity across time, giving rise to personalized LLM-powered agents. In such long-term, user-dependent settings, personalization permeates the entire decision pipeline rather than remaining confined to surface-level generation. This survey provides a capability-oriented review of personalized LLM-powered agents. We organize the literature around four interdependent components: profile modeling, memory, planning, and action execution. Using this taxonomy, we synthesize representative methods and analyze how user signals are represented, propagated, and utilized, highlighting cross-component interactions and recurring design trade-offs. We further examine evaluation metrics and benchmarks tailored to personalized agents, summarize application scenarios spanning general assistance to specialized domains, and outline future directions for research and deployment. By offering a structured framework for understanding and designing personalized LLM-powered agents, this survey charts a roadmap toward more user-aligned, adaptive, robust, and deployable agentic systems, accelerating progress from prototype personalization to scalable real-world assistants.",
        "tag": "Agent è®°å¿†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22680v1ã€Agent è®°å¿†-ä¸Šæµ·ç§‘æŠ€å¤§å­¦ã€‘Toward Personalized LLM-Powered Agents_ Foundations, Evaluation, and Future Directions.pdf",
        "institution_status": "keep",
        "institution": "ä¸Šæµ·ç§‘æŠ€å¤§å­¦ã€åŒæµå¤§å­¦",
        "first_institution": "ä¸Šæµ·ç§‘æŠ€å¤§å­¦",
        "institution_category": "å…¶ä»–æœºæ„",
        "note": "ğŸ“–æ ‡é¢˜ï¼šToward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions\nğŸŒæ¥æºï¼šarXiv, 2602.22680v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºä¸ªæ€§åŒ–LLMæ™ºèƒ½ä½“ç³»ç»Ÿæ¡†æ¶\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ç³»ç»Ÿæ€§åœ°è®¾è®¡ã€è¯„ä¼°ä¸éƒ¨ç½²çœŸæ­£é¢å‘ä¸ªä½“ç”¨æˆ·çš„LLMé©±åŠ¨æ™ºèƒ½ä½“ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºé¦–ä¸ªä»¥èƒ½åŠ›ä¸ºä¸­å¿ƒã€è¦†ç›–å…¨å†³ç­–é“¾è·¯çš„å››ç»´ç»Ÿä¸€æ¡†æ¶ï¼ˆç”¨æˆ·ç”»åƒå»ºæ¨¡ã€è®°å¿†ã€è§„åˆ’ã€åŠ¨ä½œæ‰§è¡Œï¼‰ï¼Œå¹¶å»ºç«‹é…å¥—è¯„ä¼°ä½“ç³»ä¸æœªæ¥æ–¹å‘ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å°†ä¸ªæ€§åŒ–å®šä¹‰ä¸ºè´¯ç©¿ä»£ç†å…¨ç”Ÿå‘½å‘¨æœŸçš„ç³»ç»Ÿçº§å±æ€§ï¼Œè€Œéå±€éƒ¨ç”Ÿæˆè°ƒæ•´ï¼Œæ®æ­¤åˆ’åˆ†profile modelingã€memoryã€planningã€action executionå››å¤§äº’ä¾ç»„ä»¶ã€‚  \nğŸ”¸å›´ç»•æ¯ä¸ªç»„ä»¶ï¼Œç³»ç»Ÿæ¢³ç†ä»£è¡¨æ€§æ–¹æ³•ï¼šå¦‚ç”»åƒå»ºæ¨¡åŒºåˆ†persona-basedä¸response-basedè·¯å¾„ï¼›è®°å¿†æ¨¡å—å¯¹æ¯”æ–‡æœ¬å‹ä¸ç»“æ„åŒ–ï¼ˆå›¾/æ ‘/å‘é‡ï¼‰å­˜å‚¨åŠç›¸ä¼¼æ€§/æ¨ç†é©±åŠ¨æ›´æ–°æœºåˆ¶ï¼›è§„åˆ’åˆ†ä¸ºä¸€æ¬¡æ€§çº¦æŸæ³¨å…¥ä¸åé¦ˆé©±åŠ¨è¿­ä»£ä¸¤ç±»èŒƒå¼ï¼›åŠ¨ä½œæ‰§è¡Œç»†åˆ†ä¸ºé¢„åŠ¨ä½œç­–ç•¥é€‰æ‹©ä¸ååŠ¨ä½œç»“æœä¿®æ­£ã€‚  \nğŸ”¸æ„å»ºå¤šç»´åº¦è¯„ä¼°ä½“ç³»ï¼Œæ¶µç›–æœ‰æ•ˆæ€§ã€é€‚åº”æ€§ã€æ³›åŒ–æ€§ã€é²æ£’æ€§ä¸é£é™©äº”å¤§ç›®æ ‡ï¼Œå¹¶å¯¹åº”è‡ªåŠ¨è¯„åˆ†ã€è§„åˆ™æ ¡éªŒã€LLMè¯„æµ‹å™¨ã€LLM-as-a-judgeå››ç±»èŒƒå¼ï¼Œæ•´åˆäº¤äº’å¯¹é½ä¸ç”¨æˆ·æ›¿ä»£ä¸¤å¤§åŸºå‡†å®¶æ—ã€‚  \nğŸ”¸æŒ‰åº”ç”¨åœºæ™¯æ¨ªå‘æ¢³ç†å¯¹è¯åŠ©æ‰‹ã€æƒ…æ„Ÿé™ªä¼´ã€æ•™è‚²ä»£ç†ã€å†…å®¹åˆ›ä½œã€å§”æ‰˜åŠ©ç†åŠå‚ç›´é¢†åŸŸï¼ˆåŒ»ç–—/é‡‘è/æ³•å¾‹/ç§‘ç ”ï¼‰ä¸­çš„ä¸ªæ€§åŒ–å®è·µä¸æŒ‘æˆ˜ï¼Œå¼ºè°ƒä»»åŠ¡ç‰¹æ€§å¯¹ä¸ªæ€§åŒ–æœºåˆ¶çš„å·®å¼‚åŒ–éœ€æ±‚ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸ä¸ªæ€§åŒ–æ•ˆæœé«˜åº¦ä¾èµ–è·¨ç»„ä»¶ååŒâ€”â€”ä¾‹å¦‚profileå»ºæ¨¡è´¨é‡ç›´æ¥å½±å“memoryæ£€ç´¢ç›¸å…³æ€§ï¼Œè€Œmemoryæ›´æ–°ç­–ç•¥åˆåˆ¶çº¦planningä¸­é•¿æœŸåå¥½å»ºæ¨¡çš„å‡†ç¡®æ€§ã€‚  \nğŸ”¸ç°æœ‰æ–¹æ³•æ™®éé¢ä¸´ä¿¡å·ç¨€ç–æ€§ä¸åŠ¨æ€æ€§çŸ›ç›¾ï¼šå†å²æ•°æ®æ”¯æ’‘ç¨³å®šæ€§ï¼Œäº¤äº’æ•°æ®ä¿éšœæ—¶æ•ˆæ€§ï¼Œä½†äºŒè€…èåˆç¼ºä¹ç»Ÿä¸€å»ºæ¨¡åŸåˆ™ï¼Œæ˜“å¯¼è‡´æ¼‚ç§»æˆ–åƒµåŒ–ã€‚  \nğŸ”¸è¯„ä¼°å­˜åœ¨â€œåˆæˆåå·®â€ï¼šä¸»æµåŸºå‡†å¤šä¾èµ–LLMç”Ÿæˆç”¨æˆ·æ•°æ®ï¼Œéš¾ä»¥åæ˜ çœŸå®äººç±»å¤šæ ·æ€§ä¸ä¸»è§‚æ€§ï¼Œä¸”LLM-as-a-judgeç»“æœä¸å®é™…ç”¨æˆ·æ»¡æ„åº¦ç›¸å…³æ€§å­˜ç–‘ã€‚  \nğŸ”¸éšç§ä¸å¯æ§æ€§å°šæœªå†…ç”Ÿäºæ¶æ„ï¼šå¤šæ•°è®°å¿†ä¸ç”»åƒæ¨¡å—ç¼ºä¹ç”¨æˆ·å¯å®¡è®¡ã€å¯ç¼–è¾‘ã€å¯é—å¿˜çš„è®¾è®¡ï¼Œå¯¼è‡´ä¿¡ä»»ç“¶é¢ˆï¼Œå°¤å…¶åœ¨åŒ»ç–—ã€é‡‘èç­‰é«˜æ•åœºæ™¯ä¸­æˆä¸ºè½åœ°å…³é”®éšœç¢ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡æœ€å¤§åˆ›æ–°åœ¨äºçªç ´ä¼ ç»Ÿâ€œä¸ªæ€§åŒ–å³å¾®è°ƒ/æç¤ºå·¥ç¨‹â€çš„çª„å£å¾„è®¤çŸ¥ï¼Œé¦–æ¬¡å°†ä¸ªæ€§åŒ–å‡ç»´ä¸ºæ™ºèƒ½ä½“ç³»ç»Ÿçš„ç»“æ„æ€§ç‰¹å¾ï¼Œå¹¶ä»¥é—­ç¯è§†è§’ï¼ˆç”¨æˆ·è¯·æ±‚â†’å››ç»„ä»¶ååŒâ†’å“åº”â†’åé¦ˆâ†’ç”»åƒæ›´æ–°ï¼‰æ­ç¤ºå…¶å†…åœ¨æ¼”åŒ–é€»è¾‘ã€‚å…¶æå‡ºçš„å››ç»´æ¡†æ¶ä¸ä»…å…·å¤‡å¼ºè§£é‡Šæ€§ï¼Œæ›´ç›´æŒ‡å½“å‰ç¢ç‰‡åŒ–ç ”ç©¶çš„æ•´åˆç¼ºå£ï¼Œä¸ºåç»­å·¥ä½œæä¾›äº†å¯æ‰©å±•ã€å¯è¯Šæ–­ã€å¯éƒ¨ç½²çš„ç³»ç»Ÿæ€§è“å›¾ã€‚\n    "
    },
    {
        "title": "Towards Better RL Training Data Utilization via Second-Order Rollout",
        "authors": [
            "Zhe Yang",
            "Yudong Wang",
            "Rang Li",
            "Zhifang Sui"
        ],
        "categories": [
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22765v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22765v1",
        "summary": "Reinforcement Learning (RL) has empowered Large Language Models (LLMs) with strong reasoning capabilities, but vanilla RL mainly focuses on generation capability improvement by training with only first-order rollout (generating multiple responses for a question), and we argue that this approach fails to fully exploit the potential of training data because of the neglect of critique capability training. To tackle this problem, we further introduce the concept of second-order rollout (generating multiple critiques for a response) and propose a unified framework for jointly training generation and critique capabilities. Extensive experiments across various models and datasets demonstrate that our approach can utilize training data more effectively than vanilla RL and achieve better performance under the same training data. Additionally, we uncover several insightful findings regarding second-order rollout and critique training, such as the importance of label balance in critique training and the noise problem of outcome-based rewards, which can be mitigated through sampling techniques. Our work offers a preliminary exploration of dynamic data augmentation and joint generation-critique training in RL, providing meaningful inspiration for the further advancement of RL training",
        "tag": "å¼ºåŒ–å­¦ä¹ è®­ç»ƒ",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22765v1ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒ-åŒ—äº¬å¤§å­¦ã€‘Towards Better RL Training Data Utilization via Second-Order Rollout.pdf",
        "institution_status": "keep",
        "institution": "åŒ—äº¬å¤§å­¦ã€å­—èŠ‚è·³åŠ¨",
        "first_institution": "åŒ—äº¬å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šTowards Better RL Training Data Utilization via Second-Order Rollout\nğŸŒæ¥æºï¼šarXiv, 2602.22765v1\n\nç¬”è®°æ ‡é¢˜ï¼šå¼•å…¥äºŒé˜¶å±•å¼€æå‡RLæ•°æ®åˆ©ç”¨\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸å¢åŠ æ ‡æ³¨æ•°æ®çš„å‰æä¸‹ï¼Œæ›´å……åˆ†åœ°æŒ–æ˜å¼ºåŒ–å­¦ä¹ ä¸­å·²æœ‰è®­ç»ƒæ•°æ®çš„æ½œåŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºâ€œäºŒé˜¶å±•å¼€â€æ¦‚å¿µä¸ç”Ÿæˆ-æ‰¹åˆ¤è”åˆè®­ç»ƒæ¡†æ¶GC-RLï¼Œæ˜¾è‘—æå‡åŒä¸€è®­ç»ƒæ•°æ®ä¸‹çš„ç”Ÿæˆä¸æ‰¹åˆ¤åŒèƒ½åŠ›ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å®šä¹‰äºŒé˜¶å±•å¼€ï¼šåœ¨ä¼ ç»Ÿä¸€é˜¶å±•å¼€ï¼ˆå¯¹ä¸€ä¸ªé—®é¢˜é‡‡æ ·å¤šä¸ªå›ç­”ï¼‰åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å¯¹æ¯ä¸ª<é—®é¢˜,å›ç­”>å¯¹é‡‡æ ·å¤šä¸ªæ‰¹åˆ¤ï¼Œå½¢æˆç¬¬äºŒå±‚ç­–ç•¥è¾“å‡ºã€‚  \nğŸ”¸æ„å»ºåŠ¨æ€æ•°æ®ç¼“å­˜æœºåˆ¶ï¼šé€šè¿‡æ•°æ®è¿‡æ»¤å™¨ä»ä¸€é˜¶å±•å¼€ç»“æœä¸­ç­›é€‰å‡ºæ­£ç¡®ä¸é”™è¯¯å›ç­”å„ä¸€ä¸ªï¼Œå­˜å…¥é—®ç­”ç¼“å­˜ï¼Œä¾›äºŒé˜¶å±•å¼€ä½¿ç”¨ã€‚  \nğŸ”¸æ··åˆ rollout è®­ç»ƒï¼šå°†ä¸€é˜¶ï¼ˆå›ç­”ï¼‰ä¸äºŒé˜¶ï¼ˆæ‰¹åˆ¤ï¼‰é‡‡æ ·ç»Ÿä¸€å»ºæ¨¡ï¼Œå…±äº«åŒä¸€ç­–ç•¥æ¨¡å‹ï¼Œå¹¶ç”¨GRPOç®—æ³•è”åˆæ›´æ–°ã€‚  \nğŸ”¸å†·å¯åŠ¨è®¾è®¡ï¼šå…ˆç”¨GPT-5è’¸é¦é«˜è´¨é‡æ‰¹åˆ¤æ•°æ®å¹¶è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œè§£å†³åŸºåº§æ¨¡å‹æŒ‡ä»¤éµå¾ªå¼±ã€æ‰¹åˆ¤æ ¼å¼æ··ä¹±é—®é¢˜ã€‚  \nğŸ”¸å¥–åŠ±è®¾è®¡åˆ†å±‚ï¼šå›ç­”é‡‡ç”¨ç¡®å®šæ€§è§„åˆ™å¥–åŠ±ï¼›æ‰¹åˆ¤ä»…åŸºäºæœ€ç»ˆäºŒå…ƒåˆ¤æ–­ç»™äºˆç»“æœå¥–åŠ±ï¼Œå¹¶å¼•å…¥åŠ æƒä¸å»å™ªæœºåˆ¶ç¼“è§£å™ªå£°ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸æ•°æ®è¿‡æ»¤å™¨è‡³å…³é‡è¦ï¼šéšæœºé‡‡æ ·å¯¼è‡´æ‰¹åˆ¤æ•°æ®ä¸¥é‡æ ‡ç­¾ä¸å¹³è¡¡ï¼ˆé”™è¯¯å›ç­”è¿œå¤šäºæ­£ç¡®ï¼‰ï¼Œä½¿æ¨¡å‹åå‘åˆ¤é”™ï¼›è¿‡æ»¤å™¨å¼ºåˆ¶1:1é…æ¯”åæ€§èƒ½æœ€ä¼˜ã€‚  \nğŸ”¸æ‰¹åˆ¤å¥–åŠ±å­˜åœ¨å›ºæœ‰å™ªå£°ï¼šå› æ— æ³•éªŒè¯ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä»…é ç»“æœå¥–åŠ±æ˜“è¯¯å¯¼ï¼›å¤šè½®è‡ªä¿®æ­£é‡‡æ ·å¯æœ‰æ•ˆå»å™ªï¼Œæå‡ç”Ÿæˆä¸æ‰¹åˆ¤åŒç²¾åº¦ã€‚  \nğŸ”¸åŠ¨æ€æ•°æ®ä¼˜äºé™æ€æ•°æ®ï¼šåœ¨GC-RLä¸­ï¼Œæ¨¡å‹è‡ªç”Ÿæˆçš„å›ç­”ä½œä¸ºæ‰¹åˆ¤è¾“å…¥æ•ˆæœæ›´å¥½ï¼›ä½†åœ¨çº¯æ‰¹åˆ¤è®­ç»ƒï¼ˆC-RLï¼‰ä¸­ï¼Œé™æ€é¢„ç½®æ•°æ®æ›´ç¨³å®šï¼Œé¿å…å¥–åŠ±ä½œå¼Šã€‚  \nğŸ”¸å¥–åŠ±å‡½æ•°å¯è°ƒæ§æ‰¹åˆ¤è¡Œä¸ºï¼šè°ƒæ•´æ­£è´Ÿæ ·æœ¬å¥–åŠ±æƒé‡ï¼Œèƒ½ç²¾ç»†æ§åˆ¶æ¨¡å‹çš„ç²¾ç¡®ç‡ä¸å¬å›ç‡å€¾å‘ï¼Œå®ç°ä»»åŠ¡é€‚é…çš„æ‰¹åˆ¤ç­–ç•¥å®šåˆ¶ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°æ€§åœ¨äºå°†â€œæ‰¹åˆ¤â€ä»è¾…åŠ©æ¨¡å—å‡æ ¼ä¸ºä¸â€œç”Ÿæˆâ€å¯¹ç­‰çš„æ ¸å¿ƒè®­ç»ƒç›®æ ‡ï¼Œå¹¶é€šè¿‡äºŒé˜¶å±•å¼€å®ç°é›¶æˆæœ¬æ•°æ®å¢å¼ºâ€”â€”æ‰€æœ‰æ‰¹åˆ¤æ•°æ®å‡ç”±æ¨¡å‹è‡ªèº«åœ¨ä¸€é˜¶å±•å¼€ç»“æœä¸ŠåŠ¨æ€ç”Ÿæˆï¼Œæ— éœ€äººå·¥æ ‡æ³¨æˆ–é¢å¤–æ•°æ®æºã€‚å…¶æ¡†æ¶æœ¬è´¨æ˜¯éšå¼å»ºæ¨¡ç”Ÿæˆä¸æ‰¹åˆ¤çš„èƒ½åŠ›è€¦åˆï¼Œå®éªŒè¯æ˜è”åˆè®­ç»ƒä¸ä»…ä¸äº’æ–¥ï¼Œåè€Œç›¸äº’å¢ç›Šï¼Œä¸ºLLMå¼ºåŒ–å­¦ä¹ æä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "Towards Faithful Industrial RAG: A Reinforced Co-adaptation Framework for Advertising QA",
        "authors": [
            "Wenwei Li",
            "Ming Xu",
            "Tianle Xia",
            "Lingxiang Hu",
            "Yiding Sun",
            "Linfang Shang",
            "Liqun Liu",
            "Peng Shu",
            "Huan Yu",
            "Jie Jiang"
        ],
        "categories": [
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22584v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22584v1",
        "summary": "Industrial advertising question answering (QA) is a high-stakes task in which hallucinated content, particularly fabricated URLs, can lead to financial loss, compliance violations, and legal risk. Although Retrieval-Augmented Generation (RAG) is widely adopted, deploying it in production remains challenging because industrial knowledge is inherently relational, frequently updated, and insufficiently aligned with generation objectives. We propose a reinforced co-adaptation framework that jointly optimizes retrieval and generation through two components: (1) Graph-aware Retrieval (GraphRAG), which models entity-relation structure over a high-citation knowledge subgraph for multi-hop, domain-specific evidence selection; and (2) evidence-constrained reinforcement learning via Group Relative Policy Optimization (GRPO) with multi-dimensional rewards covering faithfulness, style compliance, safety, and URL validity. Experiments on an internal advertising QA dataset show consistent gains across expert-judged dimensions including accuracy, completeness, and safety, while reducing the hallucination rate by 72\\%. A two-week online A/B test demonstrates a 28.6\\% increase in like rate, a 46.2\\% decrease in dislike rate, and a 92.7\\% reduction in URL hallucination. The system has been running in production for over half a year and has served millions of QA interactions.",
        "tag": "RAG æ£€ç´¢ä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22584v1ã€RAG æ£€ç´¢ä¼˜åŒ–-è…¾è®¯ã€‘Towards Faithful Industrial RAG_ A Reinforced Co-adaptation Framework for Advertising QA.pdf",
        "institution_status": "keep",
        "institution": "è…¾è®¯",
        "first_institution": "è…¾è®¯",
        "institution_category": "å›½å†…å·¥ä¸šç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šTowards Faithful Industrial RAG: A Reinforced Co-adaptation Framework for Advertising QA\nğŸŒæ¥æºï¼šarXiv, 2602.22584v1\n\nç¬”è®°æ ‡é¢˜ï¼šå·¥ä¸šRAGå¯ä¿¡ååŒä¼˜åŒ–\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨é«˜é£é™©å·¥ä¸šå¹¿å‘Šé—®ç­”åœºæ™¯ä¸­ï¼Œç³»ç»Ÿæ€§é™ä½å¤§æ¨¡å‹å¹»è§‰ï¼ˆå°¤å…¶æ˜¯ä¼ªé€ URLï¼‰ï¼ŒåŒæ—¶ä¿éšœç”Ÿæˆç­”æ¡ˆçš„äº‹å®å¿ å®æ€§ã€åˆè§„æ€§ä¸ä¸šåŠ¡å®ç”¨æ€§ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºé¦–ä¸ªé¢å‘å·¥ä¸šå¹¿å‘ŠQAçš„æ£€ç´¢-ç”Ÿæˆè”åˆå¼ºåŒ–ååŒæ¡†æ¶ï¼Œé€šè¿‡å›¾æ„ŸçŸ¥æ£€ç´¢ä¸å¤šç»´å¥–åŠ±çº¦æŸçš„RLè”åˆä¼˜åŒ–ï¼Œæ˜¾è‘—æŠ‘åˆ¶å¹»è§‰å¹¶æå‡çº¿ä¸Šç”¨æˆ·æ»¡æ„åº¦ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºå›¾æ„ŸçŸ¥æ£€ç´¢ï¼ˆGraphRAGï¼‰æ¨¡å—ï¼ŒåŸºäºé«˜é¢‘å¼•ç”¨çŸ¥è¯†å­å›¾å»ºæ¨¡å®ä½“-å…³ç³»ç»“æ„ï¼Œæ”¯æŒå¤šè·³æ¨ç†ï¼Œå¹¶é‡‡ç”¨å¹¶è¡Œé€šé“ï¼ˆGraphRAG+ä¼ ç»ŸRAGï¼‰å…¼é¡¾æ•ˆç‡ä¸å¬å›ã€‚  \nğŸ”¸è®¾è®¡è¯æ®çº¦æŸçš„å¼ºåŒ–å­¦ä¹ ç”Ÿæˆæ¨¡å—ï¼Œä»¥Qwen3-32Bä¸ºåŸºåº§ï¼Œé‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•ï¼Œåœ¨æ— ç‹¬ç«‹criticæ¨¡å‹ä¸‹ç¨³å®šè®­ç»ƒã€‚  \nğŸ”¸å®šä¹‰å››ç»´å¥–åŠ±å‡½æ•°ï¼šè¯æ®å¿ å®æ€§ï¼ˆLLM-as-judgeæ¯”å¯¹ï¼‰ã€é£æ ¼åˆè§„æ€§ï¼ˆå¹¿å‘Šé¢†åŸŸè¯­æ°”/æ ¼å¼ï¼‰ã€å®‰å…¨æ€§ï¼ˆæ”¿ç­–è¿è§„æ£€æµ‹ï¼‰ã€URLæœ‰æ•ˆæ€§ï¼ˆé“¾æ¥å­˜åœ¨æ€§+HTTPçŠ¶æ€ç éªŒè¯ï¼‰ã€‚  \nğŸ”¸å¼•å…¥é«˜å¼•ç”¨çŸ¥è¯†åº“åŠ¨æ€è£å‰ªæœºåˆ¶ï¼Œä¾æ®çº¿ä¸ŠæŸ¥è¯¢æ—¥å¿—çš„â€œå¼•ç”¨çƒ­åº¦â€è‡ªåŠ¨ç­›é€‰Top-N%çŸ¥è¯†èŠ‚ç‚¹ï¼Œå¹³è¡¡å›¾è®¡ç®—å¼€é”€ä¸è¯­ä¹‰è¦†ç›–èƒ½åŠ›ã€‚  \nğŸ”¸å…¨æµç¨‹éƒ¨ç½²å®‰å…¨æŠ¤æ ï¼šæµå¼ç”Ÿæˆä¸­å®æ—¶æ£€æµ‹å¹¶è¿‡æ»¤å¹»è§‰URLä¸è¿è§„å†…å®¹ï¼Œç¡®ä¿æœ€ç»ˆè¾“å‡ºé›¶URLå¹»è§‰ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸GraphRAGä½¿æ¯æŸ¥è¯¢æœ‰æ•ˆçŸ¥è¯†ç‰‡æ®µæ•°æå‡61.5%ï¼Œå¬å›æœ‰æ•ˆæ€§è¾¾90.5%ï¼Œæ˜¾è‘—ä¼˜äºBase RAGï¼ˆ73.6%ï¼‰ï¼ŒéªŒè¯å…¶å¯¹å¤æ‚æµç¨‹çŸ¥è¯†çš„å»ºæ¨¡ä¼˜åŠ¿ã€‚  \nğŸ”¸RLå¾®è°ƒä½¿å¹»è§‰ç‡ç›¸å¯¹ä¸‹é™72%ï¼ˆ0.0047â†’0.0013ï¼‰ï¼Œä¸”åœ¨FaithEvalåäº‹å®/æ— ç­”æ¡ˆåœºæ™¯ä¸‹æ‹’ç»èƒ½åŠ›æ›´å¼ºï¼Œè¯æ˜å…¶æ³›åŒ–å¿ å®æ€§æå‡ã€‚  \nğŸ”¸çº¿ä¸ŠA/Bæµ‹è¯•æ˜¾ç¤ºï¼šå–œæ¬¢ç‡+28.6%ï¼Œä¸å–œæ¬¢ç‡âˆ’46.2%ï¼ŒURLå¹»è§‰ç‡âˆ’92.7%ï¼Œè¯å®ç”¨æˆ·æ„ŸçŸ¥è´¨é‡ä¸å¯é æ€§åŒé‡æå‡ã€‚  \nğŸ”¸å¤šç»´å¥–åŠ±è®­ç»ƒä¸­ï¼Œå¿ å®æ€§ä¸URLæœ‰æ•ˆæ€§æ”¶æ•›æœ€å¿«ï¼Œé£æ ¼ä¸å®‰å…¨æ€§æå‡è¾ƒç¼“ï¼Œåæ˜ å·¥ä¸šåˆè§„è¦æ±‚éœ€æ›´ç²¾ç»†å»ºæ¨¡ã€‚  \nğŸ”¸ç«¯åˆ°ç«¯å»¶è¿Ÿ3.1ç§’ï¼ˆ+24%ï¼‰ï¼Œä»åœ¨å¯æ¥å—èŒƒå›´ï¼›GraphRAGå•æ¨¡å—è€—æ—¶æœ€é«˜ï¼ˆ852msï¼‰ï¼ŒéªŒè¯é«˜å¼•ç”¨å­å›¾è£å‰ªè®¾è®¡çš„å¿…è¦æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡åˆ›æ–°æ€§åœ¨äºæ‰“ç ´RAGä¸­æ£€ç´¢ä¸ç”Ÿæˆçš„å‰²è£‚èŒƒå¼ï¼Œé¦–æ¬¡å°†å›¾ç»“æ„å…ˆéªŒçŸ¥è¯†å»ºæ¨¡ä¸å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ·±åº¦è€¦åˆï¼Œä¸”æ‰€æœ‰è®¾è®¡å‡é”šå®šå·¥ä¸šè½åœ°ç—›ç‚¹ï¼šç”¨å¼•ç”¨çƒ­åº¦é©±åŠ¨å›¾æ„å»ºè§£å†³æ›´æ–°æ»åï¼Œç”¨GRPOè§„é¿criticä¸ç¨³å®šæ€§ï¼Œç”¨URLæœ‰æ•ˆæ€§ç¡¬çº¦æŸç›´å‡»é«˜å±å¹»è§‰ã€‚å…¶ä»·å€¼ä¸ä»…åœ¨äºæŠ€æœ¯ç»„åˆï¼Œæ›´åœ¨äºä¸ºé«˜é£é™©å‚ç›´é¢†åŸŸRAGæä¾›äº†â€œå¯éªŒè¯ã€å¯éƒ¨ç½²ã€å¯åº¦é‡â€çš„å¯ä¿¡èŒƒå¼ã€‚\n    "
    },
    {
        "title": "Vectorizing the Trie: Efficient Constrained Decoding for LLM-based Generative Retrieval on Accelerators",
        "authors": [
            "Zhengyang Su",
            "Isay Katsman",
            "Yueqi Wang",
            "Ruining He",
            "Lukasz Heldt",
            "Raghunandan Keshavan",
            "Shao-Chuan Wang",
            "Xinyang Yi",
            "Mingyan Gao",
            "Onkar Dalal",
            "Lichan Hong",
            "Ed Chi",
            "Ningren Han"
        ],
        "categories": [
            "cs.IR",
            "cs.CL",
            "cs.LG"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22647v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22647v1",
        "summary": "Generative retrieval has emerged as a powerful paradigm for LLM-based recommendation. However, industrial recommender systems often benefit from restricting the output space to a constrained subset of items based on business logic (e.g. enforcing content freshness or product category), which standard autoregressive decoding cannot natively support. Moreover, existing constrained decoding methods that make use of prefix trees (Tries) incur severe latency penalties on hardware accelerators (TPUs/GPUs). In this work, we introduce STATIC (Sparse Transition Matrix-Accelerated Trie Index for Constrained Decoding), an efficient and scalable constrained decoding technique designed specifically for high-throughput LLM-based generative retrieval on TPUs/GPUs. By flattening the prefix tree into a static Compressed Sparse Row (CSR) matrix, we transform irregular tree traversals into fully vectorized sparse matrix operations, unlocking massive efficiency gains on hardware accelerators. We deploy STATIC on a large-scale industrial video recommendation platform serving billions of users. STATIC produces significant product metric impact with minimal latency overhead (0.033 ms per step and 0.25% of inference time), achieving a 948x speedup over a CPU trie implementation and a 47-1033x speedup over a hardware-accelerated binary-search baseline. Furthermore, the runtime overhead of STATIC remains extremely low across a wide range of practical configurations. To the best of our knowledge, STATIC enables the first production-scale deployment of strictly constrained generative retrieval. In addition, evaluation on academic benchmarks demonstrates that STATIC can considerably improve cold-start performance for generative retrieval. Our code is available at https://github.com/youtube/static-constraint-decoding.",
        "tag": "RAG æ£€ç´¢ä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22647v1ã€RAG æ£€ç´¢ä¼˜åŒ–-è°·æ­Œã€‘Vectorizing the Trie_ Efficient Constrained Decoding for LLM-based Generative Retrieval on Accelerators.pdf",
        "institution_status": "keep",
        "institution": "è°·æ­Œã€è€¶é²å¤§å­¦",
        "first_institution": "è°·æ­Œ",
        "institution_category": "å›½å¤–å·¥ä¸šç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šVectorizing the Trie: Efficient Constrained Decoding for LLM-based Generative Retrieval on Accelerators\nğŸŒæ¥æºï¼šarXiv, 2602.22647v1\n\nç¬”è®°æ ‡é¢˜ï¼šå‘é‡åŒ–Trieå®ç°é«˜æ•ˆçº¦æŸè§£ç \n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨TPU/GPUç­‰ç¡¬ä»¶åŠ é€Ÿå™¨ä¸Šå®ç°ä½å»¶è¿Ÿã€é«˜ååçš„LLMç”Ÿæˆå¼æ£€ç´¢çº¦æŸè§£ç ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºSTATICæ¡†æ¶ï¼Œå°†å‰ç¼€æ ‘ï¼ˆTrieï¼‰çº¦æŸè½¬åŒ–ä¸ºé™æ€ç¨€ç–çŸ©é˜µè¿ç®—ï¼Œé¦–æ¬¡å®ç°å·¥ä¸šçº§ä¸¥æ ¼çº¦æŸç”Ÿæˆå¼æ£€ç´¢çš„é›¶æ˜¾è‘—å»¶è¿Ÿéƒ¨ç½²ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å°†çº¦æŸè¯æ±‡é›†æ„å»ºçš„Trieç»“æ„ç¦»çº¿æ‰å¹³åŒ–ä¸ºå‹ç¼©ç¨€ç–è¡Œï¼ˆCSRï¼‰æ ¼å¼çš„è½¬ç§»çŸ©é˜µï¼Œæ¶ˆé™¤æŒ‡é’ˆè·³è½¬ã€‚  \nğŸ”¸è®¾è®¡åˆ†æ”¯æ— å…³ï¼ˆbranch-freeï¼‰çš„å‘é‡åŒ–èŠ‚ç‚¹è½¬ç§»æ ¸ï¼ˆVNTKï¼‰ï¼Œé€šè¿‡åŠ¨æ€åˆ‡ç‰‡+æ©ç æ¸…æ´—å®ç°å›ºå®šè®¡ç®—å›¾ï¼Œé€‚é…XLA/Inductorç¼–è¯‘ã€‚  \nğŸ”¸é‡‡ç”¨æ··åˆç­–ç•¥ï¼šå¯¹æµ…å±‚ï¼ˆå‰d=2æ­¥ï¼‰ä½¿ç”¨ç¨ å¯†å¸ƒå°”å¼ é‡æ©ç å®ç°O(1)æŸ¥è¡¨ï¼›æ·±å±‚åˆ™ä¾èµ–CSRç¨€ç–çŸ©é˜µå‘é‡åŒ–æŸ¥æ‰¾ã€‚  \nğŸ”¸å¼•å…¥å †å CSRå†…å­˜å¸ƒå±€ï¼Œå°†åˆ—ç´¢å¼•ä¸å€¼åˆå¹¶å­˜å‚¨ï¼Œä½¿å•æ¬¡å†…å­˜äº‹åŠ¡åŒæ—¶è·å–token IDå’Œä¸‹ä¸€èŠ‚ç‚¹IDï¼Œå‡å°‘éšæœºè®¿å­˜ã€‚  \nğŸ”¸åœ¨è®¾å¤‡HBMä¸­å…¨å‰¯æœ¬å­˜å‚¨ç¨€ç–çŸ©é˜µï¼Œé¿å…è·¨èŠ¯ç‰‡é€šä¿¡ï¼Œä¿éšœçº¿æ€§æ‰©å±•æ€§ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸STATICåœ¨YouTubeçœŸå®åœºæ™¯ä¸‹ä»…å¢åŠ 0.033ms/æ­¥å»¶è¿Ÿï¼ˆå æ¨ç†æ€»æ—¶é•¿0.25%ï¼‰ï¼Œè¾ƒCPU Trieæé€Ÿ948Ã—ï¼Œè¾ƒæœ€ä¼˜äºŒåˆ†æœç´¢åŸºçº¿ï¼ˆPPV Exactï¼‰æé€Ÿ1033Ã—ã€‚  \nğŸ”¸å†…å­˜å¼€é”€å¯æ§ï¼šæ¯ç™¾ä¸‡çº¦æŸé¡¹çº¦90MB HBMï¼Œ2000ä¸‡é¡¹ä»…éœ€çº¦1.5GBï¼Œè¿œä½äºç¨ å¯†æ©ç çš„PBçº§éœ€æ±‚ã€‚  \nğŸ”¸å»¶è¿Ÿå‡ ä¹ä¸éšçº¦æŸé›†å¤§å°|C|å¢é•¿ï¼ˆO(1) I/Oå¤æ‚åº¦ï¼‰ï¼Œè€ŒPPVç­‰æ–¹æ³•å‘ˆO(log|C|)å¢é•¿ï¼Œåœ¨|C|=1e8æ—¶å·®è·è¾¾ç™¾å€ã€‚  \nğŸ”¸å»¶è¿Ÿå¯¹è¯­ä¹‰IDè¯è¡¨å¤§å°|V|ä¹Ÿè¿‘ä¼¼æ’å®šï¼Œå› æµ…å±‚ç”¨ç¨ å¯†æ©ç ã€æ·±å±‚æœ€å¤§åˆ†æ”¯å› å­å®é™…å¾ˆå°ã€‚  \nğŸ”¸åœ¨çº¿A/Bæµ‹è¯•æ˜¾ç¤ºï¼Œâ€œ7å¤©æ–°é²œåº¦â€çº¦æŸä½¿æ–°é²œè§†é¢‘è§‚çœ‹é‡æå‡5.1%ï¼ŒCTRä¸ç”¨æˆ·æ»¡æ„åº¦å‡æ˜¾è‘—ä¸Šå‡ï¼›å†·å¯åŠ¨å®éªŒRecall@1æœ€é«˜æå‡è¶…4å€ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºæ·±åˆ»æ´å¯Ÿç¡¬ä»¶ç“¶é¢ˆæœ¬è´¨â€”â€”éç®—æ³•é€»è¾‘ï¼Œè€Œæ˜¯å†…å­˜è®¿é—®æ¨¡å¼ä¸ç¼–è¯‘èŒƒå¼å†²çªã€‚å®ƒè·³å‡ºä¼ ç»Ÿæ ‘éå†æ€ç»´ï¼Œç”¨çº¿æ€§ä»£æ•°é‡æ„çº¦æŸè§£ç ï¼Œå°†â€œä¸å¯ç¼–è¯‘çš„åŠ¨æ€æ§åˆ¶æµâ€è½¬åŒ–ä¸ºâ€œå¯èåˆçš„é™æ€å¼ é‡æ“ä½œâ€ï¼Œæ˜¯ç³»ç»Ÿä¸ç®—æ³•æ·±åº¦ååŒçš„å…¸èŒƒã€‚å…¶CSR+VNTK+æ··åˆæ©ç è®¾è®¡ï¼Œå…¼å…·ç†è®ºç®€æ´æ€§ä¸å·¥ç¨‹é²æ£’æ€§ï¼Œä¸ºå¤§æ¨¡å‹è½åœ°å·¥ä¸šæ¨èæ ‘ç«‹äº†æ–°æ ‡æ†ã€‚\n    "
    },
    {
        "title": "dLLM: Simple Diffusion Language Modeling",
        "authors": [
            "Zhanhui Zhou",
            "Lingjie Chen",
            "Hanghang Tong",
            "Dawn Song"
        ],
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22661v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22661v1",
        "summary": "Although diffusion language models (DLMs) are evolving quickly, many recent models converge on a set of shared components. These components, however, are distributed across ad-hoc research codebases or lack transparent implementations, making them difficult to reproduce or extend. As the field accelerates, there is a clear need for a unified framework that standardizes these common components while remaining flexible enough to support new methods and architectures.\n  To address this gap, we introduce dLLM, an open-source framework that unifies the core components of diffusion language modeling -- training, inference, and evaluation -- and makes them easy to customize for new designs. With dLLM, users can reproduce, finetune, deploy, and evaluate open-source large DLMs such as LLaDA and Dream through a standardized pipeline. The framework also provides minimal, reproducible recipes for building small DLMs from scratch with accessible compute, including converting any BERT-style encoder or autoregressive LM into a DLM. We also release the checkpoints of these small DLMs to make DLMs more accessible and accelerate future research.",
        "tag": "æ‰©æ•£è¯­è¨€æ¨¡å‹æ¡†æ¶",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22661v1ã€æ‰©æ•£è¯­è¨€æ¨¡å‹æ¡†æ¶-åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡ã€‘dLLM_ Simple Diffusion Language Modeling.pdf",
        "institution_status": "keep",
        "institution": "åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡ã€ä¼Šåˆ©è¯ºä¼Šå¤§å­¦å„å·´çº³ - é¦™æ§Ÿåˆ†æ ¡",
        "first_institution": "åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡",
        "institution_category": "å›½å¤–å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šdLLM: Simple Diffusion Language Modeling\nğŸŒæ¥æºï¼šarXiv, 2602.22661v1\n\nç¬”è®°æ ‡é¢˜ï¼šç»Ÿä¸€æ‰©æ•£è¯­è¨€å»ºæ¨¡æ¡†æ¶\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è§£å†³å½“å‰æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMsï¼‰å› ä»£ç åˆ†æ•£ã€å®ç°ä¸é€æ˜è€Œå¯¼è‡´çš„å¤ç°éš¾ã€æ¯”è¾ƒéš¾ã€æ‰©å±•éš¾é—®é¢˜ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºäº†dLLMâ€”â€”é¦–ä¸ªå¼€æºã€æ¨¡å—åŒ–ã€ç«¯åˆ°ç«¯ç»Ÿä¸€çš„æ‰©æ•£è¯­è¨€å»ºæ¨¡æ¡†æ¶ï¼Œæ¶µç›–è®­ç»ƒã€æ¨ç†ä¸è¯„ä¼°å…¨æµç¨‹ï¼Œå¹¶æä¾›è½»é‡çº§ä»é›¶æ„å»ºå’Œæ¨¡å‹è½¬æ¢æ–¹æ¡ˆã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡ç»Ÿä¸€Traineræ¥å£ï¼Œæ”¯æŒMasked Diffusionï¼ˆMDLMï¼‰å’ŒBlock Diffusionï¼ˆBD3LMï¼‰ç­‰ä¸»æµç›®æ ‡ï¼Œé€šè¿‡æ¨¡å—åŒ–å°è£…ï¼ˆå¦‚MDLMTrainer/BD3LMTrainerï¼‰å®ç°è®­ç»ƒé€»è¾‘ä¸æ¨¡å‹æ¶æ„è§£è€¦ã€‚  \nğŸ”¸æ„å»ºè½»é‡çº§SampleræŠ½è±¡å±‚ï¼Œä»¥plug-and-playæ–¹å¼è§£è€¦æ¨¡å‹ä¸æ¨ç†ç®—æ³•ï¼Œæ”¯æŒFast-dLLMç­‰é«˜æ•ˆè§£ç å™¨æ— ç¼æ›¿æ¢ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹ä»£ç ã€‚  \nğŸ”¸æ‰©å±•lm-evaluation-harnessï¼Œæ„å»ºé«˜ä¿çœŸè¯„ä¼°ç®¡é“ï¼Œä¸¥æ ¼å¤ç°å„æ¨¡å‹å®˜æ–¹é¢„å¤„ç†ã€è§£ç å‚æ•°ä¸åå¤„ç†æµç¨‹ï¼Œç¡®ä¿ç»“æœå¯æ¯”æ€§ã€‚  \nğŸ”¸æä¾›ä¸¤ç±»â€œå¼€ç®±å³ç”¨â€çš„å°è§„æ¨¡DLMæ„å»ºè·¯å¾„ï¼šä¸€æ˜¯å°†BERTå¼ç¼–ç å™¨å¾®è°ƒä¸ºå¯¹è¯å‹DLMï¼ˆBERT-Chatï¼‰ï¼ŒäºŒæ˜¯å°†è‡ªå›å½’LMï¼ˆå¦‚Qwenï¼‰é€šè¿‡SFTç›´æ¥è½¬ä¸ºDLMï¼ˆTiny-A2Dï¼‰ï¼Œå‡æ— éœ€æ¶æ„ä¿®æ”¹æˆ–æŒç»­é¢„è®­ç»ƒã€‚  \nğŸ”¸æ‰€æœ‰ç»„ä»¶åŸºäºHuggingFaceç”Ÿæ€ï¼ˆTransformersã€Accelerateã€PEFTã€DeepSpeedï¼‰ï¼Œå…¼é¡¾æ˜“ç”¨æ€§ä¸å¤§æ¨¡å‹å¯æ‰©å±•æ€§ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸dLLMæˆåŠŸå¤ç°LLaDAä¸Dreamç­‰ä¸»æµDLMçš„å®˜æ–¹è¯„æµ‹ç»“æœï¼ˆè¯¯å·®<1.5%ï¼‰ï¼ŒéªŒè¯äº†å…¶è¯„ä¼°ç®¡é“çš„é«˜ä¿çœŸæ€§ã€‚  \nğŸ”¸Fast-dLLMåœ¨dLLMä¸­å®ç°åï¼Œæ¨ç†é€Ÿåº¦æå‡æœ€é«˜è¾¾11Ã—ï¼Œä¸”ç²¾åº¦åŸºæœ¬æ— æŸï¼Œè¯å®æ¡†æ¶å¯¹é«˜æ•ˆç®—æ³•çš„è‰¯å¥½æ”¯æŒã€‚  \nğŸ”¸MDLMå¾®è°ƒå¯æ˜¾è‘—æå‡å¤§DLMçš„æ¨ç†èƒ½åŠ›ï¼ˆå¦‚LLaDA-Instructåœ¨GSM8Kä¸Š+0.68%ï¼‰ï¼Œä½†Baseæ¨¡å‹åœ¨OODä»»åŠ¡ä¸Šå­˜åœ¨é€€åŒ–ç°è±¡ï¼Œæç¤ºSFTéœ€ä»»åŠ¡é€‚é…ã€‚  \nğŸ”¸BERT-Chatåœ¨é›¶æ¶æ„æ”¹åŠ¨ä¸‹è¶…è¶ŠGPT-2ç³»åˆ—ï¼Œè¯æ˜ç¼–ç å™¨ç»“æ„å…·å¤‡ç”Ÿæˆæ½œåŠ›ï¼›Tiny-A2Dä¸­BD3LMå˜ä½“åœ¨HumanEvalä¸Šåè¶…åŸARåŸºçº¿ï¼ŒéªŒè¯å—æ‰©æ•£å¯¹ä»£ç ç”Ÿæˆçš„ä¼˜åŠ¿ã€‚  \nğŸ”¸æ‰€æœ‰å°DLMå‡ä»…éœ€å•é˜¶æ®µSFTå®Œæˆè½¬æ¢ï¼Œè®­ç»ƒæˆæœ¬ä½ï¼ˆå¦‚BERT-Chatç”¨8Ã—A100è®­ç»ƒ10è½®ï¼‰ï¼Œå¤§å¹…é™ä½DLMå…¥é—¨é—¨æ§›ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œæ ¸å¿ƒåˆ›æ–°åœ¨äºç³»ç»Ÿæ€§å·¥ç¨‹æŠ½è±¡ï¼šä¸æ˜¯æå‡ºæ–°æ¨¡å‹ï¼Œè€Œæ˜¯è¯†åˆ«å‡ºDLMç ”ç©¶ä¸­é‡å¤é€ è½®çš„å…±æ€§ç—›ç‚¹ï¼ˆè®­ç»ƒç›®æ ‡ç¢ç‰‡åŒ–ã€æ¨ç†APIä¸ä¸€è‡´ã€è¯„æµ‹ä¸å¯æ§ï¼‰ï¼Œå¹¶ä»¥å·¥ä¸šçº§è½¯ä»¶å·¥ç¨‹æ€ç»´æ„å»ºæ ‡å‡†åŒ–æ¥å£ã€‚å…¶â€œæœ€å°å¯è¡Œè½¬æ¢â€ç†å¿µï¼ˆå¦‚ARâ†’DLMä»…éœ€å‡ è¡Œé…ç½®å˜æ›´ï¼‰æå…·å®è·µä»·å€¼ï¼ŒçœŸæ­£æ¨åŠ¨DLMä»å®éªŒå®¤åŸå‹èµ°å‘å¯å¤ç°ã€å¯è¿­ä»£ã€å¯å…±äº«çš„å¼€æ”¾ç§‘ç ”èŒƒå¼ã€‚\n    "
    },
    {
        "title": "pQuant: Towards Effective Low-Bit Language Models via Decoupled Linear Quantization-Aware Training",
        "authors": [
            "Wenzheng Zhang",
            "Bingzheng Liu",
            "Yang Hu",
            "Xiaoying Bai",
            "Wentao Zhang",
            "Bin Cui"
        ],
        "categories": [
            "cs.LG",
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22592v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22592v1",
        "summary": "Quantization-Aware Training from scratch has emerged as a promising approach for building efficient large language models (LLMs) with extremely low-bit weights (sub 2-bit), which can offer substantial advantages for edge deployment. However, existing methods still fail to achieve satisfactory accuracy and scalability. In this work, we identify a parameter democratization effect as a key bottleneck: the sensitivity of all parameters becomes homogenized, severely limiting expressivity. To address this, we propose pQuant, a method that decouples parameters by splitting linear layers into two specialized branches: a dominant 1-bit branch for efficient computation and a compact high-precision branch dedicated to preserving the most sensitive parameters. Through tailored feature scaling, we explicitly guide the model to allocate sensitive parameters to the high-precision branch. Furthermore, we extend this branch into multiple, sparsely-activated experts, enabling efficient capacity scaling. Extensive experiments indicate our pQuant achieves state-of-the-art performance in extremely low-bit quantization.",
        "tag": "ä½æ¯”ç‰¹é‡åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22592v1ã€ä½æ¯”ç‰¹é‡åŒ–-åŒ—äº¬å¤§å­¦ã€‘pQuant_ Towards Effective Low-Bit Language Models via Decoupled Linear Quantization-Aware Training.pdf",
        "institution_status": "keep",
        "institution": "åŒ—äº¬å¤§å­¦ã€å¤æ—¦å¤§å­¦",
        "first_institution": "åŒ—äº¬å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼špQuant: Towards Effective Low-Bit Language Models via Decoupled Linear Quantization-Aware Training\nğŸŒæ¥æºï¼šarXiv, 2602.22592v1\n\nç¬”è®°æ ‡é¢˜ï¼šè§£è€¦æ•æ„Ÿå‚æ•°æå‡ä½æ¯”ç‰¹æ¨¡å‹è¡¨è¾¾åŠ›\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è§£å†³æä½æ¯”ç‰¹ï¼ˆäºš2æ¯”ç‰¹ï¼‰è¯­è¨€æ¨¡å‹åœ¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒä¸­å› å‚æ•°æ•æ„Ÿæ€§å‡è´¨åŒ–è€Œå¯¼è‡´çš„è¡¨è¾¾åŠ›ä¸å¯æ‰©å±•æ€§ç“¶é¢ˆï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºpQuantæ–¹æ³•ï¼Œé€šè¿‡è§£è€¦çº¿æ€§å±‚ä¸º1æ¯”ç‰¹ä¸»å¹²ä¸é«˜ç²¾åº¦æ•æ„Ÿåˆ†æ”¯ï¼Œå¹¶å¼•å…¥ç‰¹å¾ç¼©æ”¾æœºåˆ¶åŠ¨æ€å¼•å¯¼æ•æ„Ÿå‚æ•°åˆ†é…ï¼Œæ˜¾è‘—æå‡æä½æ¯”ç‰¹æ¨¡å‹çš„å‡†ç¡®ç‡ä¸å¯æ‰©å±•æ€§ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è¯†åˆ«å¹¶å®šä¹‰â€œå‚æ•°æ°‘ä¸»åŒ–â€ç°è±¡â€”â€”å³æç«¯é‡åŒ–ä¸‹æƒé‡æ•æ„Ÿæ€§è¶‹äºå‡åŒ€ï¼ŒæŸå®³æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ã€‚  \nğŸ”¸è®¾è®¡è§£è€¦çº¿æ€§å±‚ï¼šåœ¨FFNä¸­å°†æƒé‡çŸ©é˜µæ‹†åˆ†ä¸º1æ¯”ç‰¹ä¸»åˆ†æ”¯ï¼ˆä¿éšœæ•ˆç‡ï¼‰å’Œ8æ¯”ç‰¹é«˜ç²¾åº¦å­åˆ†æ”¯ï¼ˆä¿ç•™æ•æ„Ÿå‚æ•°ï¼‰ï¼ŒäºŒè€…å¹¶è¡Œè®¡ç®—ååŠ æƒèåˆã€‚  \nğŸ”¸å¼•å…¥å¯å­¦ä¹ ç‰¹å¾ç¼©æ”¾ï¼ˆÎ±â‰«Î²åˆå§‹åŒ–ï¼‰ï¼Œä½¿æ¢¯åº¦ä¼˜å…ˆæµå‘é«˜ç²¾åº¦åˆ†æ”¯ï¼Œæ˜¾å¼å¼•å¯¼æ¨¡å‹å°†å…³é”®å‚æ•°åˆ†é…è‡³è¯¥è·¯å¾„ã€‚  \nğŸ”¸å°†é«˜ç²¾åº¦åˆ†æ”¯æ‰©å±•ä¸ºç¨€ç–æ¿€æ´»çš„å¤šä¸“å®¶ç»“æ„ï¼ˆTop-1è·¯ç”±ï¼‰ï¼Œå®ç°å®¹é‡é«˜æ•ˆæ‰©å±•è€Œä¸å¢åŠ æ¨ç†è´Ÿæ‹…ã€‚  \nğŸ”¸åœ¨MHAä¸­ç»Ÿä¸€é‡‡ç”¨1æ¯”ç‰¹é‡åŒ–ï¼Œåœ¨FFNä¸­èšç„¦è§£è€¦è®¾è®¡ï¼Œå…¼é¡¾æ•ˆç‡ä¸æ•æ„Ÿæ€§ä¿ç•™çš„æ¨¡å—å·®å¼‚åŒ–ç­–ç•¥ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸pQuantåœ¨1.3Bè§„æ¨¡ä¸‹å°†å›°æƒ‘åº¦é™ä½32.0%ï¼Œå¹³å‡å‡†ç¡®ç‡è¶…è¶ŠBitNet 1-bitè¾¾2.4ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”é€¼è¿‘2-bit BitNet1.58æ€§èƒ½ã€‚  \nğŸ”¸æ•æ„Ÿæ€§åˆ†æè¯å®ï¼špQuantæˆåŠŸæ¢å¤å·®å¼‚åŒ–æ•æ„Ÿåˆ†å¸ƒï¼Œè€ŒBitNetå‘ˆç°å¹³å¦å‡è´¨åŒ–ï¼ŒéªŒè¯å‚æ•°æ°‘ä¸»åŒ–è¢«æœ‰æ•ˆç¼“è§£ã€‚  \nğŸ”¸æ‰©å±•è‡³8ä¸ª8æ¯”ç‰¹ä¸“å®¶æ—¶ï¼ŒpQuantåœ¨1.3Bå‚æ•°é‡ä¸‹åŒ¹é…FP16 LLaMA-2çš„ä¸‹æ¸¸æ€§èƒ½ï¼ŒåŒæ—¶æ¨ç†ååæå‡18.2%ã€‚  \nğŸ”¸å†…å­˜å ç”¨ä»…0.98GBï¼ˆ1.3B pQuantï¼‰ï¼Œè¾ƒLLaMA-2é™ä½92%ï¼Œè¾ƒBitNet1.58ä½31%ï¼Œä¸”å•æ¬¡å‰å‘ä»…æ¿€æ´»ä¸€ä¸ª8æ¯”ç‰¹åˆ†æ”¯ï¼Œå¸¦å®½å‹å¥½ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šç‰¹å¾ç¼©æ”¾ä¸å¯æˆ–ç¼ºï¼›å›ºå®šé«˜ç²¾åº¦ä½ç½®æˆ–é€šé“/åˆ†ç»„é‡åŒ–æ•ˆæœè¿œé€ŠäºåŠ¨æ€è§£è€¦+ç¼©æ”¾æœºåˆ¶ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ­ç¤ºå¹¶å‘½åâ€œå‚æ•°æ°‘ä¸»åŒ–â€è¿™ä¸€åˆ¶çº¦æä½æ¯”ç‰¹è®­ç»ƒçš„æ ¹æœ¬ç°è±¡ï¼Œå¹¶æå‡ºç»“æ„åŒ–è§£è€¦+åŠ¨æ€å¼•å¯¼çš„åŒé‡æœºåˆ¶äºˆä»¥ç ´è§£ï¼›å…¶å°†MoEæ€æƒ³è¿ç§»è‡³é‡åŒ–æ¶æ„è®¾è®¡ï¼Œä»¥æå°ç¡¬ä»¶å¼€é”€ï¼ˆå•ä¸“å®¶æ¿€æ´»ï¼‰æ¢å–æ˜¾è‘—è¡¨è¾¾åŠ›æå‡ï¼Œå…¼å…·ç†è®ºæ´å¯Ÿä¸å·¥ç¨‹å®æ•ˆæ€§ã€‚\n    "
    }
]