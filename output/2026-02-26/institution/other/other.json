[
    {
        "title": "Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents",
        "authors": [
            "Ryan Liu",
            "Dilip Arumugam",
            "Cedegao E. Zhang",
            "Sean Escola",
            "Xaq Pitkow",
            "Thomas L. Griffiths"
        ],
        "categories": [
            "cs.AI",
            "cs.CL",
            "q-bio.NC"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22523v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22523v1",
        "summary": "While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This position paper argues that potential blueprints for designing such modular language agents can be found in the existing literature on cognitive models and artificial intelligence (AI) algorithms. To make this point clear, we formalize the idea of an agent template that specifies roles for individual LLMs and how their functionalities should be composed. We then survey a variety of existing language agents in the literature and highlight their underlying templates derived directly from cognitive models or AI algorithms. By highlighting these designs, we aim to call attention to agent templates inspired by cognitive science and AI as a powerful tool for developing effective, interpretable language agents.",
        "tag": "Agent æ¶æ„",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22523v1ã€Agent æ¶æ„-æ™®æ—æ–¯é¡¿å¤§å­¦ã€‘Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents.pdf",
        "institution_status": "keep",
        "institution": "æ™®æ—æ–¯é¡¿å¤§å­¦ã€éº»çœç†å·¥å­¦é™¢ã€å“¥ä¼¦æ¯”äºšå¤§å­¦ã€å¡å†…åŸºæ¢…éš†å¤§å­¦",
        "first_institution": "æ™®æ—æ–¯é¡¿å¤§å­¦",
        "institution_category": "å…¶ä»–æœºæ„",
        "note": "ğŸ“–æ ‡é¢˜ï¼šCognitive Models and AI Algorithms Provide Templates for Designing Language Agents\nğŸŒæ¥æºï¼šarXiv, 2602.22523v1\n\nç¬”è®°æ ‡é¢˜ï¼šè®¤çŸ¥æ¨¡å‹å¯å‘è¯­è¨€ä»£ç†è®¾è®¡\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ç³»ç»ŸåŒ–åœ°è®¾è®¡å¤šæ¨¡å—è¯­è¨€ä»£ç†ï¼Œè€Œéä¾èµ–è¯•é”™æˆ–ç›´è§‰ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºâ€œä»£ç†æ¨¡æ¿â€æ¦‚å¿µï¼Œè®ºè¯è®¤çŸ¥æ¨¡å‹ä¸ç»å…¸AIç®—æ³•å¯ä½œä¸ºå¯è§£é‡Šã€å¯å¤ç”¨çš„è¯­è¨€ä»£ç†æ¶æ„è“å›¾ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å®šä¹‰ä»£ç†æ¨¡æ¿ä¸ºæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼ŒèŠ‚ç‚¹æ˜¯LLMæˆ–å·¥å…·ï¼Œè¾¹è¡¨ç¤ºæ•°æ®æµä¸æ‰§è¡Œé¡ºåºï¼Œå½¢å¼åŒ–åˆ»ç”»æ¨¡å—è§’è‰²ä¸ç»„åˆé€»è¾‘ã€‚  \nğŸ”¸æ¢³ç†è®¤çŸ¥æ¨¡å‹ä¸‰å¤§ç±»æ¨¡æ¿ï¼šé€šä¿¡é¢†åŸŸé‡‡ç”¨ç†æ€§è¨€è¯­è¡Œä¸ºï¼ˆRSAï¼‰å®ç°é€’å½’ç¤¾ä¼šæ¨ç†ï¼›æ¨ç†ä¸è§„åˆ’é¢†åŸŸå€Ÿé‰´å‰é¢å¶åŠŸèƒ½å»ºæ¨¡ä¸æ€ç»´ aloud åè®®ï¼›è¡¨å¾é¢†åŸŸå¼•å…¥â€œæ€æƒ³è¯­è¨€â€ï¼ˆLoTï¼‰èŒƒå¼ï¼Œä»¥ä»£ç ç”Ÿæˆ+æ‰§è¡Œæ„æˆreasoner-interpreteråŒæ¨¡å—ç»“æ„ã€‚  \nğŸ”¸å½’çº³AIç®—æ³•å››ç±»æ¨¡æ¿ï¼šæœç´¢ç±»ï¼ˆå¦‚Tree of Thoughtså¯¹åº”A*ä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼‰ã€åˆ†æ²»ç±»ï¼ˆå¦‚Least-to-Most promptingï¼‰ã€å¼ºåŒ–å­¦ä¹ ç±»ï¼ˆå¦‚ICPIå®ç°ç­–ç•¥è¿­ä»£ã€PSRLå®ç°åéªŒé‡‡æ ·ï¼‰ã€ä¿¡æ¯å¯¼å‘ç±»ï¼ˆIDSå¹³è¡¡æ¢ç´¢ä¸ä¿¡æ¯å¢ç›Šï¼‰ã€‚  \nğŸ”¸å¼ºè°ƒæ¨¡æ¿ä¼˜åŠ¿åœ¨äºç»§æ‰¿å·²æœ‰ç†è®ºéªŒè¯æ€§â€”â€”é¿å…ä»é›¶ä¼˜åŒ–ï¼Œæå‡å¯è§£é‡Šæ€§ï¼Œå¹¶æ”¯æŒçŠ¶æ€ä¼ é€’ï¼ˆå¦‚æ–‡æœ¬åŒ–â€œåéªŒâ€ï¼‰ã€å¤šè¾“å…¥/è¾“å‡ºç­‰å®é™…éœ€æ±‚ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸ç°æœ‰æˆåŠŸè¯­è¨€ä»£ç†ï¼ˆå¦‚MAPè§„åˆ’å™¨ã€CodeActã€Tree of Thoughtsï¼‰å‡å¯æ˜ å°„åˆ°æ˜ç¡®çš„è®¤çŸ¥æˆ–AIç®—æ³•åŸå‹ï¼ŒéªŒè¯æ¨¡æ¿çš„å®ç”¨æ€§ä¸æ³›åŒ–åŠ›ã€‚  \nğŸ”¸æ¨¡æ¿é©±åŠ¨çš„è®¾è®¡åœ¨é€šä¿¡è´¨é‡ã€å¤šæ­¥æ¨ç†ã€å¤æ‚è§„åˆ’ç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œä¸”äººç±»è¯„ä¼°è®¤å¯å…¶åˆç†æ€§ä¸å¯ç†è§£æ€§ã€‚  \nğŸ”¸å¯¹æ¯”çº¯æ•°æ®é©±åŠ¨çš„è‡ªåŠ¨æ¶æ„æœç´¢ï¼ˆå¦‚GPTSwarmè¿›åŒ–ï¼‰ï¼Œæ¨¡æ¿æ–¹æ³•æ— éœ€æµ·é‡è®­ç»ƒæ•°æ®ï¼Œåœ¨åŒ»ç–—ç­‰é«˜é£é™©åœºæ™¯æ›´å…·å¯è¡Œæ€§ã€‚  \nğŸ”¸å®éªŒè¡¨æ˜ï¼ŒPSRLä¸IDSæ¨¡æ¿åœ¨Wordleç­‰è¯­è¨€åŒ–å†³ç­–ä»»åŠ¡ä¸­ä¿æŒåŸç®—æ³•çš„é«˜æ•ˆæ¢ç´¢ç‰¹æ€§ï¼Œè¯æ˜è®¤çŸ¥/AIåŸç†å¯è·¨æ¨¡æ€è¿ç§»ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥æ–‡æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†â€œä»£ç†è®¾è®¡â€ä»å·¥ç¨‹å®è·µå‡ç»´ä¸ºç§‘å­¦å»ºæ¨¡ï¼šä¸æ˜¯è®©LLMæ‹Ÿäººï¼Œè€Œæ˜¯ç”¨äººç±»å·²éªŒè¯çš„è®¤çŸ¥æœºåˆ¶ä¸AIè®¡ç®—èŒƒå¼çº¦æŸLLMè¡Œä¸ºï¼Œä½¿ä»£ç†å…¼å…·æ€§èƒ½ä¸å¯è§£é‡Šæ€§ã€‚å®ƒæ‰“ç ´äº†LLMé»‘ç®±ä¸è®¤çŸ¥ç§‘å­¦æŠ½è±¡ä¹‹é—´çš„éš”é˜‚ï¼Œä¸ºæ„å»ºå¯ä¿¡ã€å¯æ§ã€å¯æ¼”åŒ–çš„è¯­è¨€æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ–¹æ³•è®ºåŸºçŸ³ã€‚\n    "
    },
    {
        "title": "General Agent Evaluation",
        "authors": [
            "Elron Bandel",
            "Asaf Yehudai",
            "Lilach Eden",
            "Yehoshua Sagron",
            "Yotam Perlitz",
            "Elad Venezian",
            "Natalia Razinkov",
            "Natan Ergas",
            "Shlomit Shachor Ifergan",
            "Segev Shlomov",
            "Michal Jacovi",
            "Leshem Choshen",
            "Liat Ein-Dor",
            "Yoav Katz",
            "Michal Shmueli-Scheuer"
        ],
        "categories": [
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22953v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22953v1",
        "summary": "The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.",
        "tag": "é€šç”¨ Agent è¯„ä¼°",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22953v1ã€é€šç”¨ Agent è¯„ä¼°-IBM Researchã€‘General Agent Evaluation.pdf",
        "institution_status": "keep",
        "institution": "IBM Researchã€MIT",
        "first_institution": "IBM Research",
        "institution_category": "å…¶ä»–æœºæ„",
        "note": "ğŸ“–æ ‡é¢˜ï¼šGeneral Agent Evaluation\nğŸŒæ¥æºï¼šarXiv, 2602.22953v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºé€šç”¨æ™ºèƒ½ä½“è¯„ä¼°æ–°èŒƒå¼\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ç³»ç»Ÿã€å…¬å¹³åœ°è¯„ä¼°ä¸ä¾èµ–é¢†åŸŸå®šåˆ¶çš„é€šç”¨æ™ºèƒ½ä½“åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­çš„çœŸå®æ³›åŒ–èƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºé¦–ä¸ªé¢å‘é€šç”¨æ™ºèƒ½ä½“çš„æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶Exgenticï¼ŒåŒ…å«ç»Ÿä¸€åè®®ã€å¼€æºå·¥å…·é“¾ä¸å…¬å¼€æ’è¡Œæ¦œï¼Œé¦–æ¬¡å®ç°è·¨ç¯å¢ƒã€è·¨æ¶æ„çš„æ— ååŸºå‡†æµ‹è¯•ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æå‡ºUnified Protocolâ€”â€”ä¸€ç§è§£è€¦æ™ºèƒ½ä½“æ¥å£ä¸åŸºå‡†ä»»åŠ¡çš„ä¸­ä»‹åè®®ï¼Œå®šä¹‰ä»»åŠ¡ï¼ˆtaskï¼‰ã€ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰ã€åŠ¨ä½œï¼ˆactionsï¼‰ä¸‰è¦ç´ ï¼Œæ”¯æŒCLIã€MCPã€å·¥å…·è°ƒç”¨ç­‰å¼‚æ„äº¤äº’æ–¹å¼çš„æ— æŸæ˜ å°„ã€‚  \nğŸ”¸è®¾è®¡å¤–éƒ¨é€‚é…å™¨æœºåˆ¶ï¼Œé¿å…ä¾µå…¥å¼ä¿®æ”¹ç¬¬ä¸‰æ–¹æ™ºèƒ½ä½“æˆ–åŸºå‡†ä»£ç ï¼Œé€šè¿‡è¿›ç¨‹éš”ç¦»+åè®®ç¿»è¯‘å®ç°å³æ’å³ç”¨é›†æˆã€‚  \nğŸ”¸æ„å»ºExgenticè¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒå¹¶è¡Œæ‰§è¡Œã€è½¨è¿¹è®°å½•ã€æˆæœ¬è®¡é‡ä¸ç»“æœæ ‡å‡†åŒ–ï¼Œæä¾›Python APIä¸GUIåŒå…¥å£ã€‚  \nğŸ”¸å‘å¸ƒOpen General Agent Leaderboardï¼Œè¦†ç›–5ç±»ä¸»æµæ™ºèƒ½ä½“æ¶æ„ã€3ä¸ªå‰æ²¿å¤§æ¨¡å‹ã€6ä¸ªå¼‚æ„åŸºå‡†ç¯å¢ƒï¼ˆå«è½¯ä»¶å·¥ç¨‹ã€å®¢æœã€æ·±ç ”ã€Appæ“ä½œç­‰ï¼‰ï¼Œæ€»è€—èµ„2.2ä¸‡ç¾å…ƒã€‚  \nğŸ”¸é‡‡ç”¨ç»„ä»¶çº§åˆ†æè§†è§’ï¼Œç³»ç»Ÿæ‹†è§£æ‰§è¡Œè¿è¡Œæ—¶ã€å·¥å…·ç­›é€‰ã€æ¨¡å¼æ ¡éªŒã€é€šä¿¡åè®®ã€è®°å¿†ä¸è§„åˆ’ç­‰æ¨¡å—ï¼Œé‡åŒ–å„ç»„ä»¶å¯¹æ€§èƒ½çš„å½±å“ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸é€šç”¨æ™ºèƒ½ä½“å…·å¤‡æ˜¾è‘—è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼šå¤šæ•°é…ç½®åœ¨å¤šä¸ªåŸºå‡†ä¸Šè¡¨ç°æ¥è¿‘ç”šè‡³è¶…è¶Šé¢†åŸŸä¸“ç”¨ç³»ç»Ÿï¼ŒéªŒè¯äº†â€œé€šç”¨æ€§â€å¯è¡Œæ€§ã€‚  \nğŸ”¸è¯­è¨€æ¨¡å‹è´¨é‡æ˜¯æ€§èƒ½ä¸»å¯¼å› ç´ ï¼ˆè§£é‡Š28.2%æ–¹å·®ï¼‰ï¼Œè¿œè¶…æ™ºèƒ½ä½“æ¶æ„å½±å“ï¼ˆä»…0.6%ï¼‰ï¼Œä¸”æ¨¡å‹ç¨³å®šæ€§å·®å¼‚æ˜¾è‘—ï¼ˆClaude Opusæœ€ç¨³å®šï¼ŒGPT 5.2æœ€æ•æ„Ÿï¼‰ã€‚  \nğŸ”¸æ— å•ä¸€æ™ºèƒ½ä½“æ¶æ„å…¨é¢é¢†å…ˆï¼šOpenAI Soloåœ¨API/ç¼–ç ä»»åŠ¡å ä¼˜ï¼ŒSmolagentåœ¨å¤šåº”ç”¨ç¯å¢ƒæ›´ä½³ï¼Œä½“ç°æ¶æ„-ä»»åŠ¡åŒ¹é…çš„é‡è¦æ€§ã€‚  \nğŸ”¸å…³é”®ç»„ä»¶ä»·å€¼æ˜ç¡®ï¼šæ¨¡å¼æ ¡éªŒï¼ˆschema guardï¼‰æ™®éå­˜åœ¨äºTop3æ¶æ„ä¸­ï¼›å·¥å…·çŸ­åˆ—è¡¨ï¼ˆtool shortlistingï¼‰ä½¿GPT 5.2åœ¨é«˜å·¥å…·æ•°ç¯å¢ƒä»ä¸å¯ç”¨å˜ä¸ºå¯ç”¨ï¼Œæå‡5ä¸ªç™¾åˆ†ç‚¹ã€‚  \nğŸ”¸å¤±è´¥ä»£ä»·è¢«ä¸¥é‡ä½ä¼°ï¼šå¤±è´¥ä»»åŠ¡å¹³å‡æ­¥æ•°æ¯”æˆåŠŸä»»åŠ¡é«˜20%â€“54%ï¼Œæ„å‘³ç€å¯é æ€§ä¸‹é™ä¼šçº¿æ€§æ”¾å¤§è®¡ç®—æˆæœ¬ä¸å»¶è¿Ÿï¼Œå‡¸æ˜¾é²æ£’æ€§è¯„ä¼°çš„å¿…è¦æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†â€œé€šç”¨æ™ºèƒ½ä½“è¯„ä¼°â€æœ¬èº«ç¡®ç«‹ä¸ºä¸€çº§ç ”ç©¶é—®é¢˜ï¼Œå¹¶ä»¥å·¥ç¨‹ä¸¥è°¨æ€§æ„å»ºå¯å¤ç°ã€å¯æ‰©å±•ã€å»ä¸­å¿ƒåŒ–çš„è¯„ä¼°åŸºç¡€è®¾æ–½ã€‚å®ƒè·³å‡ºäº†ä¼ ç»Ÿâ€œä¸ºç‰¹å®šåŸºå‡†å®šåˆ¶æ™ºèƒ½ä½“â€çš„è·¯å¾„ä¾èµ–ï¼Œè½¬è€Œé€šè¿‡åè®®æŠ½è±¡ä¸é€‚é…å™¨è§£è€¦ï¼Œä½¿è¯„ä¼°çœŸæ­£æœåŠ¡äºé€šç”¨æ€§ç›®æ ‡ã€‚å…¶æœ€å¤§æ´è§æ˜¯æ­ç¤ºï¼šå½“å‰æ‰€è°“â€œé€šç”¨æ™ºèƒ½ä½“â€çš„ç«äº‰åŠ›ä¸»è¦æºäºåº•å±‚æ¨¡å‹èƒ½åŠ›è¿ç§»ï¼Œè€Œéæ¶æ„è®¾è®¡çªç ´ï¼›å› æ­¤ï¼Œæœªæ¥ç ”ç©¶åº”èšç„¦äºå¦‚ä½•è®©æ¶æ„æœ‰æ•ˆé‡Šæ”¾æ¨¡å‹æ½œåŠ›ï¼ˆå¦‚é€šè¿‡çŸ­åˆ—è¡¨ã€æ ¡éªŒã€è®°å¿†ç­‰è½»é‡ç»„ä»¶ï¼‰ï¼Œè€Œéè¿½æ±‚å¤æ‚åº¦å †ç Œã€‚è¿™ä¸€èŒƒå¼è½¬ç§»ï¼Œä¸ºé€šç”¨AIä»£ç†çš„ç§‘å­¦åŒ–å‘å±•å¥ å®šäº†æ–¹æ³•è®ºåŸºçŸ³ã€‚\n    "
    },
    {
        "title": "Imagination Helps Visual Reasoning, But Not Yet in Latent Space",
        "authors": [
            "You Li",
            "Chi Chen",
            "Yanghao Li",
            "Fanhu Zeng",
            "Kaiyu Huang",
            "Jinan Xu",
            "Maosong Sun"
        ],
        "categories": [
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22766v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22766v1",
        "summary": "Latent visual reasoning aims to mimic human's imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect: dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect: perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine, which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.",
        "tag": "è§†è§‰æ¨ç†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22766v1ã€è§†è§‰æ¨ç†-åŒ—äº¬äº¤é€šå¤§å­¦ã€‘Imagination Helps Visual Reasoning, But Not Yet in Latent Space.pdf",
        "institution_status": "keep",
        "institution": "åŒ—äº¬äº¤é€šå¤§å­¦ã€æ¸…åå¤§å­¦",
        "first_institution": "åŒ—äº¬äº¤é€šå¤§å­¦",
        "institution_category": "å…¶ä»–æœºæ„",
        "note": "ğŸ“–æ ‡é¢˜ï¼šImagination Helps Visual Reasoning, But Not Yet in Latent Space\nğŸŒæ¥æºï¼šarXiv, 2602.22766v1\n\nç¬”è®°æ ‡é¢˜ï¼šè´¨ç–‘éšç©ºé—´æƒ³è±¡æœ‰æ•ˆæ€§\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šéšç©ºé—´è§†è§‰æ¨ç†ä¸­çš„æ½œåœ¨æ ‡è®°ï¼ˆlatent tokensï¼‰æ˜¯å¦çœŸæ­£å‚ä¸å¹¶é©±åŠ¨äº†å› æœæ¨ç†è¿‡ç¨‹ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šé€šè¿‡å› æœä¸­ä»‹åˆ†ææ­ç¤ºéšç©ºé—´æ¨ç†ä¸­è¾“å…¥â†’éšæ ‡è®°â†’ç­”æ¡ˆçš„ä¸¤æ¡å› æœé“¾å‡æ–­è£‚ï¼Œè¿›è€Œæå‡ºæ›´æœ‰æ•ˆã€å¯è§£é‡Šçš„æ–‡æœ¬ç©ºé—´æƒ³è±¡æ–¹æ³•CapImagineã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸é‡‡ç”¨å› æœä¸­ä»‹åˆ†ææ¡†æ¶ï¼Œå°†è§†è§‰æ¨ç†å»ºæ¨¡ä¸ºXâ†’Zâ†’Yå› æœé“¾ï¼Œç³»ç»Ÿæ‰°åŠ¨è¾“å…¥Xå’Œéšæ ‡è®°Zä»¥æ£€éªŒå› æœæ•ˆåº”ã€‚  \nğŸ”¸è®¾è®¡å®ä¾‹çº§è¾“å…¥æ‰°åŠ¨å®éªŒï¼Œæµ‹é‡ä¸åŒæ ·æœ¬é—´éšæ ‡è®°çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå‘ç°å…¶é«˜åº¦åŒè´¨åŒ–ï¼Œè¡¨æ˜Xâ†’Zå› æœå¼±ã€‚  \nğŸ”¸å¯¹éšæ ‡è®°Zæ–½åŠ å¼ºå¹²é¢„ï¼ˆå¦‚å…¨ç½®é›¶ã€é«˜æ–¯å™ªå£°ã€ç»Ÿä¸€å¼ é‡ï¼‰ï¼Œè§‚å¯Ÿç­”æ¡ˆYå˜åŒ–ï¼Œå‘ç°æ€§èƒ½å‡ ä¹ä¸å˜ï¼Œè¡¨æ˜Zâ†’Yå› æœå¼±ã€‚  \nğŸ”¸å¼€å±•æ¢é’ˆåˆ†æï¼Œç”¨éšæ ‡è®°å•ç‹¬é¢„æµ‹å¤šé€‰VQAé—®é¢˜ï¼Œç»“æœæ˜¾è‘—ä½äºæ–‡æœ¬çŒœæµ‹åŸºçº¿ï¼Œè¯å®å…¶ç¼–ç è§†è§‰è¯­ä¹‰èƒ½åŠ›æå¼±ã€‚  \nğŸ”¸æå‡ºCapImagineæ–¹æ³•ï¼Œå°†ä¸­é—´å›¾åƒæ“ä½œè½¬åŒ–ä¸ºæ˜¾å¼æ–‡æœ¬æè¿°ï¼ˆå¦‚â€œçº¢è‰²çŸ©å½¢é«˜äº®æ™ºåˆ©åŒºåŸŸâ€ï¼‰ï¼Œä½¿æ¨¡å‹åœ¨æ–‡æœ¬ç©ºé—´å®Œæˆæƒ³è±¡æ¨ç†ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸éšæ ‡è®°åœ¨è·¨æ ·æœ¬å’Œè·¨ä»»åŠ¡ä¸‹é«˜åº¦ç›¸ä¼¼ï¼Œä¸”éšæ¨ç†æ­¥æ•°å¢åŠ æŒç»­åç¼©ï¼Œä¸§å¤±è¾“å…¥ç‰¹å¼‚æ€§ã€‚  \nğŸ”¸æ‰°åŠ¨éšæ ‡è®°å‡ ä¹ä¸æ”¹å˜æœ€ç»ˆç­”æ¡ˆï¼Œåœ¨å¤šä¸ªåŸºå‡†ä¸Šæ€§èƒ½æ³¢åŠ¨å°äº1%ï¼Œè¯´æ˜å…¶å¯¹è¾“å‡ºæ— å®è´¨å› æœå½±å“ã€‚  \nğŸ”¸éšæ ‡è®°æ— æ³•ç‹¬ç«‹æ”¯æŒä¸‹æ¸¸è§†è§‰é—®ç­”ï¼Œç”šè‡³ä¸å¦‚çº¯æ–‡æœ¬çŒœæµ‹ï¼Œè¯æ˜å…¶æœªæ‰¿è½½æœ‰æ•ˆè§†è§‰è¯­ä¹‰ã€‚  \nğŸ”¸æ–‡æœ¬ç©ºé—´æƒ³è±¡å˜é‡Zå±•ç°å‡ºå¼ºXâ†’Zä¾èµ–æ€§ä¸Zâ†’Yæ•æ„Ÿæ€§ï¼Œå¹²é¢„åæ€§èƒ½éª¤é™è‡³éšæœºæ°´å¹³ï¼ŒéªŒè¯å…¶çœŸå®å› æœä½œç”¨ã€‚  \nğŸ”¸CapImagineåœ¨HR-Benchã€MME-RealWorld-Liteç­‰åŸºå‡†ä¸Šå…¨é¢è¶…è¶ŠMonetç­‰éšç©ºé—´æ–¹æ³•ï¼Œå¹³å‡æå‡è¶…3%ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡åˆ›æ–°æ€§åœ°å¼•å…¥å› æœä¸­ä»‹åˆ†æè¿™ä¸€ä¸¥è°¨å·¥å…·ï¼Œé¦–æ¬¡ä»å› æœæ€§è§’åº¦ç³»ç»Ÿè§£æ„éšç©ºé—´æ¨ç†çš„â€œé»‘ç®±â€ï¼Œè€Œéä»…ä¾èµ–æ€§èƒ½æ¯”è¾ƒï¼›å…¶æ ¸å¿ƒæ´è§â€”â€”éšæ ‡è®°å®ä¸ºè½¯æç¤ºå¼å ä½ç¬¦è€Œéæƒ³è±¡è½½ä½“â€”â€”ç›´å‡»å½“å‰èŒƒå¼æœ¬è´¨ç¼ºé™·ï¼›æå‡ºçš„CapImagineè™½éç»ˆææ–¹æ¡ˆï¼Œä½†ä»¥æç®€è®¾è®¡å®ç°æ›´å¼ºå› æœæ€§ä¸æ›´é«˜æ€§èƒ½ï¼Œä¸ºæ„å»ºå¯è§£é‡Šã€å¯ä¿¡èµ–çš„è§†è§‰æ¨ç†æ¨¡å‹æä¾›äº†æ¸…æ™°æ–¹æ³•è®ºè½¬å‘ã€‚\n    "
    },
    {
        "title": "MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios",
        "authors": [
            "Zhiheng Song",
            "Jingshuai Zhang",
            "Chuan Qin",
            "Chao Wang",
            "Chao Chen",
            "Longfei Xu",
            "Kaikui Liu",
            "Xiangxiang Chu",
            "Hengshu Zhu"
        ],
        "categories": [
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22638v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22638v1",
        "summary": "Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making. However, systematic evaluation in real-world mobility settings is hindered by diverse routing demands, non-deterministic mapping services, and limited reproducibility. In this study, we introduce MobilityBench, a scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap and covers a broad spectrum of route-planning intents across multiple cities worldwide. To enable reproducible, end-to-end evaluation, we design a deterministic API-replay sandbox that eliminates environmental variance from live services. We further propose a multi-dimensional evaluation protocol centered on outcome validity, complemented by assessments of instruction understanding, planning, tool use, and efficiency. Using MobilityBench, we evaluate multiple LLM-based route-planning agents across diverse real-world mobility scenarios and provide an in-depth analysis of their behaviors and performance. Our findings reveal that current models perform competently on Basic information retrieval and Route Planning tasks, yet struggle considerably with Preference-Constrained Route Planning, underscoring significant room for improvement in personalized mobility applications. We publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench .",
        "tag": "è¯„ä¼°åŸºå‡†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22638v1ã€è¯„ä¼°åŸºå‡†-ä¸­å›½ç§‘å­¦é™¢ã€‘MobilityBench_ A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios.pdf",
        "institution_status": "keep",
        "institution": "ä¸­å›½ç§‘å­¦é™¢ã€é˜¿é‡Œ",
        "first_institution": "ä¸­å›½ç§‘å­¦é™¢",
        "institution_category": "å…¶ä»–æœºæ„",
        "note": "ğŸ“–æ ‡é¢˜ï¼šMobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios\nğŸŒæ¥æºï¼šarXiv, 2602.22638v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºçœŸå®å‡ºè¡Œåœºæ™¯è¯„æµ‹åŸºå‡†\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ç³»ç»Ÿã€å¯å¤ç°åœ°è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„è·¯å¾„è§„åˆ’æ™ºèƒ½ä½“åœ¨å¤æ‚ç°å®å‡ºè¡Œåœºæ™¯ä¸­çš„ç»¼åˆèƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºäº†MobilityBenchâ€”â€”é¦–ä¸ªåŸºäºå¤§è§„æ¨¡åŒ¿åçœŸå®ç”¨æˆ·æŸ¥è¯¢æ„å»ºã€æ”¯æŒç«¯åˆ°ç«¯å¯å¤ç°è¯„æµ‹çš„è·¯çº¿è§„åˆ’æ™ºèƒ½ä½“åŸºå‡†ï¼Œæ¶µç›–å¤šåŸå¸‚ã€å¤šæ¨¡æ€ã€å¤šçº¦æŸçš„çœŸå®å‡ºè¡Œä»»åŠ¡ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸é‡‡ç”¨â€œepisode-centricâ€è®¾è®¡ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«è‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€ç¡®å®šæ€§APIå“åº”å¿«ç…§å’Œç»“æ„åŒ–çœŸå€¼æ ‡æ³¨ï¼Œç¡®ä¿ä»»åŠ¡è‡ªæ´½ä¸”æ— éœ€ç”¨æˆ·æ¾„æ¸…ã€‚  \nğŸ”¸æ„å»ºè¦†ç›–22å›½350+åŸå¸‚çš„10ä¸‡æ ·æœ¬æ•°æ®é›†ï¼ŒæŒ‰æ„å›¾åˆ’åˆ†ä¸ºå››å¤§ä»»åŠ¡æ—ï¼ˆåŸºç¡€ä¿¡æ¯æ£€ç´¢ã€è·¯å¾„ä¾èµ–æ£€ç´¢ã€åŸºç¡€è·¯å¾„è§„åˆ’ã€åå¥½çº¦æŸè·¯å¾„è§„åˆ’ï¼‰å…±11ç±»ç»†ç²’åº¦åœºæ™¯ã€‚  \nğŸ”¸è®¾è®¡ç¡®å®šæ€§APIé‡æ”¾æ²™ç®±ï¼Œç¼“å­˜å¹¶æ ‡å‡†åŒ–åœ°å›¾æœåŠ¡å“åº”ï¼ˆå¦‚è·¯å†µã€POIã€å¤©æ°”ï¼‰ï¼Œæ¶ˆé™¤å®æ—¶æœåŠ¡æ³¢åŠ¨å¸¦æ¥çš„ä¸å¯å¤ç°æ€§ã€‚  \nğŸ”¸æå‡ºå¤šç»´è¯„æµ‹åè®®ï¼Œä»æŒ‡ä»¤ç†è§£ï¼ˆæ„å›¾æ£€æµ‹ã€ä¿¡æ¯æŠ½å–ï¼‰ã€è§„åˆ’èƒ½åŠ›ï¼ˆä»»åŠ¡åˆ†è§£ï¼‰ã€å·¥å…·ä½¿ç”¨ï¼ˆå·¥å…·é€‰æ‹©ã€æ¨¡å¼åˆè§„ï¼‰ã€å†³ç­–è´¨é‡ï¼ˆäº¤ä»˜ç‡ã€æœ€ç»ˆé€šè¿‡ç‡ï¼‰åŠæ•ˆç‡ï¼ˆè¾“å…¥/è¾“å‡ºtokenï¼‰äº”ä¸ªç»´åº¦è¿›è¡Œç»†ç²’åº¦è¯Šæ–­ã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸å½“å‰LLMè·¯å¾„è§„åˆ’æ™ºèƒ½ä½“åœ¨åŸºç¡€ä¿¡æ¯æ£€ç´¢ä¸ç‚¹å¯¹ç‚¹è·¯å¾„è§„åˆ’ä¸Šè¡¨ç°è‰¯å¥½ï¼ˆFPRè¾¾60%â€“70%ï¼‰ï¼Œä½†åœ¨åå¥½çº¦æŸè·¯å¾„è§„åˆ’ï¼ˆå¦‚é¿é«˜é€Ÿã€é™æ¢ä¹˜ï¼‰ä¸Šæ˜¾è‘—ä¸è¶³ï¼Œæš´éœ²ä¸ªæ€§åŒ–å‡ºè¡Œæ”¯æŒèƒ½åŠ›çŸ­æ¿ã€‚  \nğŸ”¸Plan-and-Executeæ¡†æ¶åœ¨é€»è¾‘å¼ºçº¦æŸä»»åŠ¡ä¸­æ›´ç¨³å®šï¼Œè€ŒReActå› é—­ç¯åé¦ˆæœºåˆ¶åœ¨æ•´ä½“æˆåŠŸç‡ä¸Šç•¥ä¼˜ï¼Œä½†æ¨ç†å¼€é”€é«˜ï¼ˆå¹³å‡è¾“å…¥tokené«˜35.4%ï¼‰ã€‚  \nğŸ”¸é—­æºæ¨¡å‹ï¼ˆClaude/Geminiï¼‰åœ¨æŒ‡ä»¤ç†è§£ä¸Šé¢†å…ˆï¼Œä½†å¼€æºå¤§æ¨¡å‹ï¼ˆQwen235B-A22Bã€DeepSeek-V3.2-Expï¼‰å·²æ¥è¿‘å…¶æ°´å¹³ï¼Œä¸”å…·å¤‡æ›´é«˜æ€§ä»·æ¯”ã€‚  \nğŸ”¸å¯ç”¨â€œThinkingâ€æ¨ç†æ¨¡å¼å¯æå‡æœ€ç»ˆé€šè¿‡ç‡ï¼ˆæœ€é«˜+5.98%ï¼‰ï¼Œä½†å¸¦æ¥æ˜¾è‘—å»¶è¿Ÿä¸tokenå¼€é”€ï¼Œåˆ¶çº¦å®æ—¶éƒ¨ç½²å¯è¡Œæ€§ã€‚  \n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†çœŸå®ä¸–ç•Œå‡ºè¡Œå¤æ‚æ€§ï¼ˆå¤šæ¨¡æ€ã€åŠ¨æ€çº¦æŸã€é•¿å°¾æ„å›¾ï¼‰ä¸ä¸¥æ ¼å¯å¤ç°æ€§ï¼ˆæ²™ç®±é‡æ”¾ã€ç»“æ„åŒ–çœŸå€¼ï¼‰ç»“åˆï¼Œå¡«è¡¥äº†é¢†åŸŸè¯„æµ‹ç©ºç™½ï¼›å…¶å¤šç»´è¯Šæ–­åè®®è¶…è¶Šä¼ ç»Ÿç«¯åˆ°ç«¯å‡†ç¡®ç‡ï¼Œä¸ºæ¨¡å‹èƒ½åŠ›å½’å› æä¾›æ–°èŒƒå¼ï¼›å…¬å¼€æ•°æ®ä¸å·¥å…·é“¾æå¤§ä¿ƒè¿›å…¬å¹³æ¯”è¾ƒä¸æŠ€æœ¯è¿­ä»£ã€‚\n    "
    },
    {
        "title": "Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search",
        "authors": [
            "Xun Huang",
            "Simeng Qin",
            "Xiaoshuang Jia",
            "Ranjie Duan",
            "Huanqian Yan",
            "Zhitao Zeng",
            "Fei Yang",
            "Yang Liu",
            "Xiaojun Jia"
        ],
        "categories": [
            "cs.AI",
            "cs.CR"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22983v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22983v1",
        "summary": "As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.",
        "tag": "å®‰å…¨å¯¹é½",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22983v1ã€å®‰å…¨å¯¹é½-å—æ´‹ç†å·¥å¤§å­¦ã€‘Obscure but Effective_ Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search.pdf",
        "institution_status": "keep",
        "institution": "å—æ´‹ç†å·¥å¤§å­¦ã€BraneMatrix AIã€å—äº¬ç†å·¥å¤§å­¦ã€ä¸œåŒ—å¤§å­¦ã€ä¸­å›½äººæ°‘å¤§å­¦ã€é˜¿é‡Œå·´å·´é›†å›¢ã€åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦ã€æ–°åŠ å¡å›½ç«‹å¤§å­¦ã€æµ™æ±Ÿå®éªŒå®¤",
        "first_institution": "å—æ´‹ç†å·¥å¤§å­¦",
        "institution_category": "å…¶ä»–æœºæ„",
        "note": "ğŸ“–æ ‡é¢˜ï¼šObscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search\nğŸŒæ¥æºï¼šarXiv, 2602.22983v1\n\nç¬”è®°æ ‡é¢˜ï¼šå¤å…¸ä¸­æ–‡æ¿€å‘é»‘ç›’è¶Šç‹±\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šä¸ºä½•å¤å…¸ä¸­æ–‡èƒ½æœ‰æ•ˆç»•è¿‡å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨å¯¹é½æœºåˆ¶å¹¶å®ç°é«˜æˆåŠŸç‡çš„é»‘ç›’è¶Šç‹±æ”»å‡»ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šé¦–æ¬¡ç³»ç»Ÿæ€§æå‡ºå°†å¤å…¸ä¸­æ–‡ä½œä¸ºå¯¹æŠ—æç¤ºç”Ÿæˆçš„æ–°è¯­è¨€ç»´åº¦ï¼Œæ„å»ºåŸºäºå…«ç»´ç­–ç•¥ç©ºé—´ä¸æœè‡ä»¿ç”Ÿä¼˜åŒ–çš„é»‘ç›’è¶Šç‹±æ¡†æ¶CC-BOSã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æå‡ºå¤å…¸ä¸­æ–‡è¯­å¢ƒä¸‹çš„å…«ç»´ç­–ç•¥ç©ºé—´ï¼Œæ¶µç›–è§’è‰²èº«ä»½ã€è¡Œä¸ºå¼•å¯¼ã€ä½œç”¨æœºåˆ¶ã€éšå–»æ˜ å°„ã€è¡¨è¾¾é£æ ¼ã€çŸ¥è¯†å…³è”ã€æƒ…å¢ƒè®¾å®šå’Œè§¦å‘æ¨¡å¼ï¼Œç»Ÿä¸€å»ºæ¨¡ç°æœ‰è¶Šç‹±ç­–ç•¥ä¸å¤æ–‡ç‰¹æ€§ã€‚  \nğŸ”¸è®¾è®¡æœè‡ä»¿ç”Ÿä¼˜åŒ–ç®—æ³•ï¼Œèåˆå—…è§‰æœç´¢ï¼ˆå±€éƒ¨æ‰°åŠ¨ï¼‰ã€è§†è§‰æœç´¢ï¼ˆå‘æœ€ä¼˜è§£å¸å¼•ï¼‰ä¸æŸ¯è¥¿å˜å¼‚ï¼ˆé€ƒé€¸å±€éƒ¨æœ€ä¼˜ï¼‰ï¼Œåœ¨ç¦»æ•£ç­–ç•¥ç©ºé—´ä¸­é«˜æ•ˆè¿­ä»£å¯»ä¼˜ã€‚  \nğŸ”¸æ„å»ºä¸¤é˜¶æ®µç¿»è¯‘æ¨¡å—ï¼ˆæ–‡è¨€â†’ç™½è¯â†’è‹±æ–‡ï¼‰ï¼Œç¼“è§£è¯­ä¹‰å‹ç¼©ä¸éšå–»æ­§ä¹‰ï¼Œä¿éšœè·¨è¯­è¨€å“åº”è¯„ä¼°çš„ä¸€è‡´æ€§ä¸å¯é æ€§ã€‚  \nğŸ”¸é‡‡ç”¨å“ˆå¸Œå»é‡ä¸æ—©åœæœºåˆ¶æå‡æœç´¢ç¨³å®šæ€§ï¼Œå¹¶ä»¥å…³é”®è¯åŒ¹é…ä¸æ„å›¾ä¸€è‡´æ€§åŒæŒ‡æ ‡åŠ æƒå®šä¹‰é€‚åº”åº¦å‡½æ•°ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸CC-BOSåœ¨6ä¸ªä¸»æµé»‘ç›’æ¨¡å‹ä¸Šå‡è¾¾100%æ”»å‡»æˆåŠŸç‡ï¼Œæ˜¾è‘—è¶…è¶ŠPAIRã€TAPã€ICRTç­‰SOTAæ–¹æ³•ï¼Œä¸”å¹³å‡æ¯’æ€§å¾—åˆ†æ›´é«˜ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šå¤å…¸ä¸­æ–‡åŸºåº•ï¼ˆ+18%â†’60%ï¼‰ã€å…«ç»´ç­–ç•¥ï¼ˆ+60%â†’100%ï¼‰ä¸ä»¿ç”Ÿä¼˜åŒ–ä¸‰è€…ç¼ºä¸€ä¸å¯ï¼ŒååŒæå‡è¶Šç‹±æ•ˆèƒ½ã€‚  \nğŸ”¸ç¿»è¯‘æ¨¡å—ä½¿ASRä»90%æå‡è‡³100%ï¼ŒéªŒè¯å…¶å¯¹è¯„ä¼°å‡†ç¡®æ€§çš„å…³é”®ä½œç”¨ï¼›åœ¨Llama-Guard-3é˜²å¾¡ä¸‹ä»ä¿æŒ40%æˆåŠŸç‡ï¼Œå±•ç°å¼ºé²æ£’æ€§ã€‚  \nğŸ”¸è·¨æ¨¡å‹è¿ç§»å®éªŒæ˜¾ç¤ºï¼Œç”Ÿæˆçš„å¤å…¸ä¸­æ–‡æç¤ºåœ¨ä¸åŒLLMé—´å…·æœ‰é«˜è¾¾96%çš„è½¬ç§»æˆåŠŸç‡ï¼Œè¯æ˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚  \nğŸ”¸æ‹‰ä¸æ–‡ä¸æ¢µæ–‡æ‰©å±•å®éªŒè¡¨æ˜è¯¥æ¼æ´æºäºâ€œé«˜ç†è§£åŠ›â€”ä½å®‰å…¨å¯¹é½â€çš„ç»“æ„æ€§å¤±é…ï¼Œéå¤å…¸ä¸­æ–‡ç‰¹æœ‰ï¼Œå…·è¯­è¨€æ™®é€‚æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡åˆ›æ–°æ€§åœ¨äºå°†è¯­è¨€å­¦ç‰¹æ€§ï¼ˆå¤å…¸ä¸­æ–‡çš„å‡ç»ƒæ€§ã€å¤šä¹‰æ€§ã€éšå–»æ€§ï¼‰è½¬åŒ–ä¸ºå¯è®¡ç®—çš„å¯¹æŠ—ç»´åº¦ï¼Œçªç ´ä»¥å¾€ä»…ä¾èµ–ç°ä»£è¯­è¨€æˆ–æ¨¡æ¿å·¥ç¨‹çš„å±€é™ï¼›å…¶å…«ç»´ç­–ç•¥ç©ºé—´è®¾è®¡å…¼å…·ç†è®ºæŠ½è±¡æ€§ä¸å·¥ç¨‹å¯æ“ä½œæ€§ï¼Œä¸ºå¤šæ¨¡æ€ã€è·¨æ–‡åŒ–AIå®‰å…¨ç ”ç©¶å¼€è¾Ÿæ–°è·¯å¾„ï¼›ä½†éœ€è­¦æƒ•è¯¥æŠ€æœ¯è¢«æ»¥ç”¨é£é™©ï¼Œå…¶æ ¸å¿ƒä»·å€¼åœ¨äºæš´éœ²å¯¹é½ç›²åŒºï¼Œæ¨åŠ¨æ„å»ºæ›´é²æ£’çš„è·¨è¯­è¨€å®‰å…¨æŠ¤æ ã€‚\n    "
    },
    {
        "title": "SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs",
        "authors": [
            "Guanting Ye",
            "Qiyan Zhao",
            "Wenhao Yu",
            "Liangyu Yuan",
            "Mingkai Li",
            "Xiaofeng Zhang",
            "Jianmin Ji",
            "Yanyong Zhang",
            "Qing Jiang",
            "Ka-Veng Yuen"
        ],
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22716v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22716v1",
        "summary": "3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. The vanilla RoPE formulation fails to preserve essential three-dimensional spatial structures when encoding 3D tokens, and its relative distance computation overlooks angular dependencies, hindering the model's ability to capture directional variations in visual representations. To overcome these limitations, we introduce Spherical Coordinate-based Positional Embedding (SoPE). Our method maps point-cloud token indices into a 3D spherical coordinate space, enabling unified modeling of spatial locations and directional angles. This formulation preserves the inherent geometric structure of point-cloud data, enhances spatial awareness, and yields more consistent and expressive geometric representations for multimodal learning. In addition, we introduce a multi-scale frequency mixing strategy to fuse feature information across different frequency domains. Experimental results on multiple 3D scene benchmarks validate the effectiveness of our approach, while real-world deployment experiments further demonstrate its strong generalization capability.",
        "tag": "ä½ç½®ç¼–ç ä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22716v1ã€ä½ç½®ç¼–ç ä¼˜åŒ–-æ¾³é—¨å¤§å­¦ã€‘SoPE_ Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs.pdf",
        "institution_status": "keep",
        "institution": "æ¾³é—¨å¤§å­¦ã€ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦ã€ä¸Šæµ·äº¤é€šå¤§å­¦ã€åˆè‚¥å·¥ä¸šå¤§å­¦ã€æ–°åŠ å¡å›½ç«‹å¤§å­¦",
        "first_institution": "æ¾³é—¨å¤§å­¦",
        "institution_category": "å…¶ä»–æœºæ„",
        "note": "ğŸ“–æ ‡é¢˜ï¼šSoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs\nğŸŒæ¥æºï¼šarXiv, 2602.22716v1\n\nç¬”è®°æ ‡é¢˜ï¼šSoPEæå‡3Dç©ºé—´æ„ŸçŸ¥\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•æ”¹è¿›3Då¤§è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰å¯¹ç‚¹äº‘å‡ ä½•ç»“æ„ä¸æ–¹å‘ä¿¡æ¯å»ºæ¨¡èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºåŸºäºçƒåæ ‡ç³»çš„ä½ç½®ç¼–ç SoPEï¼Œå°†ç‚¹äº‘tokenæ˜ å°„è‡³(t, r, Î¸, Ï†)å››ç»´ç©ºé—´ï¼Œè”åˆå»ºæ¨¡æ—¶åºã€æ·±åº¦ä¸æ–¹å‘ï¼Œå¹¶å¼•å…¥å¤šå°ºåº¦é¢‘ç‡æ··åˆç­–ç•¥ï¼Œæ˜¾è‘—å¢å¼º3Dç©ºé—´ä¸æ–¹å‘æ„ŸçŸ¥èƒ½åŠ›ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å°†ç‚¹äº‘tokençš„ç¬›å¡å°”åæ ‡(x,y,z)é‡æ˜ å°„ä¸ºçƒåæ ‡(r,Î¸,Ï†)ï¼Œå¹¶ä¿ç•™åŸå§‹æ—¶åºç´¢å¼•tï¼Œæ„å»ºå‡ ä½•æ„ŸçŸ¥çš„å››ç»´ä½ç½®ç´¢å¼•(t,r,Î¸,Ï†)ã€‚  \nğŸ”¸æ‰©å±•RoPEç›¸å¯¹ä½ç½®è®¡ç®—ï¼Œåˆ†åˆ«å¯¹tã€rã€Î¸ã€Ï†å››ä¸ªç»´åº¦ç‹¬ç«‹å»ºæ¨¡ç›¸å¯¹ä½ç§»Î”tã€Î”rã€Î”Î¸ã€Î”Ï†ï¼Œä½¿æ³¨æ„åŠ›æœºåˆ¶èƒ½æ˜¾å¼æ•æ‰ç©ºé—´è·ç¦»ä¸è§’åº¦å˜åŒ–ã€‚  \nğŸ”¸è®¾è®¡å››ç»´é¢‘ç‡åˆ†é…ç­–ç•¥ï¼ˆt:r:Î¸:Ï†=24:2:3:3ï¼‰ï¼Œå°†ä½é¢‘æ®µåˆ†é…ç»™æ—¶åºtä»¥ä¿éšœé•¿ç¨‹ä¸€è‡´æ€§ï¼Œé«˜é¢‘æ®µåˆ†é…ç»™çƒåæ ‡åˆ†é‡ä»¥å¢å¼ºç»†ç²’åº¦ç©ºé—´/æ–¹å‘åˆ†è¾¨åŠ›ã€‚  \nğŸ”¸æå‡ºå¤šå°ºåº¦ç›¸ä½æ··åˆç­–ç•¥ï¼Œå¯¹æ¯ä¸ªåæ ‡åˆ†é‡åº”ç”¨çº¿æ€§ã€å¯¹æ•°å‹ç¼©å’Œå‘¨æœŸæ€§ä¸‰ç§å˜æ¢ï¼Œèåˆä¸åŒå°ºåº¦çš„ä½ç½®å…ˆéªŒï¼Œå…¼é¡¾å±€éƒ¨ç»†èŠ‚ä¸å…¨å±€ç»“æ„ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸SoPEåœ¨Structured3Då’ŒARKitScenesç­‰åŸºå‡†ä¸ŠæŒç»­æå‡å¸ƒå±€ä¼°è®¡ä¸3Dç›®æ ‡æ£€æµ‹æ€§èƒ½ï¼ŒIoUæŒ‡æ ‡å…¨é¢è¶…è¶ŠSpatialLMåŠRoPE-3Dç­‰åŸºçº¿ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šçƒåæ ‡é‡å‚æ•°åŒ–ã€éå‡åŒ€é¢‘ç‡åˆ†é…ã€å¤šå°ºåº¦æ··åˆä¸‰è€…å‡å¸¦æ¥æ˜¾è‘—å¢ç›Šï¼Œå…¶ä¸­çƒåæ ‡å»ºæ¨¡æ˜¯æå‡æ–¹å‘æ„ŸçŸ¥çš„æ ¸å¿ƒã€‚  \nğŸ”¸å¯è§†åŒ–æ˜¾ç¤ºSoPEç”Ÿæˆæ›´å‡è¡¡ã€å…¨å±€åŒ–çš„è·¨æ¨¡æ€æ³¨æ„åŠ›å›¾ï¼Œæœ‰æ•ˆç¼“è§£ä¼ ç»ŸRoPEå¯¼è‡´çš„â€œçƒ­ç‚¹é›†ä¸­â€ä¸ç©ºé—´æ„ŸçŸ¥åå·®é—®é¢˜ã€‚  \nğŸ”¸çœŸå®æœºå™¨äººéƒ¨ç½²éªŒè¯SoPEå¯æ”¯æ’‘ç«¯åˆ°ç«¯åœºæ™¯ç†è§£ä¸ä»»åŠ¡è§„åˆ’ï¼Œæ˜¾è‘—æå‡å°ç‰©ä½“æ£€æµ‹é²æ£’æ€§åŠå¤šè§†è§’é¢„æµ‹ä¸€è‡´æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†ç‰©ç†ç©ºé—´å‡ ä½•å…ˆéªŒï¼ˆçƒåæ ‡ï¼‰æ·±åº¦èå…¥ä½ç½®ç¼–ç è®¾è®¡ï¼Œçªç ´äº†RoPEçº¯åºåˆ—å»ºæ¨¡çš„å±€é™ï¼›å…¶å››ç»´è§£è€¦+å¤šå°ºåº¦æ··åˆçš„æ¶æ„å…¼å…·ç†è®ºåˆç†æ€§ä¸å·¥ç¨‹å®ç”¨æ€§ï¼Œä¸º3Då¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„ä½ç½®å»ºæ¨¡æä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions",
        "authors": [
            "Yue Xu",
            "Qian Chen",
            "Zizhan Ma",
            "Dongrui Liu",
            "Wenxuan Wang",
            "Xiting Wang",
            "Li Xiong",
            "Wenjie Wang"
        ],
        "categories": [
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22680v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22680v1",
        "summary": "Large language models have enabled agents that reason, plan, and interact with tools and environments to accomplish complex tasks. As these agents operate over extended interaction horizons, their effectiveness increasingly depends on adapting behavior to individual users and maintaining continuity across time, giving rise to personalized LLM-powered agents. In such long-term, user-dependent settings, personalization permeates the entire decision pipeline rather than remaining confined to surface-level generation. This survey provides a capability-oriented review of personalized LLM-powered agents. We organize the literature around four interdependent components: profile modeling, memory, planning, and action execution. Using this taxonomy, we synthesize representative methods and analyze how user signals are represented, propagated, and utilized, highlighting cross-component interactions and recurring design trade-offs. We further examine evaluation metrics and benchmarks tailored to personalized agents, summarize application scenarios spanning general assistance to specialized domains, and outline future directions for research and deployment. By offering a structured framework for understanding and designing personalized LLM-powered agents, this survey charts a roadmap toward more user-aligned, adaptive, robust, and deployable agentic systems, accelerating progress from prototype personalization to scalable real-world assistants.",
        "tag": "Agent è®°å¿†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22680v1ã€Agent è®°å¿†-ä¸Šæµ·ç§‘æŠ€å¤§å­¦ã€‘Toward Personalized LLM-Powered Agents_ Foundations, Evaluation, and Future Directions.pdf",
        "institution_status": "keep",
        "institution": "ä¸Šæµ·ç§‘æŠ€å¤§å­¦ã€åŒæµå¤§å­¦",
        "first_institution": "ä¸Šæµ·ç§‘æŠ€å¤§å­¦",
        "institution_category": "å…¶ä»–æœºæ„",
        "note": "ğŸ“–æ ‡é¢˜ï¼šToward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions\nğŸŒæ¥æºï¼šarXiv, 2602.22680v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºä¸ªæ€§åŒ–LLMæ™ºèƒ½ä½“ç³»ç»Ÿæ¡†æ¶\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ç³»ç»Ÿæ€§åœ°è®¾è®¡ã€è¯„ä¼°ä¸éƒ¨ç½²çœŸæ­£é¢å‘ä¸ªä½“ç”¨æˆ·çš„LLMé©±åŠ¨æ™ºèƒ½ä½“ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºé¦–ä¸ªä»¥èƒ½åŠ›ä¸ºä¸­å¿ƒã€è¦†ç›–å…¨å†³ç­–é“¾è·¯çš„å››ç»´ç»Ÿä¸€æ¡†æ¶ï¼ˆç”¨æˆ·ç”»åƒå»ºæ¨¡ã€è®°å¿†ã€è§„åˆ’ã€åŠ¨ä½œæ‰§è¡Œï¼‰ï¼Œå¹¶å»ºç«‹é…å¥—è¯„ä¼°ä½“ç³»ä¸æœªæ¥æ–¹å‘ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å°†ä¸ªæ€§åŒ–å®šä¹‰ä¸ºè´¯ç©¿ä»£ç†å…¨ç”Ÿå‘½å‘¨æœŸçš„ç³»ç»Ÿçº§å±æ€§ï¼Œè€Œéå±€éƒ¨ç”Ÿæˆè°ƒæ•´ï¼Œæ®æ­¤åˆ’åˆ†profile modelingã€memoryã€planningã€action executionå››å¤§äº’ä¾ç»„ä»¶ã€‚  \nğŸ”¸å›´ç»•æ¯ä¸ªç»„ä»¶ï¼Œç³»ç»Ÿæ¢³ç†ä»£è¡¨æ€§æ–¹æ³•ï¼šå¦‚ç”»åƒå»ºæ¨¡åŒºåˆ†persona-basedä¸response-basedè·¯å¾„ï¼›è®°å¿†æ¨¡å—å¯¹æ¯”æ–‡æœ¬å‹ä¸ç»“æ„åŒ–ï¼ˆå›¾/æ ‘/å‘é‡ï¼‰å­˜å‚¨åŠç›¸ä¼¼æ€§/æ¨ç†é©±åŠ¨æ›´æ–°æœºåˆ¶ï¼›è§„åˆ’åˆ†ä¸ºä¸€æ¬¡æ€§çº¦æŸæ³¨å…¥ä¸åé¦ˆé©±åŠ¨è¿­ä»£ä¸¤ç±»èŒƒå¼ï¼›åŠ¨ä½œæ‰§è¡Œç»†åˆ†ä¸ºé¢„åŠ¨ä½œç­–ç•¥é€‰æ‹©ä¸ååŠ¨ä½œç»“æœä¿®æ­£ã€‚  \nğŸ”¸æ„å»ºå¤šç»´åº¦è¯„ä¼°ä½“ç³»ï¼Œæ¶µç›–æœ‰æ•ˆæ€§ã€é€‚åº”æ€§ã€æ³›åŒ–æ€§ã€é²æ£’æ€§ä¸é£é™©äº”å¤§ç›®æ ‡ï¼Œå¹¶å¯¹åº”è‡ªåŠ¨è¯„åˆ†ã€è§„åˆ™æ ¡éªŒã€LLMè¯„æµ‹å™¨ã€LLM-as-a-judgeå››ç±»èŒƒå¼ï¼Œæ•´åˆäº¤äº’å¯¹é½ä¸ç”¨æˆ·æ›¿ä»£ä¸¤å¤§åŸºå‡†å®¶æ—ã€‚  \nğŸ”¸æŒ‰åº”ç”¨åœºæ™¯æ¨ªå‘æ¢³ç†å¯¹è¯åŠ©æ‰‹ã€æƒ…æ„Ÿé™ªä¼´ã€æ•™è‚²ä»£ç†ã€å†…å®¹åˆ›ä½œã€å§”æ‰˜åŠ©ç†åŠå‚ç›´é¢†åŸŸï¼ˆåŒ»ç–—/é‡‘è/æ³•å¾‹/ç§‘ç ”ï¼‰ä¸­çš„ä¸ªæ€§åŒ–å®è·µä¸æŒ‘æˆ˜ï¼Œå¼ºè°ƒä»»åŠ¡ç‰¹æ€§å¯¹ä¸ªæ€§åŒ–æœºåˆ¶çš„å·®å¼‚åŒ–éœ€æ±‚ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸ä¸ªæ€§åŒ–æ•ˆæœé«˜åº¦ä¾èµ–è·¨ç»„ä»¶ååŒâ€”â€”ä¾‹å¦‚profileå»ºæ¨¡è´¨é‡ç›´æ¥å½±å“memoryæ£€ç´¢ç›¸å…³æ€§ï¼Œè€Œmemoryæ›´æ–°ç­–ç•¥åˆåˆ¶çº¦planningä¸­é•¿æœŸåå¥½å»ºæ¨¡çš„å‡†ç¡®æ€§ã€‚  \nğŸ”¸ç°æœ‰æ–¹æ³•æ™®éé¢ä¸´ä¿¡å·ç¨€ç–æ€§ä¸åŠ¨æ€æ€§çŸ›ç›¾ï¼šå†å²æ•°æ®æ”¯æ’‘ç¨³å®šæ€§ï¼Œäº¤äº’æ•°æ®ä¿éšœæ—¶æ•ˆæ€§ï¼Œä½†äºŒè€…èåˆç¼ºä¹ç»Ÿä¸€å»ºæ¨¡åŸåˆ™ï¼Œæ˜“å¯¼è‡´æ¼‚ç§»æˆ–åƒµåŒ–ã€‚  \nğŸ”¸è¯„ä¼°å­˜åœ¨â€œåˆæˆåå·®â€ï¼šä¸»æµåŸºå‡†å¤šä¾èµ–LLMç”Ÿæˆç”¨æˆ·æ•°æ®ï¼Œéš¾ä»¥åæ˜ çœŸå®äººç±»å¤šæ ·æ€§ä¸ä¸»è§‚æ€§ï¼Œä¸”LLM-as-a-judgeç»“æœä¸å®é™…ç”¨æˆ·æ»¡æ„åº¦ç›¸å…³æ€§å­˜ç–‘ã€‚  \nğŸ”¸éšç§ä¸å¯æ§æ€§å°šæœªå†…ç”Ÿäºæ¶æ„ï¼šå¤šæ•°è®°å¿†ä¸ç”»åƒæ¨¡å—ç¼ºä¹ç”¨æˆ·å¯å®¡è®¡ã€å¯ç¼–è¾‘ã€å¯é—å¿˜çš„è®¾è®¡ï¼Œå¯¼è‡´ä¿¡ä»»ç“¶é¢ˆï¼Œå°¤å…¶åœ¨åŒ»ç–—ã€é‡‘èç­‰é«˜æ•åœºæ™¯ä¸­æˆä¸ºè½åœ°å…³é”®éšœç¢ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡æœ€å¤§åˆ›æ–°åœ¨äºçªç ´ä¼ ç»Ÿâ€œä¸ªæ€§åŒ–å³å¾®è°ƒ/æç¤ºå·¥ç¨‹â€çš„çª„å£å¾„è®¤çŸ¥ï¼Œé¦–æ¬¡å°†ä¸ªæ€§åŒ–å‡ç»´ä¸ºæ™ºèƒ½ä½“ç³»ç»Ÿçš„ç»“æ„æ€§ç‰¹å¾ï¼Œå¹¶ä»¥é—­ç¯è§†è§’ï¼ˆç”¨æˆ·è¯·æ±‚â†’å››ç»„ä»¶ååŒâ†’å“åº”â†’åé¦ˆâ†’ç”»åƒæ›´æ–°ï¼‰æ­ç¤ºå…¶å†…åœ¨æ¼”åŒ–é€»è¾‘ã€‚å…¶æå‡ºçš„å››ç»´æ¡†æ¶ä¸ä»…å…·å¤‡å¼ºè§£é‡Šæ€§ï¼Œæ›´ç›´æŒ‡å½“å‰ç¢ç‰‡åŒ–ç ”ç©¶çš„æ•´åˆç¼ºå£ï¼Œä¸ºåç»­å·¥ä½œæä¾›äº†å¯æ‰©å±•ã€å¯è¯Šæ–­ã€å¯éƒ¨ç½²çš„ç³»ç»Ÿæ€§è“å›¾ã€‚\n    "
    }
]