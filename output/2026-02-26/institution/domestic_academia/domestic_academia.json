[
    {
        "title": "Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement",
        "authors": [
            "Shuchen Zhu",
            "Rizhen Hu",
            "Mingze Wang",
            "Mou Sun",
            "Xue Wang",
            "Kun Yuan",
            "Zaiwen Wen"
        ],
        "categories": [
            "cs.LG"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22681v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22681v1",
        "summary": "Pre-training Large Language Models requires immense computational resources, making optimizer efficiency essential. The optimization landscape is highly anisotropic, with loss reduction driven predominantly by progress along flat directions. While matrix-based optimizers such as Muon and SOAP leverage fine-grained curvature information to outperform AdamW, their updates tend toward isotropy -- relatively conservative along flat directions yet potentially aggressive along sharp ones. To address this limitation, we first establish a unified Riemannian Ordinary Differential Equation (ODE) framework that elucidates how common adaptive algorithms operate synergistically: the preconditioner induces a Riemannian geometry that mitigates ill-conditioning, while momentum serves as a Riemannian damping term that promotes convergence. Guided by these insights, we propose LITE, a generalized acceleration strategy that enhances training dynamics by applying larger Hessian damping coefficients and learning rates along flat trajectories. Extensive experiments demonstrate that LITE significantly accelerates both Muon and SOAP across diverse architectures (Dense, MoE), parameter scales (130M--1.3B), datasets (C4, Pile), and learning-rate schedules (cosine, warmup-stable-decay). Theoretical analysis confirms that LITE facilitates faster convergence along flat directions in anisotropic landscapes, providing a principled approach to efficient LLM pre-training. The code is available at https://github.com/SHUCHENZHU/LITE.",
        "tag": "è®­ç»ƒä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22681v1ã€è®­ç»ƒä¼˜åŒ–-åŒ—äº¬å¤§å­¦ã€‘Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement.pdf",
        "institution_status": "keep",
        "institution": "åŒ—äº¬å¤§å­¦ã€Zhejiang Labã€ç¾å›¢",
        "first_institution": "åŒ—äº¬å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šAccelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement\nğŸŒæ¥æºï¼šarXiv, 2602.22681v1\n\nç¬”è®°æ ‡é¢˜ï¼šå¢å¼ºæ‰å¹³æ–¹å‘è®­ç»ƒåŠ¨åŠ›\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨LLMé¢„è®­ç»ƒä¸­å…‹æœå„å‘å¼‚æ€§æŸå¤±æ™¯è§‚å¯¼è‡´çš„ä¼˜åŒ–ç¼“æ…¢ï¼Œå°¤å…¶æå‡æ²¿æ‰å¹³æ–¹å‘ï¼ˆä¸»å¯¼æŸå¤±ä¸‹é™ä½†è¿›å±•ç¼“æ…¢ï¼‰çš„æ”¶æ•›é€Ÿåº¦ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºLITEæ–¹æ³•ï¼Œé¦–æ¬¡å»ºç«‹ç»Ÿä¸€é»æ›¼ODEæ¡†æ¶æ­ç¤ºé¢„æ¡ä»¶å™¨ä¸åŠ¨é‡çš„ååŒæœºåˆ¶ï¼Œå¹¶æ®æ­¤è®¾è®¡é¢å‘æ‰å¹³æ–¹å‘çš„åŠ¨æ€å¢å¼ºç­–ç•¥ï¼Œæ˜¾è‘—åŠ é€ŸMuonå’ŒSOAPç­‰å…ˆè¿›ä¼˜åŒ–å™¨ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºç»Ÿä¸€é»æ›¼ODEæ¡†æ¶ï¼ˆRISHDï¼‰ï¼Œå°†AdamWã€Muonã€SOAPç­‰è‡ªé€‚åº”ä¼˜åŒ–å™¨å»ºæ¨¡ä¸ºå¸¦Hessiané˜»å°¼çš„é»æ›¼æµå½¢ä¸Šçš„æƒ¯æ€§ç³»ç»Ÿï¼Œæ˜ç¡®é¢„æ¡ä»¶å™¨å®šä¹‰é»æ›¼å‡ ä½•ä»¥ç¼“è§£ç—…æ€ï¼ŒåŠ¨é‡åˆ™ä½œä¸ºé»æ›¼é˜»å°¼é¡¹ä¿ƒè¿›æ”¶æ•›ã€‚  \nğŸ”¸åŸºäºè¯¥æ¡†æ¶å‘ç°ç°æœ‰çŸ©é˜µä¼˜åŒ–å™¨æ›´æ–°å¹…å€¼è¶‹äºå„å‘åŒæ€§â€”â€”åœ¨æ‰å¹³æ–¹å‘è¿‡äºä¿å®ˆã€åœ¨å°–é”æ–¹å‘åˆå¯èƒ½æ¿€è¿›ï¼Œä»è€Œé™åˆ¶æ•´ä½“æ•ˆç‡ã€‚  \nğŸ”¸æå‡ºLITEç­–ç•¥ï¼šåœ¨æ‰å¹³å­ç©ºé—´å†…å¢å¤§Hessiané˜»å°¼ç³»æ•°Î²â‚‚å’Œå­¦ä¹ ç‡æ”¾å¤§æ¯”Ï‡ï¼Œè€Œåœ¨å°–é”å­ç©ºé—´ä¿æŒåŸæœ‰è¶…å‚ä»¥ä¿éšœç¨³å®šæ€§ï¼Œå®ç°é€‰æ‹©æ€§åŠ é€Ÿã€‚  \nğŸ”¸è®¾è®¡é«˜æ•ˆå®ç°æ–¹æ¡ˆï¼šåˆ©ç”¨Muon/SOAPè‡ªèº«é¢„æ¡ä»¶å™¨ï¼ˆå¦‚GâŠ¤Gæˆ–GGâŠ¤ï¼‰çš„ä¸»ç‰¹å¾å­ç©ºé—´è¿‘ä¼¼æ‰å¹³æ–¹å‘ï¼Œé¿å…é¢å¤–çŠ¶æ€ä¸é«˜å¼€é”€SVDï¼Œä»…å¼•å…¥å¯å¿½ç•¥çš„NSè¿­ä»£å¼€é”€ã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸LITEåœ¨å¤šç§è®¾ç½®ä¸‹å‡ç¨³å®šé™ä½ç»ˆç«¯æŸå¤±ï¼šæ¶µç›–Denseï¼ˆLLaMA 0.13Bâ€“1.3Bï¼‰ä¸MoEï¼ˆQwenMoE 1Bï¼‰æ¶æ„ã€C4/Pileæ•°æ®é›†ã€cos/wsdå­¦ä¹ ç‡è°ƒåº¦ï¼ŒéªŒè¯å…¶å¼ºæ³›åŒ–æ€§ã€‚  \nğŸ”¸LITE-Lï¼ˆä»…å¢å­¦ä¹ ç‡ï¼‰ä¸LITE-Hï¼ˆä»…å¢é˜»å°¼ï¼‰æ•ˆæœå‡å¼±äºå®Œæ•´LITEï¼Œè¯å®åŒæ—¶è°ƒèŠ‚Ï‡ä¸Î²â‚‚å¯¹æ‰å¹³æ–¹å‘åŠ¨åŠ›å­¦å¢å¼ºçš„å¿…è¦æ€§ã€‚  \nğŸ”¸åœ¨é•¿å‘¨æœŸè®­ç»ƒä¸­ï¼ˆtoken budgetè¾¾200Ã—å‚æ•°é‡ï¼‰ï¼ŒLITEå®ç°çº¦2å€åŠ é€Ÿï¼Œä¸”ç¼©æ”¾å¾‹æ›´ä¼˜ï¼Œè¡¨æ˜å…¶å…·å¤‡å‘æ›´å¤§æ¨¡å‹ä¸æ›´å¤štokenæ‰©å±•çš„æ½œåŠ›ã€‚  \nğŸ”¸æ¶ˆèå®éªŒæ˜¾ç¤ºï¼šè‹¥å°†LITEç­–ç•¥é”™è¯¯æ–½åŠ äºå°–é”æ–¹å‘ï¼ˆå¦‚ç»Ÿä¸€è®¾Î²=0.5/1.0ï¼‰ï¼Œç»ˆç«¯æŸå¤±åè¶…åŸºçº¿ï¼Œå°è¯â€œé€‰æ‹©æ€§â€è®¾è®¡çš„æ­£ç¡®æ€§ä¸å…³é”®æ€§ã€‚  \nğŸ”¸ç†è®ºåˆ†æè¯æ˜ï¼šLITEèƒ½åŠ å¿«æ²¿æ‰å¹³æµå½¢ï¼ˆRiverï¼‰çš„å¸å¼•é€Ÿåº¦ï¼Œå¹¶ä½¿æŠ•å½±è½¨è¿¹zâ‚œæ»¡è¶³âˆ«Î·â‚œâ€–P_R Fâ»Â¹âˆ‡f(zâ‚œ)â€–Â²_F dt â‰¤ 2Î”f/(Ï‡Î²â‚‚)ï¼Œå³Ï‡ä¸Î²â‚‚è¶Šå¤§ï¼Œæ¢¯åº¦ä¸‹é™ç§¯åˆ†ä¸Šç•Œè¶Šç´§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºï¼šä¸€æ˜¯é¦–æ¬¡ä»è¿ç»­æ—¶é—´é»æ›¼æµå½¢è§†è§’ç»Ÿä¸€åˆ»ç”»ä¸»æµè‡ªé€‚åº”ä¼˜åŒ–å™¨ï¼Œæ­ç¤ºé¢„æ¡ä»¶å™¨ä¸åŠ¨é‡çš„å‡ ä½•ååŒæœ¬è´¨ï¼›äºŒæ˜¯çªç ´â€œå„å‘åŒæ€§æ›´æ–°â€å±€é™ï¼Œæå‡ºé¦–ä¸ªé¢å‘æ‰å¹³æ–¹å‘çš„ã€å…¼å…·ç†è®ºä¿è¯ä¸å·¥ç¨‹å¯è¡Œæ€§çš„åŠ¨æ€å¢å¼ºèŒƒå¼ï¼ˆLITEï¼‰ï¼›ä¸‰æ˜¯å°†Hessiané˜»å°¼ä»å…¨å±€å›ºå®šç³»æ•°æ‹“å±•ä¸ºæ–¹å‘è‡ªé€‚åº”ï¼Œèµ‹äºˆåŠ¨é‡æœºåˆ¶æ›´å¼ºçš„äºŒé˜¶å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºLLMé«˜æ•ˆé¢„è®­ç»ƒæä¾›äº†æ–°åŸç†ä¸æ–°å·¥å…·ã€‚\n    "
    },
    {
        "title": "AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning",
        "authors": [
            "Yutong Wang",
            "Siyuan Xiong",
            "Xuebo Liu",
            "Wenkang Zhou",
            "Liang Ding",
            "Miao Zhang",
            "Min Zhang"
        ],
        "categories": [
            "cs.AI",
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.23258v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23258v1",
        "summary": "While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.",
        "tag": "Agent åä½œä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.23258v1ã€Agent åä½œä¼˜åŒ–-å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ã€‘AgentDropoutV2_ Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning.pdf",
        "institution_status": "keep",
        "institution": "å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ã€é˜¿é‡Œ",
        "first_institution": "å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šAgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning\nğŸŒæ¥æºï¼šarXiv, 2602.23258v1\n\nç¬”è®°æ ‡é¢˜ï¼šåŠ¨æ€æ‹¦æˆªçº é”™é˜²ä¼ æ’­\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸é‡æ–°è®­ç»ƒçš„å‰æä¸‹ï¼Œäºæ¨ç†æ—¶å®æ—¶é˜»æ–­å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­é”™è¯¯ä¿¡æ¯çš„çº§è”ä¼ æ’­ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºAgentDropoutV2æ¡†æ¶ï¼Œé¦–æ¬¡å®ç°æµ‹è¯•æ—¶â€œå…ˆè¿­ä»£çº æ­£ã€å†ä¸å¯ä¿®å¤åˆ™è£å‰ªâ€çš„åŠ¨æ€ä¿¡æ¯æµä¼˜åŒ–æœºåˆ¶ï¼Œæ˜¾è‘—æå‡MASé²æ£’æ€§ä¸å‡†ç¡®æ€§ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡æµ‹è¯•æ—¶æ‹¦æˆª-çº æ­£-è£å‰ªä¸‰æ€é—¨æ§æœºåˆ¶ï¼šå¯¹æ¯ä¸ªæ™ºèƒ½ä½“è¾“å‡ºå®æ—¶æ‹¦æˆªï¼ŒåŸºäºæ£€ç´¢åˆ°çš„å¤±è´¥æ¨¡å¼æŒ‡ç¤ºå™¨è¿›è¡Œå¤šè½®è¯Šæ–­ä¸åé¦ˆé©±åŠ¨ä¿®æ­£ã€‚  \nğŸ”¸æ„å»ºå¤±è´¥é©±åŠ¨çš„æŒ‡ç¤ºå™¨æ± ï¼šç¦»çº¿æŒ–æ˜MASå¤±è´¥è½¨è¿¹ï¼Œç”±æ•™å¸ˆæ¨¡å‹æç‚¼ç»“æ„åŒ–é”™è¯¯æ¨¡å¼ï¼ˆå«åç§°ã€å®šä¹‰ã€è§¦å‘æ¡ä»¶ï¼‰ï¼Œç»åŒé˜¶æ®µå»é‡ç¡®ä¿é«˜ç†µç´§å‡‘æ€§ã€‚  \nğŸ”¸å¼•å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ£€ç´¢ç­–ç•¥ï¼šé€šè¿‡æå–ä»»åŠ¡åœºæ™¯ä¸åŠ¨ä½œç±»å‹å…³é”®è¯ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œè¯­ä¹‰åŒ¹é…æœ€ç›¸å…³çš„Kä¸ªæŒ‡ç¤ºå™¨ï¼Œå®ç°ç²¾å‡†é”™è¯¯å®šä½ã€‚  \nğŸ”¸è®¾ç½®å…¨å±€å›é€€æœºåˆ¶ï¼šå½“æœ‰æ•ˆæ¶ˆæ¯æ•°ä½äºå®‰å…¨é˜ˆå€¼Î³æ—¶è§¦å‘ç³»ç»Ÿé‡ç½®ï¼Œé˜²æ­¢å› è¿‡åº¦è£å‰ªå¯¼è‡´åä½œç»“æ„å´©æºƒã€‚  \nğŸ”¸æ”¯æŒé›¶æ ·æœ¬æ³›åŒ–ï¼šæä¾›é€šç”¨é€»è¾‘æ£€æŸ¥æŒ‡ç¤ºå™¨ï¼Œåœ¨æ— é¢†åŸŸæŒ‡æ ‡æ± æ—¶ä»å¯å¯åŠ¨åŸºç¡€çº é”™æµç¨‹ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨9ä¸ªæ•°å­¦åŸºå‡†ä¸Šå¹³å‡å‡†ç¡®ç‡æå‡6.3ä¸ªç™¾åˆ†ç‚¹ï¼Œå°¤å…¶åœ¨AIME25ç­‰é«˜éš¾ä»»åŠ¡ä¸­æå‡è¾¾6.67%ï¼ŒéªŒè¯çº é”™æœ‰æ•ˆæ€§ã€‚  \nğŸ”¸è£å‰ªç‡ä¸ä»»åŠ¡éš¾åº¦å¼ºç›¸å…³ï¼šç®€å•ä»»åŠ¡é¦–è½®é€šè¿‡ç‡è¶…60%ï¼Œè€ŒAIME24/25æ‹’ç»ç‡è¶…60%ï¼Œè¡¨æ˜ç³»ç»Ÿèƒ½è‡ªé€‚åº”è°ƒèŠ‚å¹²é¢„å¼ºåº¦ã€‚  \nğŸ”¸æŒ‡ç¤ºå™¨æ± å…·å¤‡è·¨æ¨¡å‹è¿ç§»èƒ½åŠ›ï¼šQwen3-8Bæ„å»ºçš„æ± ç›´æ¥ç”¨äºQwen3-4Bä»è·ç¨³å®šå¢ç›Šï¼Œè¯å®é”™è¯¯æ¨¡å¼å…·æœ‰å°ºåº¦ä¸å˜æ€§ã€‚  \nğŸ”¸è·¨åŸŸæ³›åŒ–æœ‰æ•ˆï¼šåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­å¹³å‡å‡†ç¡®ç‡æå‡2.21%ï¼Œå¤æ‚æ•°æ®é›†ï¼ˆå¦‚CodeContestsï¼‰æå‡è¾¾3.2%ï¼Œè¯´æ˜æœºåˆ¶å…·é€šç”¨æ€§ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜å…³é”®è®¾è®¡å¿…è¦æ€§ï¼šéšæœºæ£€ç´¢æŒ‡ç¤ºå™¨ä½¿æ€§èƒ½åä½äºåŸºçº¿ï¼›å»é™¤å»é‡å¯¼è‡´å‡†ç¡®ç‡ä¸‹é™2.22%ï¼Œå‡¸æ˜¾æ± è´¨é‡é‡è¦æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ¨äºå°†ä¼ ç»Ÿé™æ€å‰ªæå‡çº§ä¸ºâ€œå¯é€†å¼åŠ¨æ€å‡€åŒ–â€â€”â€”ä»¥å¤±è´¥çŸ¥è¯†ä¸ºå…ˆéªŒã€ä»¥æ£€ç´¢ä¸ºå¯¼èˆªã€ä»¥è¿­ä»£ä¸ºæ‰‹æ®µï¼Œåœ¨æ¨ç†é“¾ä¸­åµŒå…¥è½»é‡çº§ä½†é«˜ç²¾åº¦çš„å®æ—¶çº é”™å±‚ã€‚å…¶æ ¸å¿ƒçªç ´æ˜¯è§£è€¦äº†é”™è¯¯æ£€æµ‹ä¸ä¿®æ­£èƒ½åŠ›ï¼Œæ—¢é¿å…äº†å¾®è°ƒä¾èµ–ï¼Œåˆè¶…è¶Šäº†è¢«åŠ¨è¿‡æ»¤ï¼Œä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å¯ä¿¡éƒ¨ç½²æä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "AgentVista: Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios",
        "authors": [
            "Zhaochen Su",
            "Jincheng Gao",
            "Hangyu Guo",
            "Zhenhua Liu",
            "Lueyang Zhang",
            "Xinyu Geng",
            "Shijue Huang",
            "Peng Xia",
            "Guanyu Jiang",
            "Cheng Wang",
            "Yue Zhang",
            "Yi R. Fung",
            "Junxian He"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.23166v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23166v1",
        "summary": "Real-world multimodal agents solve multi-step workflows grounded in visual evidence. For example, an agent can troubleshoot a device by linking a wiring photo to a schematic and validating the fix with online documentation, or plan a trip by interpreting a transit map and checking schedules under routing constraints. However, existing multimodal benchmarks mainly evaluate single-turn visual reasoning or specific tool skills, and they do not fully capture the realism, visual subtlety, and long-horizon tool use that practical agents require. We introduce AgentVista, a benchmark for generalist multimodal agents that spans 25 sub-domains across 7 categories, pairing realistic and detail-rich visual scenarios with natural hybrid tool use. Tasks require long-horizon tool interactions across modalities, including web search, image search, page navigation, and code-based operations for both image processing and general programming. Comprehensive evaluation of state-of-the-art models exposes significant gaps in their ability to carry out long-horizon multimodal tool use. Even the best model in our evaluation, Gemini-3-Pro with tools, achieves only 27.3% overall accuracy, and hard instances can require more than 25 tool-calling turns. We expect AgentVista to accelerate the development of more capable and reliable multimodal agents for realistic and ultra-challenging problem solving.",
        "tag": "å¤šæ¨¡æ€æ™ºèƒ½ä½“è¯„ä¼°åŸºå‡†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.23166v1ã€å¤šæ¨¡æ€æ™ºèƒ½ä½“è¯„ä¼°åŸºå‡†-é¦™æ¸¯ç§‘æŠ€å¤§å­¦ã€‘AgentVista_ Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios.pdf",
        "institution_status": "keep",
        "institution": "é¦™æ¸¯ç§‘æŠ€å¤§å­¦ã€åŒ—å¡ç½—æ¥çº³å¤§å­¦æ•™å ‚å±±åˆ†æ ¡ã€æµ™æ±Ÿå¤§å­¦ã€æ–°åŠ å¡å›½ç«‹å¤§å­¦",
        "first_institution": "é¦™æ¸¯ç§‘æŠ€å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šAgentVista: Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios\nğŸŒæ¥æºï¼šarXiv, 2602.23166v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºè¶…éš¾å¤šæ¨¡æ€ä»£ç†è¯„æµ‹åŸºå‡†\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è¯„ä¼°å¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨çœŸå®ã€å¤æ‚ã€é•¿ç¨‹è§†è§‰ä»»åŠ¡ä¸­çš„ç»¼åˆèƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºäº†AGENTVISTAâ€”â€”é¦–ä¸ªèšç„¦çœŸå®åœºæ™¯ã€ç»†ç²’åº¦è§†è§‰ä¾èµ–ã€è·¨æ¨¡æ€é•¿ç¨‹å·¥å…·ååŒçš„å¤šæ¨¡æ€é€šç”¨æ™ºèƒ½ä½“è¯„æµ‹åŸºå‡†ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è®¾è®¡ä¸ƒå¤§ç±»25å­é¢†åŸŸå…±209ä¸ªä»»åŠ¡ï¼Œå…¨éƒ¨åŸºäºçœŸå®å›¾åƒä¸ç”¨æˆ·éœ€æ±‚ï¼Œå¼ºè°ƒè§†è§‰è¯æ®ä¸å¯æ›¿ä»£æ€§ã€‚  \nğŸ”¸æ¯é¡¹ä»»åŠ¡å¼ºåˆ¶è¦æ±‚å¤šå·¥å…·äº¤é”™è°ƒç”¨ï¼ˆå«ç½‘é¡µæœç´¢ã€å›¾åƒæœç´¢ã€é¡µé¢è®¿é—®ã€ä»£ç è§£é‡Šå™¨ï¼‰ï¼Œä¸”è‡³å°‘è·¨è¶Šä¸¤ç±»å·¥å…·ã€‚  \nğŸ”¸æ„å»ºå››é˜¶æ®µä¸¥æ ¼æ•°æ®æµæ°´çº¿ï¼šæ¨¡å‹è¾…åŠ©åˆç­›â†’ä¸“å®¶é‡å†™ä¸ºè‡ªç„¶ç”¨æˆ·è¯·æ±‚â†’æ‰§è¡ŒéªŒè¯ç¡®ä¿å·¥å…·å¿…è¦æ€§â†’åŒè½®äººå·¥å¤æ ¸è§†è§‰ä¾æ®ä¸ç­”æ¡ˆå¯éªŒè¯æ€§ã€‚  \nğŸ”¸é‡‡ç”¨ç»Ÿä¸€å¯æ§å·¥å…·ç¯å¢ƒï¼Œæ‰€æœ‰å·¥å…·å…·å¤‡ç»“æ„åŒ–è¾“å…¥è¾“å‡ºä¸è¯¦ç»†æè¿°ï¼Œæ”¯æŒå¯å¤ç°è¯„ä¼°ã€‚  \nğŸ”¸å¼•å…¥â€œè§†è§‰ä¸­å¿ƒæ€§â€åŸåˆ™ï¼šç¦æ­¢æ–‡æœ¬ç›´ç­”ï¼Œå…³é”®çº¿ç´¢å¿…é¡»æ¥è‡ªå›¾åƒç»†èŠ‚ï¼ˆå¦‚å¾®å°æ ‡è¯†ã€æ¨¡ç³Šæ–‡å­—ã€å¤šè§†è§’æ¯”å¯¹ï¼‰ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸å½“å‰æœ€ä¼˜æ¨¡å‹GEMINI-3-PROæ•´ä½“å‡†ç¡®ç‡ä»…27.3%ï¼Œå¹³å‡éœ€12.67æ­¥å·¥å…·è°ƒç”¨ï¼Œè¯å®é•¿ç¨‹å¤šæ¨¡æ€å·¥å…·ä½¿ç”¨ä»æ˜¯é‡å¤§ç“¶é¢ˆã€‚  \nğŸ”¸è§†è§‰è¯¯è¯†åˆ«æ˜¯é¦–è¦å¤±è´¥åŸå› ï¼ˆå æ¯”è¿‘40%ï¼‰ï¼Œè¿œè¶…çŸ¥è¯†å¹»è§‰ã€è®¡ç®—é”™è¯¯ç­‰ï¼Œå‡¸æ˜¾ç»†ç²’åº¦è§†è§‰ç†è§£çš„è„†å¼±æ€§ã€‚  \nğŸ”¸å¤šå›¾è¾“å…¥åè€Œæå‡æ€§èƒ½ï¼ˆå¦‚GEMINI-3-PROä»23.7%å‡è‡³36.8%ï¼‰ï¼Œè¯´æ˜ä¿¡æ¯äº’è¡¥å¯ç¼“è§£å•å›¾æ­§ä¹‰ï¼Œç“¶é¢ˆä¸åœ¨å›¾åƒæ•°é‡è€Œåœ¨æ¨ç†è¿è´¯æ€§ã€‚  \nğŸ”¸ä¸åŒæ¨¡å‹å·¥å…·åå¥½æ˜¾è‘—ï¼šGPTç³»åˆ—é‡åº¦ä¾èµ–ä»£ç è§£é‡Šå™¨ï¼ˆå°¤å…¶cropæ“ä½œï¼‰ï¼ŒGemini/Claudeæ›´å€¾å‘æ£€ç´¢é©±åŠ¨ï¼Œåæ˜ èƒ½åŠ›åˆ†å¸ƒä¸å‡ã€‚  \nğŸ”¸ç§»é™¤ä»»ä¸€å·¥å…·ç±»å‹å‡å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå…¨å·¥å…·ååŒè®¾ç½®è¡¨ç°æœ€ä½³ï¼ŒéªŒè¯æ··åˆå·¥ä½œæµè®¾è®¡çš„åˆç†æ€§ä¸å¿…è¦æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œæœ€å¤§åˆ›æ–°åœ¨äºå°†â€œçœŸå®æ„Ÿâ€ç³»ç»Ÿæ€§æ³¨å…¥è¯„æµ‹è®¾è®¡ï¼šä»å›¾åƒæ¥æºï¼ˆ30ä¸‡+çœŸå®åœºæ™¯ï¼‰ã€ä»»åŠ¡æ„é€ ï¼ˆç¤¾åŒºæ±‚åŠ©/æ—¥å¸¸æˆªå›¾ï¼‰ã€çº¦æŸè¡¨è¾¾ï¼ˆæ—¶é—´/å®‰å…¨/å…¼å®¹æ€§ç­‰ç”¨æˆ·å¼è¡¨è¿°ï¼‰åˆ°å¤±è´¥å½’å› ï¼ˆèšç„¦è§†è§‰é”šå®šåå·®ï¼‰ï¼Œå…¨é¢è§„é¿äººå·¥ç®€åŒ–é™·é˜±ï¼›å…¶â€œè¶…æŒ‘æˆ˜æ€§â€å¹¶éæºäºæŠ½è±¡éš¾åº¦ï¼Œè€Œæºäºå¯¹ç°å®ä¸–ç•Œè§†è§‰æ¨¡ç³Šæ€§ã€å·¥å…·é“¾æ‰°åŠ¨æ€§ä¸å¤šæ­¥ä¾èµ–è„†å¼±æ€§çš„å¿ å®å»ºæ¨¡ï¼Œä¸ºä¸‹ä¸€ä»£å¤šæ¨¡æ€ä»£ç†å‘å±•æä¾›äº†ä¸å¯æ›¿ä»£çš„æ ‡å°ºã€‚\n    "
    },
    {
        "title": "ContextRL: Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL",
        "authors": [
            "Xingyu Lu",
            "Jinpeng Wang",
            "YiFan Zhang",
            "Shijie Ma",
            "Xiao Hu",
            "Tianke Zhang",
            "Haonan fan",
            "Kaiyu Jiang",
            "Changyi Liu",
            "Kaiyu Tang",
            "Bin Wen",
            "Fan Yang",
            "Tingting Gao",
            "Han Li",
            "Chun Yuan"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22623v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22623v1",
        "summary": "We propose ContextRL, a novel framework that leverages context augmentation to overcome these bottlenecks. Specifically, to enhance Identifiability, we provide the reward model with full reference solutions as context, enabling fine-grained process verification to filter out false positives (samples with the right answer but low-quality reasoning process). To improve Reachability, we introduce a multi-turn sampling strategy where the reward model generates mistake reports for failed attempts, guiding the policy to \"recover\" correct responses from previously all-negative groups. Experimental results on 11 perception and reasoning benchmarks show that ContextRL significantly improves knowledge discovery efficiency. Notably, ContextRL enables the Qwen3-VL-8B model to achieve performance comparable to the 32B model, outperforming standard RLVR baselines by a large margin while effectively mitigating reward hacking. Our in-depth analysis reveals the significant potential of contextual information for improving reward model accuracy and document the widespread occurrence of reward hacking, offering valuable insights for future RLVR research.",
        "tag": "è¿‡ç¨‹å¥–åŠ±",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22623v1ã€è¿‡ç¨‹å¥–åŠ±-æ¸…åå¤§å­¦ã€‘ContextRL_ Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL.pdf",
        "institution_status": "keep",
        "institution": "æ¸…åå¤§å­¦ã€å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ã€ä¸­å›½ç§‘å­¦é™¢",
        "first_institution": "æ¸…åå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šContextRL: Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL\nğŸŒæ¥æºï¼šarXiv, 2602.22623v1\n\nç¬”è®°æ ‡é¢˜ï¼šä¸Šä¸‹æ–‡å¢å¼ºçš„RLVRæ¡†æ¶\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•çªç ´å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ä¸­å› å¥–åŠ±ä¿¡å·ä¸å¯é å’Œæ­£æ ·æœ¬ç¨€ç–å¯¼è‡´çš„çŸ¥è¯†å‘ç°æ•ˆç‡ç“¶é¢ˆï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºContextRLæ¡†æ¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å¢å¼ºå¥–åŠ±æ¨¡å‹ä¸ç­–ç•¥æ¨¡å‹ï¼Œæ˜¾è‘—æå‡MLLMåœ¨RLè®­ç»ƒä¸­çš„çŸ¥è¯†å‘ç°æ•ˆç‡ï¼Œç¼“è§£å¥–åŠ±ä½œå¼Šå¹¶æé«˜å°æ¨¡å‹æ€§èƒ½ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è¯†åˆ«RLVRä¸¤å¤§ä¿¡æ¯ç“¶é¢ˆï¼šå¯è¾¾æ€§ç“¶é¢ˆï¼ˆéš¾é—®é¢˜ä¸‹éš¾ä»¥é‡‡æ ·åˆ°æ­£ç¡®å“åº”ï¼‰ä¸å¯è¾¨è¯†æ€§ç“¶é¢ˆï¼ˆä»…ç”¨æœ€ç»ˆç­”æ¡ˆä½œå‚è€ƒæ˜“äº§ç”Ÿå‡é˜³æ€§ï¼Œå¯¼è‡´å¥–åŠ±ä½œå¼Šï¼‰ã€‚  \nğŸ”¸è®¾è®¡ä¸Šä¸‹æ–‡å¢å¼ºå¥–åŠ±æ¨¡å‹ï¼šå°†å®Œæ•´å‚è€ƒè§£ï¼ˆå«æ¨ç†è¿‡ç¨‹+ç­”æ¡ˆï¼‰ä½œä¸ºè¾“å…¥ï¼Œæ”¯æŒç»†ç²’åº¦è¿‡ç¨‹éªŒè¯ï¼Œé™ä½æ¡ä»¶ç†µğ»(ğ¶|ğ‘‡)ï¼ŒæŠ‘åˆ¶å‡é˜³æ€§ã€‚  \nğŸ”¸æ„å»ºä¸Šä¸‹æ–‡å¢å¼ºç­–ç•¥æ¨¡å‹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µé‡‡æ ·â€”â€”ç¬¬ä¸€é˜¶æ®µå¸¸è§„é‡‡æ ·ï¼›è‹¥å…¨ä¸ºè´Ÿæ ·æœ¬ï¼Œåˆ™ç¬¬äºŒé˜¶æ®µå°†é”™è¯¯æŠ¥å‘Šä¸åŸå§‹æŸ¥è¯¢æ‹¼æ¥ä¸ºæ–°ä¸Šä¸‹æ–‡ï¼Œå¼•å¯¼æ¨¡å‹â€œæ¢å¤â€æ­£ç¡®å“åº”ã€‚  \nğŸ”¸è®¾è®¡æ··åˆè®­ç»ƒæœºåˆ¶ï¼šå¯¹ç¬¬ä¸€é˜¶æ®µå«æ­£æ ·æœ¬çš„ç»„ä½¿ç”¨æ ‡å‡†GRPOä¼˜åŒ–ï¼›å¯¹ç¬¬äºŒé˜¶æ®µç”Ÿæˆçš„æ­£æ ·æœ¬ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å›æ»šè½¬ä¸ºå•è½®æ ·æœ¬ï¼Œå¹¶åŠ å…¥ç¼©æ”¾ä¼˜åŠ¿å‡½æ•°ä¸é€‰æ‹©æ€§KLæ­£åˆ™åŒ–è¿›è¡Œç¨³å®šè®­ç»ƒã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸ä¸Šä¸‹æ–‡å¢å¼ºä½¿32Bå¥–åŠ±æ¨¡å‹å¯¹å‡é˜³æ€§æ ·æœ¬çš„è¯†åˆ«ç‡ä»50.03%æå‡è‡³81.98%ï¼Œä¸”ç¼©å°äº†32Bä¸235Bæ¨¡å‹é—´çš„æ€§èƒ½å·®è·ï¼Œè¯æ˜å°æ¨¡å‹å€ŸåŠ©ä¸°å¯Œä¸Šä¸‹æ–‡å¯æ¥è¿‘å¤§æ¨¡å‹åˆ¤åˆ«èƒ½åŠ›ã€‚  \nğŸ”¸å‡é˜³æ€§æ ·æœ¬ä¸¥é‡æŸå®³æ¨¡å‹æ€§èƒ½ï¼šSFTå®éªŒæ˜¾ç¤ºï¼Œè®­ç»ƒæ•°æ®ä¸­æ¯å¢åŠ 10%å‡é˜³æ€§ï¼Œæ•°å­¦ç±»åŸºå‡†å¹³å‡æ€§èƒ½ä¸‹é™çº¦0.5â€“1.5ä¸ªç™¾åˆ†ç‚¹ã€‚  \nğŸ”¸ContextRLå•è½®è®­ç»ƒå³å¯ä½¿Qwen3-VL-8Båœ¨11ä¸ªåŸºå‡†ä¸Šå¹³å‡è¶…è¶ŠSFTã€GRPOã€DAPOç­‰åŸºçº¿5.25%ï¼ˆæ¨ç†ï¼‰å’Œ5.91%ï¼ˆæ„ŸçŸ¥ï¼‰ï¼Œæ€§èƒ½é€¼è¿‘32Bæ¨¡å‹ã€‚  \nğŸ”¸å®šé‡åˆ†æè¡¨æ˜ContextRLçš„ä¿¡æ¯å¢ç›Šè¾¾çº¦17%ï¼Œå…¶ä¸­8.9%æ¥è‡ªå‡é˜³æ€§æ¶ˆé™¤ï¼Œ8.56%æ¥è‡ªç¬¬äºŒé˜¶æ®µæˆåŠŸæ¢å¤æ­£æ ·æœ¬ï¼Œè¯å®å…¶åŒè·¯å¾„çªç ´ç“¶é¢ˆçš„æœ‰æ•ˆæ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†â€œä¸Šä¸‹æ–‡â€ä»æç¤ºå·¥ç¨‹å±‚é¢æå‡ä¸ºRLç³»ç»Ÿçº§è®¾è®¡è¦ç´ ï¼Œä¸ä»…è§£å†³å¥–åŠ±ä½œå¼Šè¿™ä¸€é•¿æœŸéšæ‚£ï¼Œæ›´é€šè¿‡å¯è§£é‡Šçš„é”™è¯¯åé¦ˆæœºåˆ¶èµ‹äºˆç­–ç•¥æ¨¡å‹è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›ï¼›å…¶æå‡ºçš„å¯è¾¾æ€§-å¯è¾¨è¯†æ€§åŒç“¶é¢ˆåˆ†ææ¡†æ¶ï¼Œä¸ºåç»­RLVRç ”ç©¶æä¾›äº†æ™®é€‚æ€§è¯Šæ–­å·¥å…·ã€‚\n    "
    },
    {
        "title": "Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning",
        "authors": [
            "Hao Yu",
            "Shuning Jia",
            "Guanghao Li",
            "Wenhao Jiang",
            "Chun Yuan"
        ],
        "categories": [
            "cs.LG"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22703v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22703v1",
        "summary": "Vision-language models (VLMs) often struggle with geometric reasoning due to their limited perception of fundamental diagram elements. To tackle this challenge, we introduce GeoPerceive, a benchmark comprising diagram instances paired with domain-specific language (DSL) representations, along with an efficient automatic data generation pipeline. This design enables the isolated evaluation of geometric perception independently from reasoning. To exploit the data provided by GeoPerceive for enhancing the geometric perception capabilities of VLMs, we propose GeoDPO, a translator-guided reinforcement learning (RL) framework. GeoDPO employs an NL-to-DSL translator, which is trained on synthetic pairs generated by the data engine of GeoPerceive, to bridge natural language and DSL. This translator facilitates the computation of fine-grained, DSL-level scores, which serve as reward signals in reinforcement learning. We assess GeoDPO on both in-domain and out-of-domain datasets, spanning tasks in geometric perception as well as downstream reasoning. Experimental results demonstrate that, while supervised fine-tuning (SFT) offers only marginal improvements and may even impair performance in out-of-domain scenarios, GeoDPO achieves substantial gains: $+26.5\\%$ on in-domain data, $+8.0\\%$ on out-of-domain data, and $+39.0\\%$ on downstream reasoning tasks. These findings underscore the superior performance and generalization ability of GeoDPO over SFT. All codes are released at https://github.com/Longin-Yu/GeoPerceive\n  to ensure reproducibility.",
        "tag": "å‡ ä½•è¯„ä¼°åŸºå‡†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22703v1ã€å‡ ä½•è¯„ä¼°åŸºå‡†-æ¸…åå¤§å­¦ã€‘Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning.pdf",
        "institution_status": "keep",
        "institution": "æ¸…åå¤§å­¦ã€å¹¿ä¸œäººå·¥æ™ºèƒ½ä¸æ•°å­—ç»æµå®éªŒå®¤ï¼ˆæ·±åœ³ï¼‰",
        "first_institution": "æ¸…åå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šEnhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning\nğŸŒæ¥æºï¼šarXiv, 2602.22703v1\n\nç¬”è®°æ ‡é¢˜ï¼šæå‡VLMå‡ ä½•æ„ŸçŸ¥èƒ½åŠ›\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•å‡†ç¡®è¯„ä¼°å¹¶æœ‰æ•ˆå¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹å¯¹å‡ ä½•å›¾å…ƒï¼ˆç‚¹ã€çº¿ã€åœ†åŠå…¶ç©ºé—´å…³ç³»ï¼‰çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºGEOPERCEIVEåŸºå‡†ä¸GEODPOæ¡†æ¶ï¼Œé¦–æ¬¡å®ç°å‡ ä½•æ„ŸçŸ¥çš„è§£è€¦å¼è¯„æµ‹ä¸ translator-guided å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡VLMåœ¨åŸŸå†…ã€åŸŸå¤–åŠä¸‹æ¸¸æ¨ç†ä»»åŠ¡ä¸­çš„å‡ ä½•ç†è§£é²æ£’æ€§ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºGEODSLâ€”â€”ä¸€ç§ä¸€ä¸€æ˜ å°„ã€æ— æ­§ä¹‰çš„å‡ ä½•é¢†åŸŸä¸“ç”¨è¯­è¨€ï¼Œç¡®ä¿æ¯ä¸ªå›¾å¯¹åº”å”¯ä¸€ç¨‹åºï¼Œæ”¯æ’‘ç²¾ç¡®ç¨‹åºçº§è¯„ä¼°ã€‚  \nğŸ”¸è®¾è®¡GEOPERCEIVEè‡ªåŠ¨æ•°æ®å¼•æ“ï¼šç”Ÿæˆå¼•æ“é‡‡æ ·DSLç¨‹åºï¼Œæ±‚è§£å¼•æ“é€šè¿‡å¯å¾®ä¼˜åŒ–æ¸²æŸ“åƒç´ å›¾ï¼Œæ”¯æŒå¤æ‚åº¦å¯æ§çš„å¤§è§„æ¨¡åˆæˆæ•°æ®ç”Ÿæˆã€‚  \nğŸ”¸æå‡ºGEODPOæ¡†æ¶ï¼šä¸ç›´æ¥å¾®è°ƒVLMè¾“å‡ºDSLï¼Œè€Œæ˜¯è®­ç»ƒNL-to-DSLç¿»è¯‘å™¨ï¼Œå°†å…¶è½¯åŒ¹é…åˆ†æ•°è½¬åŒ–ä¸ºDPOå¥–åŠ±ä¿¡å·ï¼Œå®ç°è‡ªç„¶è¯­è¨€è¾“å‡ºä¸‹çš„ç»†ç²’åº¦å‡ ä½•ç›‘ç£ã€‚  \nğŸ”¸é‡‡ç”¨åå¥½å­¦ä¹ èŒƒå¼ï¼šåŸºäºå‚è€ƒæ¨¡å‹é‡‡æ ·å¤šæ¡NLæè¿°ï¼ŒæŒ‰ç¿»è¯‘å¾—åˆ†æ’åºæ„é€ èƒœ/è´Ÿæ ·æœ¬å¯¹ï¼Œç”¨DPOæŸå¤±å¯¹é½å‡ ä½•ä¸€è‡´æ€§åå¥½ã€‚  \nğŸ”¸ä¿æŒNLé¢„è®­ç»ƒæµå½¢ï¼šæ•´ä¸ªæµç¨‹ä¸­VLMå§‹ç»ˆè¾“å‡ºè‡ªç„¶è¯­è¨€ï¼Œé¿å…å› å¼ºåˆ¶DSLè¾“å‡ºå¯¼è‡´çš„åˆ†å¸ƒåç§»ä¸æ•°æ®é¥¥æ¸´é—®é¢˜ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸GEODPOåœ¨GEOPERCEIVEä¸»æµ‹è¯•é›†ä¸Šå¹³å‡æå‡+26.5%ï¼Œè¿œè¶…SFTï¼ˆ+10.5%ï¼‰ï¼Œä¸”åœ¨ç‚¹ã€çº¿ã€çº¦æŸç­‰å„è¦ç´ ä¸Šå‡ç¨³å®šå¢ç›Šï¼Œå°¤å…¶çº¦æŸç±»æå‡è¾¾+19.3%ã€‚  \nğŸ”¸åœ¨äººå·¥æ„å»ºçš„OODæµ‹è¯•é›†ä¸Šï¼ŒGEODPOä»è·+8.0%æå‡ï¼Œè€ŒSFTåœ¨å¤šä¸ªæ¨¡å‹ä¸Šå‡ºç°æ€§èƒ½ä¸‹é™ï¼ŒéªŒè¯å…¶å¼ºæ³›åŒ–èƒ½åŠ›ã€‚  \nğŸ”¸ä¸‹æ¸¸å‡ ä½•æ¨ç†ä»»åŠ¡ï¼ˆMathVistaå­é›†ï¼‰æå‡è¾¾+39.0%ï¼Œè¡¨æ˜å‡ ä½•æ„ŸçŸ¥å¢å¼ºåˆ‡å®ä¼ å¯¼è‡³é«˜å±‚æ¨ç†ï¼Œéä»…è¡¨å±‚æ‹Ÿåˆã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ç¿»è¯‘å™¨è´¨é‡ç›´æ¥å½±å“DPOæ•ˆæœï¼šç¿»è¯‘å™¨F1æ¯é™5%ï¼ŒGEODPOæ•´ä½“åˆ†ä¸‹é™çº¦1.5%ï¼Œå‡¸æ˜¾translatorå¼•å¯¼çš„å…³é”®ä½œç”¨ã€‚  \nğŸ”¸å®šæ€§åˆ†ææ˜¾ç¤ºï¼ŒGEODPOæ˜¾è‘—å‡å°‘å‡ ä½•å¹»è§‰ï¼ˆå¦‚è¯¯åˆ¤ç›¸åˆ‡ä¸ºç›¸äº¤ã€æ··æ·†ç‚¹çº¿éš¶å±å…³ç³»ï¼‰ï¼Œæå‡å›¾å…ƒæ¥åœ°å‡†ç¡®æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºâ€œè¯„æµ‹â€”å»ºæ¨¡â€”ä¼˜åŒ–â€ä¸‰é‡è§£è€¦ï¼šGEODSLå®ç°æ„ŸçŸ¥èƒ½åŠ›çš„å¯å®šä¹‰ã€å¯æµ‹é‡ï¼›GEOPERCEIVEæä¾›æ— é™å¯æ§çš„åˆæˆæ•°æ®æºï¼›GEODPOä»¥translatorä¸ºæ¡¥æ¢ï¼Œå°†å¼ºåŒ–å­¦ä¹ å¼•å…¥NLè¾“å‡ºèŒƒå¼ï¼Œåœ¨ä¸ç ´åé¢„è®­ç»ƒåˆ†å¸ƒå‰æä¸‹å®ç°ç»†ç²’åº¦å‡ ä½•å¯¹é½ã€‚è¯¥èŒƒå¼ä¸ºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„ç‰¹å®šèƒ½åŠ›å®šå‘å¢å¼ºæä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "From Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models",
        "authors": [
            "Hongrui Jia",
            "Chaoya Jiang",
            "Shikun Zhang",
            "Wei Ye"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22859v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22859v1",
        "summary": "As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) methods mature, LMMs have made notable progress in complex reasoning and decision making. Yet training still relies on static data and fixed recipes, making it difficult to diagnose capability blind spots or provide dynamic, targeted reinforcement. Motivated by findings that test driven error exposure and feedback based correction outperform repetitive practice, we propose Diagnostic-driven Progressive Evolution (DPE), a spiral loop where diagnosis steers data generation and reinforcement, and each iteration re-diagnoses the updated model to drive the next round of targeted improvement. DPE has two key components. First, multiple agents annotate and quality control massive unlabeled multimodal data, using tools such as web search and image editing to produce diverse, realistic samples. Second, DPE attributes failures to specific weaknesses, dynamically adjusts the data mixture, and guides agents to generate weakness focused data for targeted reinforcement. Experiments on Qwen3-VL-8B-Instruct and Qwen2.5-VL-7B-Instruct show stable, continual gains across eleven benchmarks, indicating DPE as a scalable paradigm for continual LMM training under open task distributions. Our code, models, and data are publicly available at https://github.com/hongruijia/DPE.",
        "tag": "è¿­ä»£è®­ç»ƒ",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22859v1ã€è¿­ä»£è®­ç»ƒ-åŒ—äº¬å¤§å­¦ã€‘From Blind Spots to Gains_ Diagnostic-Driven Iterative Training for Large Multimodal Models.pdf",
        "institution_status": "keep",
        "institution": "åŒ—äº¬å¤§å­¦ã€å±±ä¸œå¤§å­¦",
        "first_institution": "åŒ—äº¬å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šFrom Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models\nğŸŒæ¥æºï¼šarXiv, 2602.22859v1\n\nç¬”è®°æ ‡é¢˜ï¼šè¯Šæ–­é©±åŠ¨çš„æ¸è¿›å¼è®­ç»ƒ\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨æ•°æ®ç¨€ç¼ºå’Œé•¿å°¾ä»»åŠ¡åˆ†å¸ƒä¸‹ï¼Œå®ç°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ç¨³å®šã€æŒç»­çš„èƒ½åŠ›æå‡ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºè¯Šæ–­é©±åŠ¨çš„æ¸è¿›å¼è¿›åŒ–ï¼ˆDPEï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¯è§£é‡Šçš„å¤±è´¥å½’å› ã€åŠ¨æ€æ•°æ®æ··åˆè°ƒæ§ä¸å¤šæ™ºèƒ½ä½“ååŒç”Ÿæˆï¼Œå®ç°é’ˆå¯¹èƒ½åŠ›ç›²ç‚¹çš„é—­ç¯å¼è¿­ä»£å¼ºåŒ–è®­ç»ƒã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºä¸¤é˜¶æ®µé—­ç¯ï¼šå…ˆç”±è¯Šæ–­æ™ºèƒ½ä½“è¯†åˆ«æ¨¡å‹åœ¨12ç±»èƒ½åŠ›ç»´åº¦ä¸Šçš„å…·ä½“å¤±è´¥æ¨¡å¼ï¼ˆå¦‚OCRæ¼çº¿ã€å›¾è¡¨å•ä½å¿½ç•¥ã€æ•°å­¦æ­¥éª¤ç¼ºå¤±ï¼‰ï¼Œå†æ®æ­¤ç”Ÿæˆç»“æ„åŒ–è¯Šæ–­æŠ¥å‘Šï¼ˆå«ç±»åˆ«æƒé‡Î±ã€å¼±ç‚¹æ‘˜è¦Fã€ç”ŸæˆæŒ‡ä»¤Hï¼‰ã€‚  \nğŸ”¸è®¾è®¡å¤šæ™ºèƒ½ä½“é—®é¢˜ç”Ÿæˆç³»ç»Ÿï¼šåŒ…å«è§„åˆ’è€…ã€å›¾åƒé€‰æ‹©å™¨ã€é—®é¢˜ç”Ÿæˆå™¨å’ŒéªŒè¯è€…å››æ¨¡å—ï¼›è§„åˆ’è€…æŒ‰è¯Šæ–­æŠ¥å‘Šåˆ†é…ç±»åˆ«é…é¢ä¸éš¾åº¦çº¦æŸï¼›å›¾åƒé€‰æ‹©å™¨ä»å¤–éƒ¨æ± æ£€ç´¢å¹¶ç¼–è¾‘å›¾åƒï¼ˆæ”¯æŒè£å‰ªã€å åŠ ã€å¤šå›¾èåˆï¼‰ï¼Œçªç ´é™æ€æ•°æ®è§†è§‰å¤šæ ·æ€§ç“¶é¢ˆã€‚  \nğŸ”¸é‡‡ç”¨å¯æ§ç”Ÿæˆä¸ä¸¥æ ¼è´¨é‡é—¨æ§ï¼šç”Ÿæˆæ ·æœ¬éœ€åŒæ—¶æ»¡è¶³ç¡¬æ€§ç±»åˆ«é…é¢çº¦æŸå’Œå››é¡¹éªŒè¯ï¼ˆç±»åˆ«ä¸€è‡´æ€§ã€å¯è§£æ€§ã€ç­”æ¡ˆå¯éªŒè¯æ€§ã€æ ¼å¼åˆè§„æ€§ï¼‰ï¼Œæ‹’ç»ä¸åˆæ ¼æ ·æœ¬ä»¥æŠ‘åˆ¶åˆ†å¸ƒæ¼‚ç§»å’Œå™ªå£°ç´¯ç§¯ã€‚  \nğŸ”¸ä½¿ç”¨GRPOå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç»“åˆç»„å½’ä¸€åŒ–ä¼˜åŠ¿ä¼°è®¡ä¸æœ€å¤§ç†µè§†è§’ä¸‹çš„éš¾åº¦æ„ŸçŸ¥è¿‡æ»¤ï¼Œä¼˜å…ˆä¿ç•™ä¸­ç­‰éš¾åº¦æ ·æœ¬ï¼Œæå‡å•æ ·æœ¬å­¦ä¹ æ•ˆç‡ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸DPEåœ¨Qwen2.5-VL-7Bå’ŒQwen3-VL-8Bä¸Šå‡å®ç°11é¡¹åŸºå‡†çš„ç¨³å®šæå‡ï¼ˆå¦‚MMMU+2.0ã€CharXiv RQ+4.11ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºVisPlayç­‰è‡ªæ¼”åŒ–æ–¹æ³•ï¼Œä¸”é¿å…æ€§èƒ½æŒ¯è¡æˆ–é€€åŒ–ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šå»é™¤è¯Šæ–­æ¨¡å—åï¼Œå„ä»»åŠ¡å‡†ç¡®ç‡å¢é•¿åœæ»ç”šè‡³ä¸‹é™ï¼ˆå¦‚MathVisionä»26.51â†’25.99ï¼‰ï¼Œè¯å®è¯Šæ–­æ˜¯ç»´æŒè¿›åŒ–æ–¹å‘æ­£ç¡®æ€§çš„æ ¸å¿ƒã€‚  \nğŸ”¸å›¾åƒæ£€ç´¢ä¸ç¼–è¾‘æ¨¡å—å¯¹OCRå’Œæ•°å­¦è§†è§‰ä»»åŠ¡è‡³å…³é‡è¦ï¼Œç§»é™¤åCharXivä¸‹é™2.81åˆ†ï¼Œè¯´æ˜è§†è§‰å¤šæ ·æ€§ç›´æ¥ç¼“è§£é•¿å°¾åœºæ™¯è¦†ç›–ä¸è¶³é—®é¢˜ã€‚  \nğŸ”¸å¤šæ ·æ€§åˆ†ææ˜¾ç¤ºï¼šDPEç”Ÿæˆçš„æ•°æ®åœ¨æ–‡æœ¬ä¸å›¾åƒåµŒå…¥ç©ºé—´çš„å¹³å‡ä½™å¼¦è·ç¦»æŒç»­é«˜äºVisPlayï¼Œè¯æ˜å…¶æœ‰æ•ˆæŠ‘åˆ¶äº†æ¨¡æ¿å¤ç”¨ä¸åˆ†å¸ƒåç¼©ã€‚  \nğŸ”¸äººå·¥è¯„ä¼°è¡¨æ˜DPEç”Ÿæˆé—®é¢˜çš„è´¨é‡å¾—åˆ†ï¼ˆQSâ‰ˆ4.8ï¼‰è¿œè¶…VisPlayï¼ˆQSâ‰ˆ3.3ï¼‰ï¼Œå°¤å…¶åœ¨å¯è§£æ€§ï¼ˆ4.9 vs 2.98ï¼‰ä¸ç­”æ¡ˆæ­£ç¡®æ€§ï¼ˆ4.7 vs 3.08ï¼‰ä¸Šä¼˜åŠ¿æ˜¾è‘—ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥è®ºæ–‡çš„åˆ›æ–°ç‚¹åœ¨äºå°†æ•™è‚²å¿ƒç†å­¦ä¸­çš„â€œè¯Šæ–­â€”çŸ«æ­£â€æœºåˆ¶ç³»ç»ŸåŒ–å¼•å…¥LMMè®­ç»ƒèŒƒå¼ï¼Œé¦–æ¬¡å®ç°äº†èƒ½åŠ›ç¼ºé™·çš„æ˜¾å¼å½’å› ã€æ•°æ®åˆ†å¸ƒçš„åŠ¨æ€è°ƒæ§ä¸è§†è§‰å†…å®¹çš„ä¸»åŠ¨æ„é€ ä¸‰è€…çš„æœ‰æœºç»Ÿä¸€ï¼›å…¶è¯Šæ–­æŠ¥å‘Šç»“æ„åŒ–ã€ç”Ÿæˆè¿‡ç¨‹å·¥å…·åŒ–ã€éªŒè¯æ ‡å‡†å¯é‡åŒ–çš„è®¾è®¡ï¼Œä¸ºè§£å†³è‡ªæ¼”åŒ–è®­ç»ƒä¸­çš„é»‘ç®±æ€§ã€ä¸ç¨³å®šæ€§ä¸é•¿å°¾è¦†ç›–éš¾æä¾›äº†å¯å¤ç°ã€å¯æ‰©å±•çš„æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "IBCircuit: Towards Holistic Circuit Discovery with Information Bottleneck",
        "authors": [
            "Tian Bian",
            "Yifan Niu",
            "Chaohao Yuan",
            "Chengzhi Piao",
            "Bingzhe Wu",
            "Long-Kai Huang",
            "Yu Rong",
            "Tingyang Xu",
            "Hong Cheng",
            "Jia Li"
        ],
        "categories": [
            "cs.LG"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22581v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22581v1",
        "summary": "Circuit discovery has recently attracted attention as a potential research direction to explain the non-trivial behaviors of language models. It aims to find the computational subgraphs, also known as circuits, within the model that are responsible for solving specific tasks. However, most existing studies overlook the holistic nature of these circuits and require designing specific corrupted activations for different tasks, which is inaccurate and inefficient. In this work, we propose an end-to-end approach based on the principle of Information Bottleneck, called IBCircuit, to identify informative circuits holistically. IBCircuit is an optimization framework for holistic circuit discovery and can be applied to any given task without tediously corrupted activation design. In both the Indirect Object Identification (IOI) and Greater-Than tasks, IBCircuit identifies more faithful and minimal circuits in terms of critical node components and edge components compared to recent related work.",
        "tag": "å¯è§£é‡Šæ€§",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22581v1ã€å¯è§£é‡Šæ€§-é¦™æ¸¯ä¸­æ–‡å¤§å­¦ã€‘IBCircuit_ Towards Holistic Circuit Discovery with Information Bottleneck.pdf",
        "institution_status": "keep",
        "institution": "é¦™æ¸¯ä¸­æ–‡å¤§å­¦ã€é˜¿é‡Œå·´å·´",
        "first_institution": "é¦™æ¸¯ä¸­æ–‡å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šIBCircuit: Towards Holistic Circuit Discovery with Information Bottleneck\nğŸŒæ¥æºï¼šarXiv, 2602.22581v1\n\nç¬”è®°æ ‡é¢˜ï¼šåŸºäºä¿¡æ¯ç“¶é¢ˆçš„æ•´ä½“ç”µè·¯å‘ç°\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸ä¾èµ–ä»»åŠ¡ç‰¹å®šå¹²æ‰°æ¿€æ´»è®¾è®¡çš„å‰æä¸‹ï¼Œæ•´ä½“æ€§åœ°è¯†åˆ«è¯­è¨€æ¨¡å‹ä¸­å¯¹ç‰¹å®šä»»åŠ¡æœ€ç›¸å…³ä¸”æœ€ç²¾ç®€çš„è®¡ç®—å­å›¾ï¼ˆå³ç”µè·¯ï¼‰ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºIBCircuitæ¡†æ¶ï¼Œé¦–æ¬¡å°†ä¿¡æ¯ç“¶é¢ˆåŸç†ç³»ç»Ÿåº”ç”¨äºç”µè·¯å‘ç°ï¼Œå®ç°ç«¯åˆ°ç«¯ã€ä»»åŠ¡æ— å…³ã€æ•´ä½“ä¼˜åŒ–çš„ç”µè·¯è¯†åˆ«ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å°†ç”µè·¯å‘ç°å»ºæ¨¡ä¸ºä¿¡æ¯ç“¶é¢ˆä¼˜åŒ–é—®é¢˜ï¼šæœ€å¤§åŒ–ç”µè·¯Cå¯¹ä»»åŠ¡è¾“å‡ºYçš„äº’ä¿¡æ¯I(Y;C)ï¼ŒåŒæ—¶æœ€å°åŒ–Cä»å…¨æ¨¡å‹Gä¸­è·å–çš„å†—ä½™ä¿¡æ¯I(G;C)ã€‚  \nğŸ”¸å¼•å…¥å¯å­¦ä¹ çš„ä¿¡æ¯ç“¶é¢ˆæƒé‡Î»ï¼Œé€šè¿‡Sigmoidå‚æ•°åŒ–ï¼Œåœ¨èŠ‚ç‚¹ï¼ˆæ³¨æ„åŠ›å¤´ï¼‰å’Œè¾¹ï¼ˆæ®‹å·®è¿æ¥ï¼‰ä¸¤ä¸ªç²’åº¦ä¸Šæ§åˆ¶é«˜æ–¯å™ªå£°æ³¨å…¥å¼ºåº¦ï¼Œå®ç°è¿ç»­å¯å¾®çš„ç”µè·¯å‚æ•°åŒ–ã€‚  \nğŸ”¸é‡‡ç”¨å˜åˆ†è¿‘ä¼¼ä¼°è®¡äº’ä¿¡æ¯ï¼šç”¨KLæ•£åº¦ä¸‹ç•Œè¿‘ä¼¼I(Y;C)ï¼Œç”¨KLæ•£åº¦ä¸Šç•Œè¿‘ä¼¼I(G;C)ï¼Œæ„å»ºå¯è®­ç»ƒçš„ç›®æ ‡å‡½æ•°ã€‚  \nğŸ”¸é€šè¿‡é˜ˆå€¼åŒ–å­¦ä¹ åˆ°çš„Î»æƒé‡ï¼ˆè‡ªé€‚åº”è®¾å®šç¨€ç–çº¦æŸkï¼‰ï¼Œç¦»æ•£æå–å…³é”®èŠ‚ç‚¹ä¸è¾¹ï¼Œå½¢æˆæœ€ç»ˆç”µè·¯ï¼Œé¿å…æ‰‹å·¥è®¾è®¡å¹²æ‰°æ¿€æ´»ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨IOIå’ŒGreater-Thanä»»åŠ¡ä¸Šï¼ŒIBCircuitè¯†åˆ«å‡ºçš„ç”µè·¯åœ¨ç›¸åŒèŠ‚ç‚¹/è¾¹æ•°é‡ä¸‹ï¼ŒLogit Differenceå’ŒGreater Probabilityæ›´é«˜ï¼ŒKLæ•£åº¦æ›´ä½ï¼Œè¡¨æ˜å…¶æ›´å¿ å®ä¸”æ›´ç²¾ç®€ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šKLæŸå¤±ä¿éšœåŠŸèƒ½ä¸€è‡´æ€§ï¼ŒMIæŸå¤±æŠ‘åˆ¶å†—ä½™ä¿¡æ¯ï¼›äºŒè€…ç¼ºä¸€ä¸å¯ï¼Œè”åˆä¼˜åŒ–æ˜¾è‘—ä¼˜äºå•ä¸€æŸå¤±è®­ç»ƒã€‚  \nğŸ”¸ç›¸æ¯”ACDCã€APç­‰åŸºçº¿ï¼ŒIBCircuitåœ¨è¾¹çº§ç”µè·¯å‘ç°ä¸Šå…¨é¢å ä¼˜ï¼Œå°¤å…¶åœ¨é«˜åº¦ç¨€ç–æ¡ä»¶ä¸‹ä»ä¿æŒé«˜faithfulnessï¼ŒéªŒè¯å…¶é²æ£’æ€§ã€‚  \nğŸ”¸IBCircuitå¯æ‰©å±•è‡³GPT-2 XLï¼ˆ1.5Bï¼‰ï¼Œæ€§èƒ½åª²ç¾ç°æœ‰æ–¹æ³•ï¼Œè¯æ˜å…¶å¯¹å¤§è§„æ¨¡æ¨¡å‹çš„æœ‰æ•ˆæ€§ä¸å¯æ‰©å±•æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†ä¿¡æ¯ç“¶é¢ˆè¿™ä¸€ç»å…¸è¡¨å¾å­¦ä¹ åŸåˆ™è¿ç§»åˆ°æœºåˆ¶å¯è§£é‡Šæ€§é¢†åŸŸï¼Œçªç ´äº†ä¼ ç»Ÿå¹²é¢„å¼æ–¹æ³•ï¼ˆå¦‚patchingï¼‰çš„å±€éƒ¨æ€§ã€ä»»åŠ¡ä¾èµ–æ€§å’Œè®¡ç®—ä½æ•ˆæ€§å±€é™ï¼›å…¶ç«¯åˆ°ç«¯å¯å¾®ä¼˜åŒ–èŒƒå¼ä¸ºç”µè·¯å‘ç°æä¾›äº†ç»Ÿä¸€ã€é€šç”¨ã€å¯æ‰©å±•çš„æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "MSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding",
        "authors": [
            "Wenhui Tan",
            "Xiaoyi Yu",
            "Jiaze Li",
            "Yijing Chen",
            "Jianzhong Ju",
            "Zhenbo Luo",
            "Ruihua Song",
            "Jian Luan"
        ],
        "categories": [
            "cs.CV"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22932v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22932v1",
        "summary": "Efficiently understanding long-form videos remains a fundamental challenge for multimodal large language models (MLLMs). In this paper, we present MLLM-Sampler Joint Evolution (MSJoE), a novel framework that jointly evolves the MLLM and a lightweight key-frame sampler for efficient long-form video understanding. MSJoE builds upon a key assumption that only a small subset of key-frames is truly informative for answering each question to a video. Specifically, MSJoE first reasons out several queries, which describe diverse visual perspectives relevant to the question. Then, these queries interact with a frozen CLIP model to produce a query-frame similarity matrix. Finally, a lightweight sampler predicts key-frame sampling weights from this matrix, selecting a compact set of informative frames, which are then fed into the MLLM for answer generation. Both the MLLM and sampler are jointly optimized through reinforcement learning, enabling co-adaptation of query-reasoning, frame-sampling, and key-frame understanding. A new long-video QA dataset containing 2.8K videos with 7K question-answer pairs is collected to support the training process. Extensive experiments on VideoMME, LongVideoBench, LVBench, and MLVU show that MSJoE achieves 8.0\\% accuracy gain upon the base MLLM, and 1.1\\% higher accuracy than strongest baseline method.",
        "tag": "è§†é¢‘ç†è§£",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22932v1ã€è§†é¢‘ç†è§£-ä¸­å›½äººæ°‘å¤§å­¦ã€‘MSJoE_ Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding.pdf",
        "institution_status": "keep",
        "institution": "ä¸­å›½äººæ°‘å¤§å­¦ã€åŒæµå¤§å­¦ã€MiLM Plus, Xiaomi Inc.",
        "first_institution": "ä¸­å›½äººæ°‘å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šMSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding\nğŸŒæ¥æºï¼šarXiv, 2602.22932v1\n\nç¬”è®°æ ‡é¢˜ï¼šè”åˆè¿›åŒ–MLLMä¸é‡‡æ ·å™¨\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨é•¿è§†é¢‘ç†è§£ä¸­å®ç°é«˜æ•ˆä¸”å‡†ç¡®çš„å…³é”®å¸§é€‰æ‹©ä¸å¤šæ¨¡æ€å¤§æ¨¡å‹ååŒä¼˜åŒ–ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºMSJoEæ¡†æ¶ï¼Œé¦–æ¬¡é€šè¿‡å¼ºåŒ–å­¦ä¹ è”åˆä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å’Œè½»é‡çº§å…³é”®å¸§é‡‡æ ·å™¨ï¼Œå®ç°æ¨ç†å¼•å¯¼çš„å¸§é€‰æ‹©ä¸æ„ŸçŸ¥-è¯­è¨€èƒ½åŠ›å…±é€‚åº”ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸åŸºäºâ€œä»…å°‘é‡å…³é”®å¸§è¶³ä»¥å›ç­”é—®é¢˜â€çš„æ ¸å¿ƒå‡è®¾ï¼Œå…ˆç”¨ç¨€ç–é¢„è§ˆå¸§å¼•å¯¼MLLMç”Ÿæˆå¤šä¸ªè§†è§‰ grounded çš„æ¨ç†æŸ¥è¯¢ï¼ˆå¦‚â€œä¸€å¼ ç‰™é½¿ç‰¹å†™å›¾â€ï¼‰ï¼Œè€Œéç›´æ¥ä½¿ç”¨åŸå§‹é—®é¢˜ã€‚  \nğŸ”¸å°†è¿™äº›æŸ¥è¯¢ä¸å¯†é›†é‡‡æ ·çš„è§†é¢‘å¸§è¾“å…¥å†»ç»“çš„CLIPæ¨¡å‹ï¼Œæ„å»ºæŸ¥è¯¢â€“å¸§ç›¸ä¼¼åº¦çŸ©é˜µï¼Œä¸ºé‡‡æ ·æä¾›è¯­ä¹‰ä¸°å¯Œã€å¤šè§†è§’çš„åŒ¹é…ä¾æ®ã€‚  \nğŸ”¸è®¾è®¡ä¸€ä¸ªä»…å«çº¦200ä¸‡å‚æ•°çš„1D U-Netè½»é‡é‡‡æ ·å™¨ï¼Œä»ç›¸ä¼¼åº¦çŸ©é˜µä¸­å­¦ä¹ ç”Ÿæˆå¸§çº§é‡‡æ ·æƒé‡ï¼Œå…¼é¡¾é«˜å¾—åˆ†åŒºåŸŸä¸æ—¶é—´å¤šæ ·æ€§ï¼Œé¿å…top-kå¯¼è‡´çš„å†—ä½™ã€‚  \nğŸ”¸é‡‡ç”¨GRPOå¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹MLLMå’Œé‡‡æ ·å™¨è¿›è¡Œç«¯åˆ°ç«¯è”åˆè®­ç»ƒï¼šMLLMå­¦ä¹ ç”Ÿæˆæ›´æœ‰æ•ˆæŸ¥è¯¢å¹¶é€‚é…ç¨€ç–å¸§è¾“å…¥ï¼Œé‡‡æ ·å™¨å­¦ä¹ é€‰æ‹©èƒ½æœ€å¤§åŒ–ä¸‹æ¸¸ç­”æ¡ˆå‡†ç¡®ç‡çš„å¸§å­é›†ã€‚  \nğŸ”¸ä¸ºæ”¯æ’‘è®­ç»ƒï¼Œæ„å»ºæ–°æ•°æ®é›†LongVideoQAï¼ˆ2.8kå°æ—¶çº§è§†é¢‘ï¼Œ7.1k QAå¯¹ï¼‰ï¼ŒåŒ…å«è‡ªåŠ¨æ ‡æ³¨ã€å¤šè·³æ¨ç†ã€éš¾åº¦åˆ†çº§ä¸ä¸¥æ ¼è¿‡æ»¤æœºåˆ¶ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šä»…ç”¨é—®é¢˜æœ¬èº«ä½œä¸ºCLIPæŸ¥è¯¢æ•ˆæœæœ‰é™ï¼ˆQ1ä¸å……åˆ†ï¼‰ï¼Œéœ€MLLMç”Ÿæˆå¤šè§†è§’æŸ¥è¯¢ï¼›æœ´ç´ top-ké‡‡æ ·æ€§èƒ½åä½äºå‡åŒ€é‡‡æ ·ï¼ŒéªŒè¯äº†å¯å­¦ä¹ é‡‡æ ·å™¨çš„å¿…è¦æ€§ï¼ˆQ2ï¼‰ã€‚  \nğŸ”¸å†»ç»“MLLMæ—¶ï¼Œå³ä½¿ä½¿ç”¨ä¼˜è´¨æŸ¥è¯¢å’Œé‡‡æ ·å™¨ï¼Œæ€§èƒ½ä»æ˜¾è‘—ä¸‹é™ï¼›è€Œè”åˆè¿›åŒ–åï¼ŒMLLMèƒ½æ›´å¥½åˆ©ç”¨ç¨€ç–å¸§å¹¶ç”Ÿæˆæ›´ç²¾å‡†æŸ¥è¯¢ï¼Œè¯å®åä½œå¿…é¡»ä¾èµ–è”åˆä¼˜åŒ–ï¼ˆQ3æˆç«‹ï¼‰ã€‚  \nğŸ”¸åœ¨VideoMMEç­‰å››å¤§åŸºå‡†ä¸Šï¼ŒMSJoEä»¥32å¸§è¾“å…¥å³è¶…è¶ŠåŸºçº¿MLLMè¾¾8.0%å‡†ç¡®ç‡ï¼Œä¸”æ¯”æœ€å¼ºåŸºçº¿TSPOé«˜1.1%ï¼ŒéªŒè¯å…¶ç²¾åº¦ä¸æ•ˆç‡åŒé‡ä¼˜åŠ¿ã€‚  \nğŸ”¸å¼•å…¥ä¿¡æ¯æ€§å¥–åŠ±ï¼ˆé¼“åŠ±å°–å³°å¼ç›¸ä¼¼åˆ†å¸ƒï¼‰å’Œéš¾åº¦æ„ŸçŸ¥å¥–åŠ±ï¼ˆå¯¹éš¾æ ·æœ¬èµ‹äºˆæ›´é«˜æ¢¯åº¦ï¼‰ï¼Œæ˜¾è‘—æå‡è®­ç»ƒç¨³å®šæ€§ä¸æœ€ç»ˆæ€§èƒ½ï¼Œç§»é™¤ä»»ä¸€å¥–åŠ±å‡å¯¼è‡´æ˜æ˜¾ä¸‹é™ã€‚  \nğŸ”¸æ¡ˆä¾‹åˆ†ææ˜¾ç¤ºï¼šMSJoEé€‰å–çš„å¸§åºåˆ—å‘ˆç°æ¸…æ™°å™äº‹é€»è¾‘ï¼ˆå¦‚é›¶é£Ÿâ†’ç‰™ç—…â†’çœ‹è¯Šï¼‰ï¼Œæ”¯æ’‘å› æœæ¨ç†ï¼›è€Œå‡åŒ€æˆ–top-ké‡‡æ ·æ˜“é™·å…¥å±€éƒ¨è§†è§‰çº¿ç´¢ï¼Œå¯¼è‡´é”™è¯¯å½’å› ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ¨äºæ‰“ç ´â€œé‡‡æ ·å™¨è¾…åŠ©MLLMâ€çš„å•å‘èŒƒå¼ï¼Œå°†äºŒè€…å»ºæ¨¡ä¸ºå¯ååŒè¿›åŒ–çš„ç­–ç•¥ç½‘ç»œï¼šMLLMä¸ä»…æ˜¯ç†è§£è€…ï¼Œæ›´æ˜¯ä¸»åŠ¨çš„è§†è§‰é—®é¢˜åˆ†è§£è€…ï¼›é‡‡æ ·å™¨ä¹Ÿä¸å†æ˜¯é»‘ç®±è¿‡æ»¤å™¨ï¼Œè€Œæ˜¯ç†è§£MLLMæ¨ç†æ„å›¾çš„è¯­ä¹‰ç¿»è¯‘å™¨ã€‚å…¶æ ¸å¿ƒæ´è§â€”â€”â€œå…³é”®å¸§é€‰æ‹©æœ¬è´¨æ˜¯è·¨æ¨¡æ€æ¨ç†çš„å…·èº«åŒ–è¡¨è¾¾â€â€”â€”ä¸ºé•¿è§†é¢‘ç†è§£æä¾›äº†æ–°æ–¹æ³•è®ºã€‚\n    "
    },
    {
        "title": "MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks",
        "authors": [
            "Shiqian Su",
            "Sen Xing",
            "Xuan Dong",
            "Muyan Zhong",
            "Bin Wang",
            "Xizhou Zhu",
            "Yuntao Chen",
            "Wenhai Wang",
            "Yue Deng",
            "Pengxiang Zhu",
            "Ziyuan Liu",
            "Tiantong Li",
            "Jiaheng Yu",
            "Zhe Chen",
            "Lidong Bing",
            "Jifeng Dai"
        ],
        "categories": [
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22808v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22808v1",
        "summary": "Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commercial APIs. In this work, we propose a high-performance and robust open-source agent framework, termed MiroFlow, which incorporates an agent graph for flexible orchestration, an optional deep reasoning mode to enhance performance, and a robust workflow execution to ensure stable and reproducible performance. Extensive experiments demonstrate that MiroFlow consistently achieves state-of-the-art performance across multiple agent benchmarks, including GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and notably FutureX. We hope it could serve as an easily accessible, reproducible, and comparable baseline for the deep research community.",
        "tag": "Agent æ¡†æ¶",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22808v1ã€Agent æ¡†æ¶-æ¸…åå¤§å­¦ã€‘MiroFlow_ Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks.pdf",
        "institution_status": "keep",
        "institution": "æ¸…åå¤§å­¦ã€MiroMind AIã€æ–°åŠ å¡å›½ç«‹å¤§å­¦ã€å—äº¬å¤§å­¦",
        "first_institution": "æ¸…åå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šMiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks\nğŸŒæ¥æºï¼šarXiv, 2602.22808v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºé«˜é²æ£’å¼€æºæ™ºèƒ½ä½“æ¡†æ¶\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è®¾è®¡ä¸€ä¸ªé«˜æ€§èƒ½ã€å¼ºé²æ£’ã€å…¨å¼€æºçš„é€šç”¨æ™ºèƒ½ä½“æ¡†æ¶ï¼Œä»¥æ”¯æ’‘å¤æ‚æ·±åº¦ç ”ç©¶ä»»åŠ¡ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºMiroFlowâ€”â€”é¦–ä¸ªèåˆä»£ç†å›¾ç¼–æ’ã€å¯é€‰æ·±åº¦æ¨ç†æ¨¡å¼ä¸é²æ£’å·¥ä½œæµæœºåˆ¶çš„é«˜æ€§èƒ½å¼€æºæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåœ¨å¤šä¸ªæƒå¨åŸºå‡†ä¸Šå®ç°å¯å¤ç°çš„SOTAæ€§èƒ½ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸é‡‡ç”¨ä¸‰å±‚åˆ†å±‚æ¶æ„ï¼ˆæ§åˆ¶å±‚/ä»£ç†å±‚/åŸºç¡€å±‚ï¼‰ï¼Œè§£è€¦è°ƒåº¦é€»è¾‘ã€è¡Œä¸ºé€»è¾‘ä¸åº•å±‚èƒ½åŠ›ï¼Œæå‡æ¨¡å—åŒ–ä¸å¯æ‰©å±•æ€§ã€‚  \nğŸ”¸å¼•å…¥æœ‰å‘ä»£ç†å›¾ï¼ˆAgent Graphï¼‰ä½œä¸ºæ ¸å¿ƒç¼–æ’èŒƒå¼ï¼Œæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰èŠ‚ç‚¹ä¾èµ–å…³ç³»ï¼Œå®ç°ä»»åŠ¡é©±åŠ¨çš„çµæ´»æ‹“æ‰‘æ„å»ºã€‚  \nğŸ”¸è®¾è®¡å¯é€‰çš„é‡æ¨ç†æ¨¡å¼ï¼ˆHeavy-Reasoning Modeï¼‰ï¼Œé€šè¿‡é›†æˆç­–ç•¥ï¼ˆå¤šæ¨¡å‹æŠ•ç¥¨ï¼‰ä¸éªŒè¯ç­–ç•¥ï¼ˆç”Ÿæˆ-æ ¡éªŒå¾ªç¯ï¼‰æå‡å…³é”®å­ä»»åŠ¡ç²¾åº¦ã€‚  \nğŸ”¸æ„å»ºé²æ£’å·¥ä½œæµæœºåˆ¶ï¼ŒåŒ…å«æ¶ˆæ¯æ ‡å‡†åŒ–ï¼ˆç»“æ„åŒ–è¾“å…¥/è¾“å‡ºï¼‰ã€å¸¦è¶…æ—¶ä¸å›é€€çš„é‡è¯•æœºåˆ¶ã€ä»¥åŠè·¨å±‚æ•…éšœéš”ç¦»æœºåˆ¶ï¼Œæ˜¾è‘—é™ä½éšæœºæ€§ä¸é”™è¯¯ä¼ æ’­é£é™©ã€‚  \nğŸ”¸å…¨é¢æ”¯æŒå¼€æºå·¥å…·é“¾ï¼ˆå¦‚Qwen2.5-VLã€Whisper-v3ç­‰ï¼‰ï¼Œåœ¨ä¸ä¾èµ–å•†ä¸šAPIå‰æä¸‹è¾¾æˆæ¥è¿‘é—­æºç³»ç»Ÿçš„æ€§èƒ½æ°´å¹³ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸MiroFlowåœ¨GAIAã€BrowseComp-EN/ZHã€HLEã€xBench-DeepSearchåŠFutureXäº”å¤§åŸºå‡†ä¸Šå‡è¾¾SOTAï¼Œä¸”æ‰€æœ‰ç»“æœåŸºäºç»Ÿä¸€é…ç½®ã€æ— éœ€ä»»åŠ¡è°ƒä¼˜ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šæ¶ˆæ¯æ ‡å‡†åŒ–ä½¿GAIA-Valæ ‡å‡†å·®ä»2.43%é™è‡³1.21%ï¼Œé‡è¯•æœºåˆ¶å°†å‡†ç¡®ç‡æå‡2.9ä¸ªç™¾åˆ†ç‚¹ï¼ŒäºŒè€…ååŒä¿éšœå¤ç°æ€§ã€‚  \nğŸ”¸é‡æ¨ç†æ¨¡å¼åœ¨GAIA-Valä¸Šå°†GPT-5æ€§èƒ½ä»71.9%æå‡è‡³75.0%ï¼ˆ4æ¨¡å‹+å¤šæ ·åŒ–æç¤ºï¼‰ï¼ŒéªŒè¯å…¶å¯¹å¤æ‚ä»»åŠ¡çš„æœ‰æ•ˆå¢ç›Šã€‚  \nğŸ”¸å¤šæ™ºèƒ½ä½“æ¶æ„åœ¨BrowseComp/HLEä¸Šä¼˜äºå•æ™ºèƒ½ä½“ï¼Œä½†åœ¨GAIAä¸Šåé€Šäºå•æ™ºèƒ½ä½“ï¼Œæ­ç¤ºä»»åŠ¡åºåˆ—æ€§è¶Šå¼ºï¼Œä¸Šä¸‹æ–‡è¿è´¯æ€§è¶Šå…³é”®ã€‚  \nğŸ”¸å¼€æºå·¥å…·é›†é…ç½®ä¸‹ï¼ŒMiroFlowåœ¨GAIA-Valä¸Šä»…æ¯”é»˜è®¤å•†ç”¨å·¥å…·ä½1.6ä¸ªç™¾åˆ†ç‚¹ï¼Œè¯å®å…¶å¯¹å·¥å…·ç”Ÿæ€çš„å¼ºé€‚åº”æ€§ä¸æˆæœ¬å¯æ§æ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºç³»ç»Ÿæ€§ç ´è§£å½“å‰å¼€æºæ™ºèƒ½ä½“çš„ä¸‰å¤§ç“¶é¢ˆï¼šç”¨ä»£ç†å›¾æ›¿ä»£ç¡¬ç¼–ç æµæ°´çº¿è§£å†³çµæ´»æ€§ä¸è¶³ï¼›ä»¥ç»“æ„åŒ–I/O+æ•…éšœéš”ç¦»+é‡è¯•æœºåˆ¶æ”»å…‹ç¨³å®šæ€§é¡½ç–¾ï¼›é€šè¿‡è½»é‡çº§å¼€æºå·¥å…·é›†æˆä¸ç»Ÿä¸€æ¡†æ¶è®¾è®¡ï¼Œå®è´¨æ€§é™ä½ç ”ç©¶é—¨æ§›ä¸éƒ¨ç½²æˆæœ¬ã€‚å…¶â€œå¯é…ç½®ã€å¯å¤ç°ã€å¯æ¯”è¾ƒâ€çš„è®¾è®¡ç†å¿µï¼Œä¸ºç¤¾åŒºæä¾›äº†çœŸæ­£å¯ç”¨çš„æ·±åº¦ç ”ç©¶åŸºåº§ã€‚\n    "
    },
    {
        "title": "MoDora: Tree-Based Semi-Structured Document Analysis System",
        "authors": [
            "Bangrui Xu",
            "Qihang Yao",
            "Zirui Tang",
            "Xuanhe Zhou",
            "Yeye He",
            "Shihan Yu",
            "Qianqian Xu",
            "Bin Wang",
            "Guoliang Li",
            "Conghui He",
            "Fan Wu"
        ],
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CL",
            "cs.DB",
            "cs.LG"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.23061v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23061v1",
        "summary": "Semi-structured documents integrate diverse interleaved data elements (e.g., tables, charts, hierarchical paragraphs) arranged in various and often irregular layouts. These documents are widely observed across domains and account for a large portion of real-world data. However, existing methods struggle to support natural language question answering over these documents due to three main technical challenges: (1) The elements extracted by techniques like OCR are often fragmented and stripped of their original semantic context, making them inadequate for analysis. (2) Existing approaches lack effective representations to capture hierarchical structures within documents (e.g., associating tables with nested chapter titles) and to preserve layout-specific distinctions (e.g., differentiating sidebars from main content). (3) Answering questions often requires retrieving and aligning relevant information scattered across multiple regions or pages, such as linking a descriptive paragraph to table cells located elsewhere in the document.\n  To address these issues, we propose MoDora, an LLM-powered system for semi-structured document analysis. First, we adopt a local-alignment aggregation strategy to convert OCR-parsed elements into layout-aware components, and conduct type-specific information extraction for components with hierarchical titles or non-text elements. Second, we design the Component-Correlation Tree (CCTree) to hierarchically organize components, explicitly modeling inter-component relations and layout distinctions through a bottom-up cascade summarization process. Finally, we propose a question-type-aware retrieval strategy that supports (1) layout-based grid partitioning for location-based retrieval and (2) LLM-guided pruning for semantic-based retrieval. Experiments show MoDora outperforms baselines by 5.97%-61.07% in accuracy. The code is at https://github.com/weAIDB/MoDora.",
        "tag": "RAG æ£€ç´¢ä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.23061v1ã€RAG æ£€ç´¢ä¼˜åŒ–-ä¸Šæµ·äº¤é€šå¤§å­¦ã€‘MoDora_ Tree-Based Semi-Structured Document Analysis System.pdf",
        "institution_status": "keep",
        "institution": "ä¸Šæµ·äº¤é€šå¤§å­¦ã€å¾®è½¯",
        "first_institution": "ä¸Šæµ·äº¤é€šå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šMoDora: Tree-Based Semi-Structured Document Analysis System\nğŸŒæ¥æºï¼šarXiv, 2602.23061v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ ‘çŠ¶ç»“æ„åŒ–æ–‡æ¡£åˆ†ææ¡†æ¶\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•æœ‰æ•ˆæ”¯æŒè‡ªç„¶è¯­è¨€é—®ç­”åœ¨å¸ƒå±€å¤æ‚ã€å…ƒç´ æ··æ‚çš„åŠç»“æ„åŒ–æ–‡æ¡£ä¸Šï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºMoDoraç³»ç»Ÿï¼Œé€šè¿‡ç»„ä»¶èšåˆã€ç»„ä»¶å…³è”æ ‘ï¼ˆCCTreeï¼‰å»ºæ¨¡ä¸é—®é¢˜ç±»å‹æ„ŸçŸ¥æ£€ç´¢ï¼Œæ˜¾è‘—æå‡è·¨å…ƒç´ ã€è·¨é¡µã€è·¨æ¨¡æ€é—®ç­”å‡†ç¡®ç‡ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸é‡‡ç”¨å±€éƒ¨å¯¹é½èšåˆç­–ç•¥ï¼Œå°†OCRç¢ç‰‡åŒ–å…ƒç´ æŒ‰è¯­ä¹‰ä¸ç©ºé—´é‚»è¿‘æ€§èšåˆæˆè‡ªåŒ…å«ç»„ä»¶ï¼ˆå¦‚æ ‡é¢˜+æ®µè½ã€å›¾è¡¨+æ ‡é¢˜ï¼‰ã€‚  \nğŸ”¸è®¾è®¡ç»„ä»¶å…³è”æ ‘ï¼ˆCCTreeï¼‰ï¼Œä»¥å±‚æ¬¡åŒ–æ–¹å¼ç»„ç»‡ç»„ä»¶ï¼Œæ˜¾å¼å»ºæ¨¡æ–‡æœ¬-æ–‡æœ¬ï¼ˆæ ‡é¢˜å±‚çº§ï¼‰ã€æ–‡æœ¬-éæ–‡æœ¬ï¼ˆæ®µè½ä¸å¯¹åº”è¡¨æ ¼ï¼‰ã€è¡¥å……å…ƒç´ ï¼ˆé¡µçœ‰/é¡µè„šï¼‰ä¸‰ç±»å…³ç³»ã€‚  \nğŸ”¸å¼•å…¥è‡ªåº•å‘ä¸Šçº§è”æ‘˜è¦æœºåˆ¶ï¼Œåœ¨æ ‘èŠ‚ç‚¹ä¸­æ³¨å…¥å­æ ‘è¯­ä¹‰æ‘˜è¦ï¼Œå¹¶ä¾æ®æ·±åº¦åŠ¨æ€æ§åˆ¶æ‘˜è¦å…³é”®è¯æ•°é‡ä»¥ç¼“è§£ä¿¡æ¯è¡°å‡ã€‚  \nğŸ”¸æ„å»ºé—®é¢˜ç±»å‹æ„ŸçŸ¥çš„åŒè·¯å¾„æ£€ç´¢ï¼šä½ç½®å‹é—®é¢˜é‡‡ç”¨ç½‘æ ¼åˆ’åˆ†+åæ ‡åŒ¹é…ï¼›è¯­ä¹‰å‹é—®é¢˜èåˆLLMå¼•å¯¼çš„èŠ‚ç‚¹ç­›é€‰ã€åµŒå…¥å›é€€æ£€ç´¢ä¸å¤šæ¨¡æ€åå‘éªŒè¯ã€‚  \nğŸ”¸åœ¨è¯æ®èšåˆé˜¶æ®µåŒæ­¥è¾“å…¥æ–‡æœ¬å†…å®¹ã€è£å‰ªå›¾åƒåŒºåŸŸï¼ˆå®šä½è¯æ®ï¼‰åŠæ ‘è·¯å¾„ç´¢å¼•ï¼ˆç»“æ„ä¸Šä¸‹æ–‡ï¼‰ï¼Œæ”¯æ’‘ç«¯åˆ°ç«¯ç­”æ¡ˆç”Ÿæˆã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸CCTreeç»“æ„å»ºæ¨¡ä½¿å±‚æ¬¡ç±»é—®é¢˜å‡†ç¡®ç‡æå‡è‡³76.73%ï¼Œå¤§å¹…é¢†å…ˆZenDBï¼ˆ52.83%ï¼‰å’ŒDocAgentï¼ˆ55.97%ï¼‰ï¼ŒéªŒè¯å…¶å¯¹æ–‡æ¡£åµŒå¥—ç»“æ„çš„åˆ»ç”»èƒ½åŠ›ã€‚  \nğŸ”¸ä½ç½®å‹é—®é¢˜ä¸ŠMoDoraè¾¾68.21%å‡†ç¡®ç‡ï¼Œä¼˜äºGPT-5ï¼ˆ47.02%ï¼‰ï¼Œè¯´æ˜ç½‘æ ¼æ˜ å°„ä¸å¸ƒå±€æ„ŸçŸ¥æ£€ç´¢å¯ç²¾å‡†å®šä½é¡µé¢åŒºåŸŸã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šç§»é™¤æ ‘ç»“æ„å¯¼è‡´å‡†ç¡®ç‡ä¸‹é™è¶…15%ï¼Œè¯å®å±‚æ¬¡åŒ–è¡¨ç¤ºå¯¹å¤šè·³æ¨ç†ä¸å¯æˆ–ç¼ºï¼›ç¼ºå¤±å®šä½è¯æ®ä½¿æ€§èƒ½ä¸‹é™5.07%ï¼Œå‡¸æ˜¾å›¾åƒåŒºåŸŸè¾“å…¥å¯¹éæ–‡æœ¬å…ƒç´ ç†è§£çš„å…³é”®ä½œç”¨ã€‚  \nğŸ”¸ç›¸æ¯”åŸºçº¿ï¼ŒMoDoraåœ¨æ··åˆå‹é—®é¢˜ï¼ˆå«è¡¨æ ¼/å›¾è¡¨ï¼‰ä¸Šä»ä¿æŒ68.00%æœ€é«˜å‡†ç¡®ç‡ï¼Œè€ŒTextRAGä¸ZenDBå› çº¯æ–‡æœ¬è½¬æ¢ä¸¢å¤±ç»“æ„ä¿¡æ¯ï¼Œå‡ ä¹æ— æ³•å¤„ç†æ­¤ç±»é—®é¢˜ã€‚  \nğŸ”¸APIæˆæœ¬åˆ†ææ˜¾ç¤ºï¼ŒMoDoraå•æŸ¥è¯¢èŠ±è´¹0.025ç¾å…ƒï¼Œç²¾åº¦æ¯”GPT-5é«˜16.24%ï¼Œæˆæœ¬ä»…ä¸ºå…¶2.5å€ï¼Œå®ç°ç²¾åº¦ä¸æ•ˆç‡çš„å®ç”¨å¹³è¡¡ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºå°†æ–‡æ¡£è§£æã€ç»“æ„å»ºæ¨¡ä¸æ£€ç´¢æ¨ç†è§£è€¦ä¸ºä¸‰ä¸ªæ­£äº¤ä½†ååŒçš„æ¨¡å—ï¼šç»„ä»¶åŒ–å°è£…è§£å†³OCRè¯­ä¹‰æ–­è£‚é—®é¢˜ï¼›CCTreeé¦–æ¬¡ç»Ÿä¸€å»ºæ¨¡è·¨æ¨¡æ€ç»„ä»¶å…³ç³»ä¸å¸ƒå±€åŒºåˆ†ï¼›é—®é¢˜é©±åŠ¨çš„åŒæ¨¡æ£€ç´¢æœºåˆ¶å…¼é¡¾ä½ç½®ç²¾ç¡®æ€§ä¸è¯­ä¹‰é²æ£’æ€§ã€‚å…¶æ ¸å¿ƒæ€æƒ³ä¸æ˜¯å †å å¤§æ¨¡å‹ï¼Œè€Œæ˜¯ç”¨ç»“æ„åŒ–å…ˆéªŒé™ä½LLMæ¨ç†è´Ÿæ‹…ï¼Œä¸ºåŠç»“æ„åŒ–æ–‡æ¡£ç†è§£æä¾›äº†å¯è§£é‡Šã€å¯æ‰©å±•çš„æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "OmniGAIA: Towards Native Omni-Modal AI Agents",
        "authors": [
            "Xiaoxi Li",
            "Wenxiang Jiao",
            "Jiarui Jin",
            "Shijian Wang",
            "Guanting Dong",
            "Jiajie Jin",
            "Hao Wang",
            "Yinuo Wang",
            "Ji-Rong Wen",
            "Yuan Lu",
            "Zhicheng Dou"
        ],
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG",
            "cs.MM"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22897v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22897v1",
        "summary": "Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.",
        "tag": "å¤šæ¨¡æ€è¯„ä¼°åŸºå‡†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22897v1ã€å¤šæ¨¡æ€è¯„ä¼°åŸºå‡†-ä¸­å›½äººæ°‘å¤§å­¦ã€‘OmniGAIA_ Towards Native Omni-Modal AI Agents.pdf",
        "institution_status": "keep",
        "institution": "ä¸­å›½äººæ°‘å¤§å­¦ã€å°çº¢ä¹¦",
        "first_institution": "ä¸­å›½äººæ°‘å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šOmniGAIA: Towards Native Omni-Modal AI Agents\nğŸŒæ¥æºï¼šarXiv, 2602.22897v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ„å»ºåŸç”Ÿå…¨æ¨¡æ€æ™ºèƒ½ä½“\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è¯„ä¼°å’Œæå‡AIæ¨¡å‹åœ¨è§†é¢‘ã€å›¾åƒã€éŸ³é¢‘ä¸‰æ¨¡æ€èåˆä¸‹çš„é•¿ç¨‹æ¨ç†ä¸å¤šæ­¥å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºé¦–ä¸ªé¢å‘åŸç”Ÿå…¨æ¨¡æ€æ™ºèƒ½ä½“çš„åŸºå‡†OmniGAIAåŠé…å¥—æ™ºèƒ½ä½“OmniAtlasï¼Œç³»ç»Ÿè§£å†³è·¨æ¨¡æ€æ·±åº¦æ¨ç†ä¸ä¸»åŠ¨æ„ŸçŸ¥ä¸‹çš„å·¥å…·ååŒéš¾é¢˜ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æ„å»ºOmniGAIAåŸºå‡†ï¼šåŸºäºå…¨æ¨¡æ€äº‹ä»¶å›¾æ–¹æ³•ï¼Œä»çœŸå®éŸ³è§†é¢‘ä¸å›¾æ–‡æ•°æ®ä¸­æŒ–æ˜ç»†ç²’åº¦ä¿¡å·ï¼Œå»ºæ¨¡è·¨æ¨¡æ€å®ä½“ä¸äº‹ä»¶å…³ç³»ï¼Œå¹¶é€šè¿‡ä¸»åŠ¨æ‰©å±•ä¸èŠ‚ç‚¹æ¨¡ç³ŠåŒ–ç”Ÿæˆéœ€å¤šè·³æ¨ç†ä¸å·¥å…·éªŒè¯çš„å¼€æ”¾å‹é—®ç­”ä»»åŠ¡ã€‚  \nğŸ”¸è®¾è®¡OmniAtlasæ™ºèƒ½ä½“ï¼šé‡‡ç”¨å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰èŒƒå¼ï¼Œæ”¯æŒâ€œæŒ‰éœ€æ„ŸçŸ¥â€â€”â€”å¯åŠ¨æ€è°ƒç”¨read_video/read_audio/read_imageç­‰æ“ä½œç²¾å‡†è·å–å…³é”®ç‰‡æ®µï¼Œé¿å…å…¨å±€ä¸‹é‡‡æ ·å¯¼è‡´çš„ä¿¡æ¯æŸå¤±ã€‚  \nğŸ”¸æå‡ºä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šå…ˆé€šè¿‡å›æº¯å¼•å¯¼çš„æ ‘æ¢ç´¢åˆæˆé«˜è´¨é‡å·¥å…·è°ƒç”¨è½¨è¿¹ï¼Œå†ç»“åˆè½¨è¿¹çº§ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸ç»†ç²’åº¦é”™è¯¯ä¿®æ­£æ–¹æ³•OmniDPOï¼Œèšç„¦ä¿®æ­£æ„ŸçŸ¥ã€æ¨ç†ã€å·¥å…·ä½¿ç”¨ç­‰æ¨¡å—çº§é”™è¯¯ã€‚  \nğŸ”¸å»ºç«‹ä¸¥æ ¼è´¨é‡ç®¡æ§æµç¨‹ï¼šèåˆå¤§æ¨¡å‹åˆç­›ï¼ˆåˆ¤æ–­é—®é¢˜è‡ªç„¶æ€§ã€æ¨¡æ€å¿…è¦æ€§ã€ç­”æ¡ˆå”¯ä¸€æ€§ï¼‰ã€éš¾åº¦å¢å¼ºä¸äººå·¥ä¸‰é‡æ ¡éªŒï¼Œç¡®ä¿ä»»åŠ¡å¯è§£ã€æ— æ­§ä¹‰ã€å…·æŒ‘æˆ˜æ€§ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸OmniGAIAæå…·æŒ‘æˆ˜æ€§ï¼šæœ€å¼ºé—­æºæ¨¡å‹Gemini-3-Proä»…è¾¾62.5 Pass@1ï¼Œè€Œæœ€ä½³å¼€æºåŸºçº¿Qwen3-Omniä»…ä¸º13.3ï¼Œå‡¸æ˜¾å½“å‰å¼€æºæ¨¡å‹åœ¨å…¨æ¨¡æ€ä»£ç†èƒ½åŠ›ä¸Šçš„å·¨å¤§å·®è·ã€‚  \nğŸ”¸å·¥å…·è°ƒç”¨æ˜¯æˆè´¥å…³é”®ï¼šå¤±è´¥æ¡ˆä¾‹ä¸­â€œæ— æ•ˆå·¥å…·è°ƒç”¨â€ä¸â€œæ¨ç†é”™è¯¯â€å æ¯”æœ€é«˜ï¼ˆè¾¾35%â€“96%ï¼‰ï¼Œä¸”ç¡¬ä»»åŠ¡ä¸­äºŒè€…å‘ˆçº§è”å¤±æ•ˆï¼›å•çº¯å¢åŠ è°ƒç”¨æ¬¡æ•°ä¸æå‡æˆåŠŸç‡ï¼Œä½æ•ˆâ€œå·¥å…·æŠ–åŠ¨â€æ™®éå­˜åœ¨ã€‚  \nğŸ”¸åŸç”Ÿæ„ŸçŸ¥ä¸å¯æ›¿ä»£ï¼šæ¶ˆèå®éªŒè¯æ˜ï¼Œå¯¹å¼ºæ¨¡å‹ï¼ˆå¦‚Gemini-3-Flashï¼‰ï¼ŒåŸç”Ÿå¤šæ¨¡æ€è¾“å…¥æ€§èƒ½æœ€ä¼˜ã€æˆæœ¬æœ€ä½ï¼›å¯¹å¼±æ¨¡å‹ï¼Œå·¥å…·è¾…åŠ©æ„ŸçŸ¥ä»…èƒ½æå‡ç®€å•ä»»åŠ¡è¡¨ç°ï¼Œæ— æ³•å¼¥è¡¥é•¿ç¨‹è·¨æ¨¡æ€æ¨ç†ç¼ºé™·ã€‚  \nğŸ”¸OmniAtlasæ˜¾è‘—ææ•ˆï¼šåœ¨Qwen3-Omniä¸Šå°†Pass@1ä»13.3æå‡è‡³20.8ï¼Œå·¥å…·è¯¯ç”¨ç‡ä¸‹é™21.7ä¸ªç™¾åˆ†ç‚¹ï¼ŒéªŒè¯å…¶è®­ç»ƒèŒƒå¼å¯¹è§£é”å¼€æºæ¨¡å‹ä»£ç†æ½œåŠ›çš„æœ‰æ•ˆæ€§ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ¨äºé¦–æ¬¡å°†â€œåŸç”Ÿå…¨æ¨¡æ€â€ä¸â€œä»£ç†å¼å·¥å…·ååŒâ€æ·±åº¦è€¦åˆï¼šä¸ä»…æ„å»ºäº†é¦–ä¸ªè¦†ç›–è§†é¢‘/å›¾åƒ/éŸ³é¢‘ã€å¼ºè°ƒå¤šè·³éªŒè¯ä¸å¼€æ”¾ç­”æ¡ˆçš„ä¸¥è‹›åŸºå‡†ï¼Œæ›´æå‡ºä¸»åŠ¨æ„ŸçŸ¥æœºåˆ¶ä¸ç»†ç²’åº¦çº é”™è®­ç»ƒæ¡†æ¶ï¼Œç›´å‡»å½“å‰æ¨¡å‹åœ¨è¯æ®é”šå®šã€å‡è®¾æ£€éªŒä¸è·¨æ¨¡æ€éªŒè¯ç­‰æ ¸å¿ƒç¯èŠ‚çš„è–„å¼±ç‚¹ï¼Œä¸ºä¸‹ä¸€ä»£é€šç”¨AIåŠ©æ‰‹æä¾›äº†å¯å¤ç°ã€å¯æ¼”è¿›çš„æŠ€æœ¯è·¯å¾„ã€‚\n    "
    },
    {
        "title": "PRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training",
        "authors": [
            "Yanyi Li",
            "Yimu Zhang",
            "Cong Fang"
        ],
        "categories": [
            "cs.LG"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.23111v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23111v1",
        "summary": "Activations have become the primary memory bottleneck in large-batch LLM training. However, existing compression methods fail to exploit the spectral structure of activations, resulting in slow convergence or limited compression. To address this, we bridge the relationship between the algorithm's fast convergence and the requirements for subspace projection, and show that an effective compression should yield an unbiased estimate of the original activation with low variance. We propose Principal-Random Subspace for LLM Activation Compression (PRAC), which novelly decomposes activations into two components: a principal subspace captured via SVD to retain dominant information, and a random subspace sampled from the orthogonal complement to approximate the tail. By introducing a precise scaling factor, we prove that PRAC yields an unbiased gradient estimator with minimum variance under certain conditions. Extensive experiments on pre-training and fine-tuning tasks demonstrate that PRAC achieves up to 36% total memory reduction with negligible performance degradation and minimal computational cost.",
        "tag": "è®­ç»ƒæ˜¾å­˜ä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.23111v1ã€è®­ç»ƒæ˜¾å­˜ä¼˜åŒ–-åŒ—äº¬å¤§å­¦ã€‘PRAC_ Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training.pdf",
        "institution_status": "keep",
        "institution": "åŒ—äº¬å¤§å­¦",
        "first_institution": "åŒ—äº¬å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šPRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training\nğŸŒæ¥æºï¼šarXiv, 2602.23111v1\n\nç¬”è®°æ ‡é¢˜ï¼šèåˆä¸»éšæœºå­ç©ºé—´å‹ç¼©æ¿€æ´»\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨å¤§æ‰¹æ¬¡LLMè®­ç»ƒä¸­é«˜æ•ˆå‹ç¼©æ¿€æ´»å€¼ï¼Œä½¿å…¶åœ¨æ˜¾è‘—é™ä½å†…å­˜çš„åŒæ—¶ä¸æŸå®³æ”¶æ•›é€Ÿåº¦å’Œæ¨¡å‹æ€§èƒ½ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºPRACæ–¹æ³•ï¼Œé¦–æ¬¡å°†æ¿€æ´»çš„è°±ç»“æ„ï¼ˆä¸»æˆåˆ†+é•¿å°¾ï¼‰æ˜¾å¼å»ºæ¨¡ï¼Œç†è®ºè¯æ˜å…¶åœ¨é€€åŒ–æ¡ä»¶ä¸‹å¯å®ç°æ— åä¸”æœ€å°æ–¹å·®çš„æ¢¯åº¦ä¼°è®¡ï¼Œè¾¾æˆæœ€é«˜36%æ€»å†…å­˜ç¼©å‡ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸åŸºäºä¼˜åŒ–æ”¶æ•›ç†è®ºï¼ŒæŒ‡å‡ºæœ‰æ•ˆæ¿€æ´»å‹ç¼©éœ€åŒæ—¶æ»¡è¶³æ— åæ€§ä¸ä½æ–¹å·®ï¼Œè€Œéä»…è¿½æ±‚é‡å»ºç²¾åº¦ã€‚  \nğŸ”¸è§‚å¯Ÿåˆ°LLMæ¿€æ´»å…·æœ‰â€œä¸»å¯¼å¥‡å¼‚å€¼+ç¼“æ…¢è¡°å‡é•¿å°¾â€çš„é€€åŒ–ç»“æ„ï¼Œæ®æ­¤æå‡ºåŒå­ç©ºé—´åˆ†è§£ï¼šSVDæå–ä¸»å­ç©ºé—´ä¿ç•™å¼ºä¿¡å·ï¼Œæ­£äº¤è¡¥ç©ºé—´å†…å‡åŒ€é‡‡æ ·éšæœºå­ç©ºé—´é€¼è¿‘é•¿å°¾ã€‚  \nğŸ”¸å¼•å…¥ç²¾ç¡®ç¼©æ”¾å› å­k=(nâˆ’râ‚)/râ‚‚ï¼Œä½¿éšæœºåˆ†é‡èƒ½é‡è¡¥å¿è¢«æˆªæ–­çš„å°¾éƒ¨èƒ½é‡ï¼Œä¸¥æ ¼ä¿è¯é‡å»ºä¸æ¢¯åº¦ä¼°è®¡çš„æ— åæ€§ã€‚  \nğŸ”¸è®¾è®¡åŠ¨æ€æ›´æ–°ç­–ç•¥ï¼šä¸»/éšæœºå­ç©ºé—´æŒ‰å›ºå®šæ­¥æ•°æƒ°æ€§æ›´æ–°ï¼›è·¨å±‚å…±äº«æŠ•å½±çŸ©é˜µï¼›çº¿æ€§å±‚ä¸éçº¿æ€§å±‚é‡‡ç”¨å·®å¼‚åŒ–ç§©é…ç½®ï¼Œå…¼é¡¾æ•ˆç‡ä¸ç¨³å®šæ€§ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸PRACåœ¨LLaMA/GPTç³»åˆ—é¢„è®­ç»ƒä¸­å®ç°æœ€é«˜36%æ€»å†…å­˜ä¸‹é™ï¼ˆå¦‚LLaMA-1Bä»94.5GBé™è‡³60.48GBï¼‰ï¼ŒéªŒè¯æŸå¤±ä¸åŸºçº¿å‡ ä¹æ— å·®å¼‚ã€‚  \nğŸ”¸ç›¸æ¯”çº¯ä¸»æˆåˆ†ï¼ˆPACï¼‰åæœŸæ”¶æ•›åœæ»ã€çº¯éšæœºï¼ˆRACï¼‰åˆæœŸé«˜æ–¹å·®éœ‡è¡ï¼ŒPRACå…¨ç¨‹ä¿æŒç¨³å®šå¿«é€Ÿæ”¶æ•›ï¼ŒéªŒè¯åŒå­ç©ºé—´ååŒä¼˜åŠ¿ã€‚  \nğŸ”¸åœ¨RoBERTaå¾®è°ƒä»»åŠ¡ä¸­ï¼ŒPRACä¸ä»…å†…å­˜å‡å°‘38%ï¼Œéƒ¨åˆ†ä»»åŠ¡æŒ‡æ ‡ç”šè‡³è¶…è¶Šå…¨å‚æ•°å¾®è°ƒï¼Œè¡¨æ˜éšæœºåˆ†é‡å…·æœ‰æ­£åˆ™åŒ–æ•ˆåº”ã€‚  \nğŸ”¸ä¸GaLoreã€RSOç­‰åŸºçº¿ç›¸æ¯”ï¼ŒPRACåœ¨åŒç­‰å†…å­˜ä¸‹æ€§èƒ½æ›´ä¼˜ï¼›ä¸å…ˆè¿›ä¼˜åŒ–å™¨ï¼ˆMuonã€Adam-miniï¼‰æ­£äº¤å…¼å®¹ï¼Œå¯å åŠ èŠ‚çœçº¦27%â€“32%å†…å­˜ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºå°†æ¿€æ´»çš„å†…åœ¨è°±ç»“æ„ï¼ˆä½ç§©+é•¿å°¾é€€åŒ–ï¼‰è½¬åŒ–ä¸ºå¯è¯æ˜æœ€ä¼˜çš„å‹ç¼©èŒƒå¼ï¼šä¸»å­ç©ºé—´ä¿éšœä¿¡æ¯ä¿çœŸï¼Œéšæœºå­ç©ºé—´ä»¥æœ€å°æ–¹å·®è¦†ç›–æ®‹å·®ï¼Œç¼©æ”¾å› å­å®ç°ç†è®ºæœ€ä¼˜æƒè¡¡ã€‚å…¶è´¡çŒ®ä¸ä»…æ˜¯å·¥ç¨‹æŠ€å·§ï¼Œæ›´æ˜¯é¦–æ¬¡ä¸ºæ¿€æ´»å‹ç¼©å»ºç«‹äº†â€œæ— å+æœ€å°æ–¹å·®â€çš„æ”¶æ•›æ€§ç†è®ºåŸºçŸ³ã€‚\n    "
    },
    {
        "title": "RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format",
        "authors": [
            "Zhehao Huang",
            "Yuhang Liu",
            "Baijiong Lin",
            "Yixin Lou",
            "Zhengbao He",
            "Hanling Tian",
            "Tao Li",
            "Xiaolin Huang"
        ],
        "categories": [
            "cs.LG",
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22538v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22538v1",
        "summary": "Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that naive merges are fragile because they overlook the output format mismatch between LRMs (with explicit thinking and response segments) and ITMs (answers-only). We introduce RAIN-Merging (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM's structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning & general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agent settings.",
        "tag": "æ¨¡å‹åˆå¹¶",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22538v1ã€æ¨¡å‹åˆå¹¶-ä¸Šæµ·äº¤é€šå¤§å­¦ã€‘RAIN-Merging_ A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format.pdf",
        "institution_status": "keep",
        "institution": "ä¸Šæµ·äº¤é€šå¤§å­¦ã€é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰",
        "first_institution": "ä¸Šæµ·äº¤é€šå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šRAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format\nğŸŒæ¥æºï¼šarXiv, 2602.22538v1\n\nç¬”è®°æ ‡é¢˜ï¼šRAIN-Mergingæå‡æŒ‡ä»¤éµå¾ª  \n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸ç ´åå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰ç»“æ„åŒ–æ€ç»´æ ¼å¼å’Œæ¨ç†èƒ½åŠ›çš„å‰æä¸‹ï¼Œæœ‰æ•ˆå¢å¼ºå…¶å¯¹å¤æ‚æŒ‡ä»¤çš„éµå¾ªèƒ½åŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºRAIN-Mergingâ€”â€”ä¸€ç§æ— éœ€æ¢¯åº¦ã€åˆ†ä¸¤é˜¶æ®µçš„æ¨¡å‹èåˆæ–¹æ³•ï¼Œé¦–æ¬¡å®ç°æŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¸æ˜¾å¼æ€ç»´æ ¼å¼çš„ååŒä¿ç•™ä¸å¢å¼ºã€‚  \n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸åˆ†æLRMä¸æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼ˆITMï¼‰ä»»åŠ¡å‘é‡åœ¨å…³é”®æ¨¡å—çš„ä¸»å­ç©ºé—´è¿‘ä¹æ­£äº¤ï¼Œä¸ºè½»é‡èåˆæä¾›ç†è®ºä¾æ®ã€‚  \nğŸ”¸å‘ç°ç›´æ¥èåˆä¼šç ´åLRMç‰¹æœ‰çš„â€œ<think>â€¦</think>â€æ€ç»´åˆ†æ®µè¾“å‡ºç»“æ„ï¼Œå¯¼è‡´æ ¼å¼é”™ä¹±ä¸çº¦æŸè¿åã€‚  \nğŸ”¸ç¬¬ä¸€é˜¶æ®µï¼šåœ¨å°è§„æ¨¡æ¨ç†æ ¡å‡†é›†ä¸Šï¼Œå°†ITMä»»åŠ¡å‘é‡æŠ•å½±è‡³æ€ç»´ç‰¹æ®Štokenå‰å‘ç‰¹å¾çš„é›¶ç©ºé—´ï¼Œå¼ºåˆ¶ä¿æŒæ€ç»´æ®µåˆ†å¸ƒä¸å˜ã€‚  \nğŸ”¸ç¬¬äºŒé˜¶æ®µï¼šåœ¨å°è§„æ¨¡æŒ‡ä»¤æ ¡å‡†é›†ä¸Šï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶é‡åŒ–å„æ¨¡å—å¯¹æŒ‡ä»¤æ®µçš„å…³æ³¨åº¦ï¼ˆalignmentï¼‰ä¸æ³„éœ²åº¦ï¼ˆleakageï¼‰ï¼Œæ¨å¯¼å‡ºå±‚/å¤´è‡ªé€‚åº”ç¼©æ”¾ç³»æ•°ã€‚  \nğŸ”¸é€šè¿‡äºŒé˜¶æ³°å‹’è¿‘ä¼¼ä¸å¯¹è§’Hessianä¼°è®¡ï¼Œåœ¨çº¯å‰å‘ä¼ æ’­ä¸‹é«˜æ•ˆæ±‚è§£æœ€ä¼˜åˆå¹¶ç³»æ•°ï¼Œå…¨ç¨‹æ— éœ€åå‘ä¼ æ’­ã€‚  \n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸RAIN-Mergingåœ¨4ä¸ªæŒ‡ä»¤éµå¾ªåŸºå‡†ä¸Šå¹³å‡æå‡+3.99%ï¼ŒåŒæ—¶åœ¨9ä¸ªæ¨ç†ä¸é€šç”¨èƒ½åŠ›åŸºå‡†ä¸Šå¹³å‡æå‡+4.56%ï¼Œæ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šå»é™¤ç¬¬ä¸€é˜¶æ®µä¼šå¯¼è‡´æ¨ç†èƒ½åŠ›ä¸‹é™2.47%ï¼Œå»é™¤ç¬¬äºŒé˜¶æ®µåˆ™æŒ‡ä»¤éµå¾ªæ€§èƒ½é™ä½1.53%ï¼Œä¸¤é˜¶æ®µäº’è¡¥ä¸”ç¼ºä¸€ä¸å¯ã€‚  \nğŸ”¸é›¶ç©ºé—´æŠ•å½±ä½¿æ€ç»´æ®µKLæ•£åº¦ä»0.1224é™è‡³0.0065ï¼Œ</think>ç¼ºå¤±ç‡ä»6.4%é™è‡³0%ï¼Œç¡®ä¿è¯æ€æ ¼å¼å®Œæ•´æ€§ã€‚  \nğŸ”¸æŒ‡ä»¤æ³¨æ„åŠ›åˆ†æ•°åœ¨å„å±‚å‡æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨æµ…å±‚æ³¨æ„åŠ›å¤´ä¸­ç³»æ•°è¾¾ä¸Šé™ï¼ŒéªŒè¯äº†æ¨¡å—å·®å¼‚åŒ–å“åº”å‡è®¾ã€‚  \nğŸ”¸åœ¨ALFWorldä¸WebShopç­‰ä»£ç†åœºæ™¯ä¸­ï¼Œæ€§èƒ½è¶…è¶ŠåŸå§‹LRMä¸ITMï¼Œè¯æ˜å…¶åœ¨å¤šè½®äº¤äº’ä»»åŠ¡ä¸­çš„å®ç”¨ä»·å€¼ã€‚  \n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†â€œæ€ç»´æ ¼å¼â€å»ºæ¨¡ä¸ºå¯çº¦æŸçš„å‰å‘ç‰¹å¾å­ç©ºé—´ï¼Œå¹¶ä»¥é›¶ç©ºé—´æŠ•å½±å®ç°ç»“æ„å¼ºä¿ï¼›åŒæ—¶é¦–åˆ›æŒ‡ä»¤æ³¨æ„åŠ›å¼•å¯¼çš„ç»†ç²’åº¦åˆå¹¶ç³»æ•°å­¦ä¹ èŒƒå¼ï¼Œå°†æŠ½è±¡çš„æŒ‡ä»¤éµå¾ªå…·è±¡ä¸ºå¯æµ‹é‡ã€å¯ä¼˜åŒ–çš„æ³¨æ„åŠ›è¡Œä¸ºã€‚å…¶æ ¸å¿ƒæ´è§åœ¨äºï¼šæŒ‡ä»¤éµå¾ªä¸æ˜¯è¦†ç›–æ¨ç†ï¼Œè€Œæ˜¯ç²¾å‡†è°ƒæ§æ¨ç†è¿‡ç¨‹ä¸­çš„æŒ‡ä»¤æ„ŸçŸ¥é€šè·¯â€”â€”è¿™ä¸€æ€æƒ³ä¸ºåç»­å¯æ§æ¨ç†ã€å¯ä¿¡ä»£ç†ç³»ç»Ÿæä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "Rejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference",
        "authors": [
            "Yushi Ye",
            "Feng Hong",
            "Huangjie Zheng",
            "Xu Chen",
            "Zhiyong Chen",
            "Yanfeng Wang",
            "Jiangchao Yao"
        ],
        "categories": [
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22868v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22868v1",
        "summary": "Diffusion Large Language Models (DLLMs) promise fast non-autoregressive inference but suffer a severe quality-speed trade-off in parallel decoding. This stems from the ''combinatorial contradiction'' phenomenon, where parallel tokens form semantically inconsistent combinations. We address this by integrating continuous representations into the discrete decoding process, as they preserve rich inter-position dependency. We propose ReMix (Rejection Mixing), a framework that introduces a novel Continuous Mixing State as an intermediate between the initial masked state and the final decoded token state. This intermediate state allows a token's representation to be iteratively refined in a continuous space, resolving mutual conflicts with other tokens before collapsing into a final discrete sample. Furthermore, a rejection rule reverts uncertain representations from the continuous state back to the masked state for reprocessing, ensuring stability and preventing error propagation. ReMix thus mitigates combinatorial contradictions by enabling continuous-space refinement during discrete diffusion decoding. Extensive experiments demonstrate that ReMix, as a training-free method, achieves a $2-8 \\times$ inference speedup without any quality degradation.",
        "tag": "é«˜æ•ˆæ¨ç†",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22868v1ã€é«˜æ•ˆæ¨ç†-ä¸Šæµ·äº¤é€šå¤§å­¦ã€‘Rejection Mixing_ Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference.pdf",
        "institution_status": "keep",
        "institution": "ä¸Šæµ·äº¤é€šå¤§å­¦",
        "first_institution": "ä¸Šæµ·äº¤é€šå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šRejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference\nğŸŒæ¥æºï¼šarXiv, 2602.22868v1\n\nç¬”è®°æ ‡é¢˜ï¼šè¿ç»­æ··åˆç¼“è§£ç»„åˆçŸ›ç›¾\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸ç‰ºç‰²ç”Ÿæˆè´¨é‡çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆDLLMï¼‰çš„å¹¶è¡Œè§£ç é€Ÿåº¦ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºReMixæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è¿ç»­æ··åˆçŠ¶æ€ä¸æ‹’ç»æœºåˆ¶ï¼Œåœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå°†DLLMæ¨ç†é€Ÿåº¦æå‡2â€“8å€ä¸”ä¸é™ä½ç”šè‡³æå‡è¾“å‡ºè´¨é‡ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è¯†åˆ«æ ¸å¿ƒç“¶é¢ˆä¸ºâ€œç»„åˆçŸ›ç›¾â€â€”â€”å¹¶è¡Œé‡‡æ ·æ—¶å„ä½ç½®ç‹¬ç«‹ç”Ÿæˆç¦»æ•£tokenï¼Œå¯¼è‡´è¯­ä¹‰å†²çªï¼ˆå¦‚â€œFull Pairâ€è€Œéâ€œFull Houseâ€ï¼‰ã€‚  \nğŸ”¸è®¾è®¡ä¸‰æ€åŠ¨æ€è§£ç æµç¨‹ï¼šä»æ©ç æ€ï¼ˆMï¼‰ç»è¿ç»­æ··åˆæ€ï¼ˆCï¼‰æœ€ç»ˆåç¼©è‡³ç¦»æ•£è¯å…ƒæ€ï¼ˆTï¼‰ï¼Œä½¿tokenè¡¨å¾å¯åœ¨è¿ç»­ç©ºé—´ä¸­è¿­ä»£ååŒä¼˜åŒ–ã€‚  \nğŸ”¸æå‡ºæ··åˆè§„åˆ™ï¼ˆMâ†’CâŸ³ï¼‰ï¼šå°†è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒçº¿æ€§åŠ æƒæ˜ å°„ä¸ºåµŒå…¥æ›´æ–°ï¼Œä¿ç•™è¯­ä¹‰ä¾èµ–ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”top-pç­–ç•¥ç¨³å®šä½ç½®ä¿¡åº¦ä½ç½®ã€‚  \nğŸ”¸å¼•å…¥æ‹’ç»è§„åˆ™ï¼ˆCâ†’Mï¼‰ï¼šå½“è¿ç»­æ­¥é—´è¾“å‡ºåˆ†å¸ƒJSæ•£åº¦è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œå°†ä¸ç¨³å®šä½ç½®é‡ç½®ä¸º[MASK]ï¼Œé˜²æ­¢é”™è¯¯ä¼ æ’­ï¼Œå¢å¼ºé²æ£’æ€§ã€‚  \nğŸ”¸å…¨ç¨‹æ— éœ€æ¨¡å‹å¾®è°ƒæˆ–é¢å¤–è®­ç»ƒï¼Œä»…ä¿®æ”¹è§£ç é€»è¾‘ï¼Œå…¼å®¹ç°æœ‰DLLMæ¶æ„ï¼ˆå¦‚LLaDAã€MMaDAï¼‰ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨8ä¸ªè¯­è¨€ä»»åŠ¡ï¼ˆGSM8Kã€ARC-Cç­‰ï¼‰ä¸Šï¼ŒReMixå¹³å‡å‡å°‘150â€“205æ­¥è§£ç ï¼Œå®ç°2.4Ã—â€“4.6Ã—ç«¯åˆ°ç«¯åŠ é€Ÿï¼ŒåŒæ—¶å‡†ç¡®ç‡å…¨é¢æå‡ï¼ˆæœ€é«˜+14.05%ï¼‰ã€‚  \nğŸ”¸åœ¨6ä¸ªå¤šæ¨¡æ€ä»»åŠ¡ï¼ˆFlickr30kã€MathVistaç­‰ï¼‰ä¸­åŒæ ·æœ‰æ•ˆï¼Œé€Ÿåº¦æå‡è¾¾3.75Ã—â€“7.52Ã—ï¼ŒCaptioningä¸å›¾è¡¨ç†è§£ä»»åŠ¡å¢ç›Šæœ€æ˜¾è‘—ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šç§»é™¤è¿ç»­æ··åˆæ€ä¼šé€€åŒ–ä¸ºæ™®é€šç½®ä¿¡åº¦è§£ç ï¼Œæ€§èƒ½æ˜æ˜¾ä¸‹é™ï¼›æ··åˆç³»æ•°Î²ä¸æ‹’ç»é˜ˆå€¼Ï„_rejå­˜åœ¨æœ€ä¼˜åŒºé—´ï¼Œè¿‡é«˜æˆ–è¿‡ä½å‡æŸå®³ç²¾åº¦ã€‚  \nğŸ”¸æ¡ˆä¾‹åˆ†ææ˜¾ç¤ºï¼ŒReMixèƒ½åœ¨æ›´å°‘æ­¥æ•°å†…é¿å…æ—©æœŸé”™è¯¯ï¼ˆå¦‚GSM8Kä¸­æ­£ç¡®ç´¯ç§¯è®¡ç®—è‡³23ï¼‰ï¼Œè€ŒåŸºçº¿å› é”™è¯¯tokenå›ºåŒ–å¯¼è‡´åç»­æ¨ç†å´©æºƒã€‚  \nğŸ”¸è®¡ç®—å¼€é”€æä½ï¼ŒReMixä¸“å±æ“ä½œä»…å æ€»è€—æ—¶9.12%ï¼Œæ— é¢å¤–å‰å‘ä¼ æ’­ï¼Œé«˜æ•ˆé€‚é…å®é™…éƒ¨ç½²ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†è¿ç»­è¡¨å¾å¼•å…¥ç¦»æ•£æ‰©æ•£è§£ç é—­ç¯ï¼Œä»¥è½»é‡çº§çŠ¶æ€æœºè®¾è®¡ç ´è§£â€œç»„åˆçŸ›ç›¾â€è¿™ä¸€æ ¹æœ¬æ€§éš¾é¢˜ï¼›å…¶æ‹’ç»æœºåˆ¶å·§å¦™å¹³è¡¡äº†è¿ç»­ä¼˜åŒ–çš„çµæ´»æ€§ä¸ç¦»æ•£è®­ç»ƒçš„å…ˆéªŒçº¦æŸï¼Œä½“ç°äº†å¯¹æ‰©æ•£èŒƒå¼æœ¬è´¨çš„æ·±åˆ»ç†è§£ã€‚\n    "
    },
    {
        "title": "Replacing Multi-Step Assembly of Data Preparation Pipelines with One-Step LLM Pipeline Generation for Table QA",
        "authors": [
            "Fengyu Li",
            "Junhao Zhu",
            "Kaishi Song",
            "Lu Chen",
            "Zhongming Yao",
            "Tianyi Li",
            "Christian S. Jensen"
        ],
        "categories": [
            "cs.DB",
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22721v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22721v1",
        "summary": "Table Question Answering (TQA) aims to answer natural language questions over structured tables. Large Language Models (LLMs) enable promising solutions to this problem, with operator-centric solutions that generate table manipulation pipelines in a multi-step manner offering state-of-the-art performance. However, these solutions rely on multiple LLM calls, resulting in prohibitive latencies and computational costs.\n  We propose Operation-R1, the first framework that trains lightweight LLMs (e.g., Qwen-4B/1.7B) via a novel variant of reinforcement learning with verifiable rewards to produce high-quality data-preparation pipelines for TQA in a single inference step. To train such an LLM, we first introduce a self-supervised rewarding mechanism to automatically obtain fine-grained pipeline-wise supervision signals for LLM training. We also propose variance-aware group resampling to mitigate training instability. To further enhance robustness of pipeline generation, we develop two complementary mechanisms: operation merge, which filters spurious operations through multi-candidate consensus, and adaptive rollback, which offers runtime protection against information loss in data transformation. Experiments on two benchmark datasets show that, with the same LLM backbone, Operation-R1 achieves average absolute accuracy gains of 9.55 and 6.08 percentage points over multi-step preparation baselines, with 79\\% table compression and a 2.2$\\times$ reduction in monetary cost.",
        "tag": "å¼ºåŒ–å­¦ä¹ ",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22721v1ã€å¼ºåŒ–å­¦ä¹ -æµ™æ±Ÿå¤§å­¦ã€‘Replacing Multi-Step Assembly of Data Preparation Pipelines with One-Step LLM Pipeline Generation for Table QA.pdf",
        "institution_status": "keep",
        "institution": "æµ™æ±Ÿå¤§å­¦ã€ä¸œåŒ—å¤§å­¦ã€å¥¥å°”å ¡å¤§å­¦",
        "first_institution": "æµ™æ±Ÿå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šReplacing Multi-Step Assembly of Data Preparation Pipelines with One-Step LLM Pipeline Generation for Table QA\nğŸŒæ¥æºï¼šarXiv, 2602.22721v1\n\nç¬”è®°æ ‡é¢˜ï¼šå•æ­¥ç”Ÿæˆè¡¨æ ¼é¢„å¤„ç†ç®¡é“\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¿è¯è¡¨æ ¼é—®ç­”ï¼ˆTQAï¼‰ç²¾åº¦çš„å‰æä¸‹ï¼Œå°†ä¼ ç»Ÿå¤šæ­¥è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆæ•°æ®å‡†å¤‡ç®¡é“çš„æ–¹å¼ï¼Œæ›¿æ¢ä¸ºä»…éœ€ä¸€æ¬¡è½»é‡çº§æ¨¡å‹æ¨ç†çš„é«˜æ•ˆæ–¹æ¡ˆï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºäº†Operation-R1æ¡†æ¶ï¼Œé¦–æ¬¡åˆ©ç”¨å¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è®­ç»ƒè½»é‡çº§LLMï¼ˆå¦‚Qwen-1.7B/4Bï¼‰ï¼Œå®ç°é¢å‘TQAçš„é«˜è´¨é‡æ•°æ®å‡†å¤‡ç®¡é“çš„å•æ­¥ç”Ÿæˆã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸æå‡ºORPOç®—æ³•â€”â€”ä¸€ç§æ“ä½œç²’åº¦çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œå°†æ•°æ®å‡†å¤‡å»ºæ¨¡ä¸ºåºåˆ—åŒ–æ“ä½œç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å¼•å…¥ç»†ç²’åº¦å¥–åŠ±æœºåˆ¶ã€‚  \nğŸ”¸è®¾è®¡è‡ªç›‘ç£ç®¡é“å¥–åŠ±æœºåˆ¶ï¼šåŸºäºâ€œå•å…ƒæ ¼èšç„¦QAâ€å‡è®¾ï¼Œé€æ“ä½œéªŒè¯æ˜¯å¦ä¿ç•™ç­”æ¡ˆå•å…ƒæ ¼ï¼ˆæ­£ç¡®æ€§å¥–åŠ±ï¼‰å¹¶è¡¡é‡å‹ç¼©ç‡ï¼ˆæ•ˆç‡å¥–åŠ±ï¼‰ï¼Œé¿å…ä¾èµ–äººå·¥æ ‡æ³¨ç®¡é“ã€‚  \nğŸ”¸æå‡ºæ–¹å·®æ„ŸçŸ¥çš„ç»„é‡é‡‡æ ·ï¼ˆVGRï¼‰ç­–ç•¥ï¼šåŠ¨æ€è¿‡æ»¤ä½æ–¹å·®æˆ–ä½è´¨é‡å“åº”ç»„ï¼Œç¼“è§£ç»†ç²’åº¦å¥–åŠ±ä¸‹RLè®­ç»ƒçš„ä¸ç¨³å®šæ€§ä¸ä¿¡å·åç¼©é—®é¢˜ã€‚  \nğŸ”¸æ„å»ºè¿è¡Œæ—¶é²æ£’æœºåˆ¶ï¼šOperation Mergeé€šè¿‡æ“ä½œå­—å…¸æ ‘å¯¹å¤šå€™é€‰ç®¡é“æŠ•ç¥¨èåˆï¼Œè¿‡æ»¤ä¸ç¨³å®šæ“ä½œï¼›Adaptive Rollbackåœ¨QAå¤±è´¥æ—¶é€çº§å›é€€è‡³æ›´åŸå§‹è¡¨æ ¼ï¼Œé˜²æ­¢ä¿¡æ¯ä¸¢å¤±ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨WikiTQå’ŒTabFactä¸Šï¼ŒOperation-R1-4Bç›¸æ¯”å¤šæ­¥åŸºçº¿å¹³å‡æå‡9.55%å’Œ6.08%ç»å¯¹å‡†ç¡®ç‡ï¼ŒåŒæ—¶å®ç°79%è¡¨æ ¼å‹ç¼©ä¸2.2å€æˆæœ¬ä¸‹é™ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šç§»é™¤ORPOè®­ç»ƒã€Operation Mergeæˆ–Adaptive Rollbackåˆ†åˆ«å¯¼è‡´5.77ã€7.36ã€5.02ä¸ªç™¾åˆ†ç‚¹æ€§èƒ½ä¸‹é™ï¼Œä¸‰è€…å‡ä¸å¯æˆ–ç¼ºã€‚  \nğŸ”¸VGRç­–ç•¥æ˜¾è‘—æå‡è®­ç»ƒç¨³å®šæ€§ï¼šå»é™¤åæ—©æœŸå¥–åŠ±éª¤é™ä¸”æ¢å¤ç¼“æ…¢ï¼Œè€Œå®Œæ•´VGRä½¿å¥–åŠ±æ›²çº¿æ›´å¹³æ»‘ã€æ”¶æ•›æ›´é«˜ã€‚  \nğŸ”¸Operatoråˆ†å¸ƒåˆ†æè¡¨æ˜ï¼šFilterä¸Selectæ“ä½œå¸¦æ¥æœ€å¤§å¢ç›Šï¼ˆè¶…17%ï¼‰ï¼Œå°è¯å…¶æœ‰æ•ˆè§£å†³â€œå¤§æµ·æé’ˆâ€é—®é¢˜ï¼›é•¿æ“ä½œé“¾å¯¹åº”å¤æ‚é—®é¢˜ï¼ŒOperation-R1å¢ç›Šéšé“¾é•¿å¢åŠ è€Œæ‰©å¤§ï¼Œä½“ç°å¼ºè¯­ä¹‰å¯¹é½èƒ½åŠ›ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†RLVRä»ç«¯åˆ°ç«¯é—®ç­”è¿ç§»è‡³å‰ç½®æ“ä½œç”Ÿæˆç¯èŠ‚ï¼Œä»¥â€œå¯éªŒè¯å•å…ƒæ ¼ä¿ç•™â€æ›¿ä»£æ¨¡ç³Šçš„æœ€ç»ˆç­”æ¡ˆè¯„ä¼°ï¼Œå®ç°äº†ç›‘ç£ä¿¡å·çš„ç»†ç²’åº¦ã€æ— æ ‡æ³¨ã€ä½æˆæœ¬ï¼›å…¶â€œæ“ä½œç©ºé—´çº¦æŸ+å•æ­¥ç”Ÿæˆ+è½»é‡æ¨¡å‹â€çš„èŒƒå¼ï¼Œä¸ºç»“æ„åŒ–æ•°æ®ç†è§£æä¾›äº†æ›´é«˜æ•ˆã€å¯éƒ¨ç½²çš„æ–°è·¯å¾„ã€‚\n    "
    },
    {
        "title": "Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents",
        "authors": [
            "Zhou Xu",
            "Bowen Zhou",
            "Qi Wang",
            "Shuwen Feng",
            "Jingyu Xiao"
        ],
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.23235v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23235v1",
        "summary": "Pure-vision GUI agents provide universal interaction capabilities but suffer from severe efficiency bottlenecks due to the massive spatiotemporal redundancy inherent in high-resolution screenshots and historical trajectories. We identify two critical misalignments in existing compression paradigms: the temporal mismatch, where uniform history encoding diverges from the agent's \"fading memory\" attention pattern, and the spatial topology conflict, where unstructured pruning compromises the grid integrity required for precise coordinate grounding, inducing spatial hallucinations. To address these challenges, we introduce GUIPruner, a training-free framework tailored for high-resolution GUI navigation. It synergizes Temporal-Adaptive Resolution (TAR), which eliminates historical redundancy via decay-based resizing, and Stratified Structure-aware Pruning (SSP), which prioritizes interactive foregrounds and semantic anchors while safeguarding global layout. Extensive evaluations across diverse benchmarks demonstrate that GUIPruner consistently achieves state-of-the-art performance, effectively preventing the collapse observed in large-scale models under high compression. Notably, on Qwen2-VL-2B, our method delivers a 3.4x reduction in FLOPs and a 3.3x speedup in vision encoding latency while retaining over 94% of the original performance, enabling real-time, high-precision navigation with minimal resource consumption.",
        "tag": "Agent æ¶æ„ä¼˜åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.23235v1ã€Agent æ¶æ„ä¼˜åŒ–-æ¸…åå¤§å­¦ã€‘Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents.pdf",
        "institution_status": "keep",
        "institution": "æ¸…åå¤§å­¦ã€é¦™æ¸¯ä¸­æ–‡å¤§å­¦",
        "first_institution": "æ¸…åå¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šSpatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents\nğŸŒæ¥æºï¼šarXiv, 2602.23235v1\n\nç¬”è®°æ ‡é¢˜ï¼šæ—¶ç©ºæ„ŸçŸ¥çš„GUIè§†è§‰å‹ç¼©\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸æŸå®³é«˜ç²¾åº¦åæ ‡å®šä½èƒ½åŠ›çš„å‰æä¸‹ï¼Œé«˜æ•ˆå‹ç¼©GUIä»£ç†ä¸­é«˜åˆ†è¾¨ç‡æˆªå›¾ä¸å†å²è½¨è¿¹å¸¦æ¥çš„æ—¶ç©ºå†—ä½™ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºæ— éœ€è®­ç»ƒçš„GUIPruneræ¡†æ¶ï¼Œé€šè¿‡æ—¶åºè‡ªé€‚åº”åˆ†è¾¨ç‡è°ƒæ•´ä¸åˆ†å±‚ç»“æ„æ„ŸçŸ¥å‰ªæï¼Œå…¼é¡¾æ•ˆç‡æå‡ä¸ç©ºé—´æ‹“æ‰‘å®Œæ•´æ€§ï¼Œå®ç°é«˜ä¿çœŸã€ä½å¼€é”€çš„GUIå¯¼èˆªã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è¯†åˆ«ä¸¤å¤§ç“¶é¢ˆï¼šå†å²ç»´åº¦å­˜åœ¨â€œæ—¶é—´è¡°å‡â€ç°è±¡ï¼ˆè¿‘æœŸå¸§éœ€é«˜åˆ†è¾¨ç‡ï¼Œè¿œæœŸä»…éœ€è¯­ä¹‰è½®å»“ï¼‰ï¼Œå½“å‰å¸§å­˜åœ¨â€œç¨€ç–-æ‹“æ‰‘å†²çªâ€ï¼ˆèƒŒæ™¯ token å æ¯”è¶…60%ï¼Œä½†éƒ¨åˆ†è¾¹ç•ŒåŒºåŸŸæ˜¯å…³é”®è¯­ä¹‰é”šç‚¹ï¼Œä¸å¯éšæ„è£å‰ªï¼‰ã€‚  \nğŸ”¸è®¾è®¡Temporal-Adaptive Resolutionï¼ˆTARï¼‰ï¼šä»¥å…¨å±€ token é¢„ç®—ä¸ºçº¦æŸï¼ŒæŒ‰çº¿æ€§è¡°å‡åˆ†é…å„å†å²å¸§åˆ†è¾¨ç‡ï¼Œå¯¹è¿œæœŸå¸§ç›´æ¥é™é‡‡æ ·ï¼Œä»æºå¤´å‰Šå‡è§†è§‰ç¼–ç è®¡ç®—é‡ã€‚  \nğŸ”¸è®¾è®¡Stratified Structure-aware Pruningï¼ˆSSPï¼‰ï¼šåœ¨æµ…å±‚LLMä¸­åˆ†ä¸‰çº§ä¿ç•™ tokenâ€”â€”ä¼˜å…ˆä¿ç•™äº¤äº’å‰æ™¯ï¼ˆæŒ‰é’®/è¾“å…¥æ¡†ï¼‰ã€æ¬¡é€‰è¯­ä¹‰æ˜¾è‘—èƒŒæ™¯ï¼ˆåŸºäºæ³¨æ„åŠ›æ’åºï¼‰ã€æœ€åç”¨å‡åŒ€ç½‘æ ¼é‡‡æ ·ï¼ˆUGSï¼‰è¡¥å…¨å…¨å±€å¸ƒå±€ï¼Œå¼ºåˆ¶ç»´æŒ2Dç©ºé—´ç»“æ„ã€‚  \nğŸ”¸é‡‡ç”¨çº¯æ¨ç†æ—¶ã€é›¶å‚æ•°æ›´æ–°çš„å³æ’å³ç”¨è®¾è®¡ï¼Œå…¼å®¹ä»»æ„é¢„è®­ç»ƒå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆå¦‚Qwen2-VLç³»åˆ—ï¼‰ï¼Œæ— éœ€å¾®è°ƒæˆ–é‡è®­ç»ƒã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨Mind2Webç­‰é«˜åˆ†è¾¨ç‡ç¨€ç–åœºæ™¯ä¸‹ï¼ŒGUIPruneræ˜¾è‘—ä¼˜äºDivPruneã€CDPrunerç­‰SOTAæ–¹æ³•ï¼Œé¿å…å…¶å› æ¿€è¿›é¢„LLMå‰ªæå¯¼è‡´çš„æ€§èƒ½å´©å¡Œï¼ˆå¦‚7Bæ¨¡å‹ä»34.7%â†’7.7%ï¼‰ã€‚  \nğŸ”¸TARæ¨¡å—åœ¨40%å†å²ä¿ç•™ç‡ä¸‹å³å¯æ¥è¿‘æ— æŸæ€§èƒ½ï¼Œåœ¨Mind2Webä¸Šç”šè‡³å°å¹…è¶…è¶ŠåŸå§‹æ¨¡å‹ï¼ŒéªŒè¯â€œè¡°å‡è®°å¿†â€æœºåˆ¶å¯æ»¤é™¤å¹²æ‰°å™ªå£°ã€‚  \nğŸ”¸SSPæ¨¡å—åœ¨45%å½“å‰å¸§ä¿ç•™ç‡ä¸‹ä»ä¿æŒ87.3%ç›¸å¯¹æ€§èƒ½ï¼Œè€Œéšæœºé‡‡æ ·ä»…è¾¾86.1%ï¼Œè¯æ˜å‡åŒ€ç½‘æ ¼é‡‡æ ·å¯¹é˜²æ­¢ç©ºé—´å¹»è§‰å…·æœ‰ä¸å¯æ›¿ä»£ä½œç”¨ã€‚  \nğŸ”¸åœ¨Qwen2-VL-2Bä¸Šå®ç°3.4Ã— FLOPsä¸‹é™ä¸3.3Ã—è§†è§‰ç¼–ç åŠ é€Ÿï¼ŒGPUæ˜¾å­˜å‹é™è‡³5.9GBï¼Œæ”¯æŒè¾¹ç¼˜è®¾å¤‡å®æ—¶è¿è¡Œã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†äººç±»å·¥ä½œè®°å¿†çš„â€œè¿‘å› æ•ˆåº”â€å»ºæ¨¡ä¸ºæ—¶åºè¡°å‡ç­–ç•¥ï¼Œå¹¶å°†GUIå¼ºç»“æ„åŒ–å…ˆéªŒï¼ˆå¦‚ç½‘æ ¼å¸ƒå±€ã€è¾¹ç•Œé”šç‚¹ï¼‰æ˜¾å¼åµŒå…¥å‰ªæè¿‡ç¨‹ï¼Œçªç ´äº†é€šç”¨å›¾åƒtokenå‹ç¼©æ–¹æ³•åœ¨åæ ‡æ•æ„Ÿä»»åŠ¡ä¸­çš„å±€é™ï¼›å…¶è§£è€¦å¼æ—¶ç©ºå¤„ç†èŒƒå¼ä¸ºå¤šæ¨¡æ€ä»£ç†çš„è½»é‡åŒ–æä¾›äº†å¯è¿ç§»çš„æ–¹æ³•è®ºå¯ç¤ºã€‚\n    "
    },
    {
        "title": "Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation",
        "authors": [
            "Zihang Xu",
            "Haozhi Xie",
            "Ziqi Miao",
            "Wuxuan Gong",
            "Chen Qian",
            "Lijun Li"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22556v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22556v1",
        "summary": "Large reasoning models (LRMs) achieve strong performance through extended reasoning traces, but they often exhibit overthinking behavior for low-complexity queries. Existing efforts to mitigate this issue are fundamentally limited by unstable accuracy-efficiency trade-offs and poor robustness to heterogeneous reasoning behaviors. To address these challenges, we propose a two-stage framework for stable adaptive thinking in LRMs. The framework first applies Hybrid Fine-Tuning to expose the model to both thinking and no-thinking behaviors, establishing well-conditioned initialization. It then performs adaptive reinforcement learning with Correctness-Preserving Advantage Shaping (CPAS) to avoid suppressing correct long-chain reasoning, and Length-Aware Gradient Regulation (LAGR) to stabilize optimization under severe reasoning-length heterogeneity. Extensive experiments on Qwen2.5-1.5B and 7B show consistent improvements over strong baselines, achieving up to +3.7/+3.6 accuracy points while reducing generated tokens by 40.6%/43.9%. Further analyses across varying problem difficulties and out-of-distribution tasks confirm the robustness and generalization of our approach.",
        "tag": "å¿«æ…¢æ€è€ƒ",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22556v1ã€å¿«æ…¢æ€è€ƒ-ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ã€‘Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation.pdf",
        "institution_status": "keep",
        "institution": "ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ã€åŒ—äº¬é‚®ç”µå¤§å­¦ã€ä¸­å›½äººæ°‘å¤§å­¦",
        "first_institution": "ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šStable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation\nğŸŒæ¥æºï¼šarXiv, 2602.22556v1\n\nç¬”è®°æ ‡é¢˜ï¼šç¨³å®šè‡ªé€‚åº”æ¨ç†æ¡†æ¶\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨å¤§å‹æ¨ç†æ¨¡å‹ä¸­å®ç°æ—¢é«˜æ•ˆåˆå‡†ç¡®çš„è‡ªé€‚åº”æ€è€ƒï¼Œé¿å…ç®€å•é—®é¢˜è¿‡æ€è€ƒã€å¤æ‚é—®é¢˜æ¬ æ€è€ƒï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºé¦–ä¸ªå…¼é¡¾è®­ç»ƒç¨³å®šæ€§ä¸éš¾åº¦æ„ŸçŸ¥èƒ½åŠ›çš„ä¸¤é˜¶æ®µè‡ªé€‚åº”æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŠ¿å¡‘å½¢ä¸é•¿åº¦æ„ŸçŸ¥æ¢¯åº¦è°ƒæ§è§£å†³ç²¾åº¦-æ•ˆç‡æƒè¡¡å¤±ç¨³å’Œæ¨ç†é“¾é•¿å¼‚è´¨æ€§å¯¼è‡´çš„ä¼˜åŒ–å´©æºƒé—®é¢˜ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨æ··åˆç›‘ç£å¾®è°ƒï¼ˆHFTï¼‰ï¼Œç»Ÿä¸€æ³¨å…¥â€œæ€è€ƒâ€ä¸â€œä¸æ€è€ƒâ€ä¸¤ç§è¡Œä¸ºæ¨¡å¼ï¼Œä¸ºåç»­å¼ºåŒ–å­¦ä¹ æä¾›è‰¯å¥½åˆå§‹åŒ–ã€‚  \nğŸ”¸ç¬¬äºŒé˜¶æ®µæ„å»ºæ”¹è¿›å‹GRPOå¼ºåŒ–å­¦ä¹ æµç¨‹ï¼Œå¼•å…¥æ­£ç¡®æ€§ä¿æŒä¼˜åŠ¿å¡‘å½¢ï¼ˆCPASï¼‰ï¼Œå¯¹æ­£ç¡®çŸ­é“¾é¢å¤–æ¿€åŠ±ã€å¯¹æ­£ç¡®é•¿é“¾ä¸æƒ©ç½šï¼Œé˜²æ­¢æ¢ç´¢èƒ½åŠ›é€€åŒ–ã€‚  \nğŸ”¸è®¾è®¡é•¿åº¦æ„ŸçŸ¥æ¢¯åº¦è°ƒèŠ‚ï¼ˆLAGRï¼‰ï¼Œä¾æ®å“åº”é•¿åº¦åŠ¨æ€é‡åŠ æƒæ¢¯åº¦ï¼Œå¹¶æ˜¾å¼å¢å¼ºæ§åˆ¶ä»¤ç‰Œæ¢¯åº¦ï¼Œç¼“è§£é•¿é“¾ç¨€é‡Šæ•ˆåº”ã€‚  \nğŸ”¸ä½¿ç”¨æ˜¾å¼æ§åˆ¶ä»¤ç‰Œï¼ˆ/think /no_thinkï¼‰å®ç°å•æ¨¡å‹å†…æ¨¡å¼è‡ªä¸»å†³ç­–ï¼Œæ— éœ€å¤–éƒ¨è·¯ç”±æˆ–å¤šæ¨¡å‹éƒ¨ç½²ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸åœ¨Qwen2.5-1.5B/7Bä¸Šï¼Œç›¸æ¯”æœ€å¼ºåŸºçº¿ï¼Œå‡†ç¡®ç‡æå‡3.7/3.6åˆ†ï¼Œç”Ÿæˆtokenå‡å°‘40.6%/43.9%ï¼ŒéªŒè¯æœ‰æ•ˆæ€§ä¸æ³›åŒ–æ€§ã€‚  \nğŸ”¸è·¨éš¾åº¦åˆ†ææ˜¾ç¤ºï¼šç®€å•é¢˜ï¼ˆMATH-500ï¼‰77.6%é€‰/no_thinkï¼Œéš¾é¢˜ï¼ˆAIMEï¼‰æ€è€ƒæ¯”ä¾‹æ˜¾è‘—ä¸Šå‡ï¼Œè¯æ˜éš¾åº¦æ„ŸçŸ¥å†³ç­–èƒ½åŠ›ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜CPASåŠ é€Ÿæ—©æœŸæ¢ç´¢å¹¶æå‡å³°å€¼æ€§èƒ½ï¼ŒLAGRä¸­Î²=0.4ä¸Î»=10æ—¶å–å¾—æœ€ä¼˜ç²¾åº¦-æ•ˆç‡å¹³è¡¡ã€‚  \nğŸ”¸åœ¨OODæ•°æ®é›†GPQAä¸Šä»æå‡å‡†ç¡®ç‡è‡³50.4%ï¼ˆ+2.9ï¼‰ã€tokenå‡å°‘51.0%ï¼Œè¯å®å¼ºæ³›åŒ–èƒ½åŠ›ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°å°†â€œç¨³å®šæ€§â€ä½œä¸ºè‡ªé€‚åº”æ¨ç†çš„æ ¸å¿ƒè®¾è®¡ç›®æ ‡ï¼Œè€Œéä»…è¿½æ±‚æŒ‡æ ‡æå‡ï¼›CPASä»å¥–åŠ±ä¿¡å·å±‚é¢ä¿éšœé•¿é“¾åˆç†æ€§ï¼ŒLAGRä»ä¼˜åŒ–æœºåˆ¶å±‚é¢åº”å¯¹é•¿åº¦å¼‚è´¨æ€§ï¼ŒäºŒè€…ååŒæ„æˆåŸç†æ¸…æ™°ã€å¯è§£é‡Šæ€§å¼ºçš„ç³»ç»Ÿæ€§è§£æ³•ï¼›æ§åˆ¶ä»¤ç‰Œ+å•æ¨¡å‹æ¶æ„å¤§å¹…é™ä½éƒ¨ç½²é—¨æ§›ï¼Œå…·å¤‡å®é™…è½åœ°ä»·å€¼ã€‚\n    "
    },
    {
        "title": "Towards Better RL Training Data Utilization via Second-Order Rollout",
        "authors": [
            "Zhe Yang",
            "Yudong Wang",
            "Rang Li",
            "Zhifang Sui"
        ],
        "categories": [
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22765v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22765v1",
        "summary": "Reinforcement Learning (RL) has empowered Large Language Models (LLMs) with strong reasoning capabilities, but vanilla RL mainly focuses on generation capability improvement by training with only first-order rollout (generating multiple responses for a question), and we argue that this approach fails to fully exploit the potential of training data because of the neglect of critique capability training. To tackle this problem, we further introduce the concept of second-order rollout (generating multiple critiques for a response) and propose a unified framework for jointly training generation and critique capabilities. Extensive experiments across various models and datasets demonstrate that our approach can utilize training data more effectively than vanilla RL and achieve better performance under the same training data. Additionally, we uncover several insightful findings regarding second-order rollout and critique training, such as the importance of label balance in critique training and the noise problem of outcome-based rewards, which can be mitigated through sampling techniques. Our work offers a preliminary exploration of dynamic data augmentation and joint generation-critique training in RL, providing meaningful inspiration for the further advancement of RL training",
        "tag": "å¼ºåŒ–å­¦ä¹ è®­ç»ƒ",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22765v1ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒ-åŒ—äº¬å¤§å­¦ã€‘Towards Better RL Training Data Utilization via Second-Order Rollout.pdf",
        "institution_status": "keep",
        "institution": "åŒ—äº¬å¤§å­¦ã€å­—èŠ‚è·³åŠ¨",
        "first_institution": "åŒ—äº¬å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼šTowards Better RL Training Data Utilization via Second-Order Rollout\nğŸŒæ¥æºï¼šarXiv, 2602.22765v1\n\nç¬”è®°æ ‡é¢˜ï¼šå¼•å…¥äºŒé˜¶å±•å¼€æå‡RLæ•°æ®åˆ©ç”¨\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸å¢åŠ æ ‡æ³¨æ•°æ®çš„å‰æä¸‹ï¼Œæ›´å……åˆ†åœ°æŒ–æ˜å¼ºåŒ–å­¦ä¹ ä¸­å·²æœ‰è®­ç»ƒæ•°æ®çš„æ½œåŠ›ï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºâ€œäºŒé˜¶å±•å¼€â€æ¦‚å¿µä¸ç”Ÿæˆ-æ‰¹åˆ¤è”åˆè®­ç»ƒæ¡†æ¶GC-RLï¼Œæ˜¾è‘—æå‡åŒä¸€è®­ç»ƒæ•°æ®ä¸‹çš„ç”Ÿæˆä¸æ‰¹åˆ¤åŒèƒ½åŠ›ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸å®šä¹‰äºŒé˜¶å±•å¼€ï¼šåœ¨ä¼ ç»Ÿä¸€é˜¶å±•å¼€ï¼ˆå¯¹ä¸€ä¸ªé—®é¢˜é‡‡æ ·å¤šä¸ªå›ç­”ï¼‰åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å¯¹æ¯ä¸ª<é—®é¢˜,å›ç­”>å¯¹é‡‡æ ·å¤šä¸ªæ‰¹åˆ¤ï¼Œå½¢æˆç¬¬äºŒå±‚ç­–ç•¥è¾“å‡ºã€‚  \nğŸ”¸æ„å»ºåŠ¨æ€æ•°æ®ç¼“å­˜æœºåˆ¶ï¼šé€šè¿‡æ•°æ®è¿‡æ»¤å™¨ä»ä¸€é˜¶å±•å¼€ç»“æœä¸­ç­›é€‰å‡ºæ­£ç¡®ä¸é”™è¯¯å›ç­”å„ä¸€ä¸ªï¼Œå­˜å…¥é—®ç­”ç¼“å­˜ï¼Œä¾›äºŒé˜¶å±•å¼€ä½¿ç”¨ã€‚  \nğŸ”¸æ··åˆ rollout è®­ç»ƒï¼šå°†ä¸€é˜¶ï¼ˆå›ç­”ï¼‰ä¸äºŒé˜¶ï¼ˆæ‰¹åˆ¤ï¼‰é‡‡æ ·ç»Ÿä¸€å»ºæ¨¡ï¼Œå…±äº«åŒä¸€ç­–ç•¥æ¨¡å‹ï¼Œå¹¶ç”¨GRPOç®—æ³•è”åˆæ›´æ–°ã€‚  \nğŸ”¸å†·å¯åŠ¨è®¾è®¡ï¼šå…ˆç”¨GPT-5è’¸é¦é«˜è´¨é‡æ‰¹åˆ¤æ•°æ®å¹¶è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œè§£å†³åŸºåº§æ¨¡å‹æŒ‡ä»¤éµå¾ªå¼±ã€æ‰¹åˆ¤æ ¼å¼æ··ä¹±é—®é¢˜ã€‚  \nğŸ”¸å¥–åŠ±è®¾è®¡åˆ†å±‚ï¼šå›ç­”é‡‡ç”¨ç¡®å®šæ€§è§„åˆ™å¥–åŠ±ï¼›æ‰¹åˆ¤ä»…åŸºäºæœ€ç»ˆäºŒå…ƒåˆ¤æ–­ç»™äºˆç»“æœå¥–åŠ±ï¼Œå¹¶å¼•å…¥åŠ æƒä¸å»å™ªæœºåˆ¶ç¼“è§£å™ªå£°ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸æ•°æ®è¿‡æ»¤å™¨è‡³å…³é‡è¦ï¼šéšæœºé‡‡æ ·å¯¼è‡´æ‰¹åˆ¤æ•°æ®ä¸¥é‡æ ‡ç­¾ä¸å¹³è¡¡ï¼ˆé”™è¯¯å›ç­”è¿œå¤šäºæ­£ç¡®ï¼‰ï¼Œä½¿æ¨¡å‹åå‘åˆ¤é”™ï¼›è¿‡æ»¤å™¨å¼ºåˆ¶1:1é…æ¯”åæ€§èƒ½æœ€ä¼˜ã€‚  \nğŸ”¸æ‰¹åˆ¤å¥–åŠ±å­˜åœ¨å›ºæœ‰å™ªå£°ï¼šå› æ— æ³•éªŒè¯ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä»…é ç»“æœå¥–åŠ±æ˜“è¯¯å¯¼ï¼›å¤šè½®è‡ªä¿®æ­£é‡‡æ ·å¯æœ‰æ•ˆå»å™ªï¼Œæå‡ç”Ÿæˆä¸æ‰¹åˆ¤åŒç²¾åº¦ã€‚  \nğŸ”¸åŠ¨æ€æ•°æ®ä¼˜äºé™æ€æ•°æ®ï¼šåœ¨GC-RLä¸­ï¼Œæ¨¡å‹è‡ªç”Ÿæˆçš„å›ç­”ä½œä¸ºæ‰¹åˆ¤è¾“å…¥æ•ˆæœæ›´å¥½ï¼›ä½†åœ¨çº¯æ‰¹åˆ¤è®­ç»ƒï¼ˆC-RLï¼‰ä¸­ï¼Œé™æ€é¢„ç½®æ•°æ®æ›´ç¨³å®šï¼Œé¿å…å¥–åŠ±ä½œå¼Šã€‚  \nğŸ”¸å¥–åŠ±å‡½æ•°å¯è°ƒæ§æ‰¹åˆ¤è¡Œä¸ºï¼šè°ƒæ•´æ­£è´Ÿæ ·æœ¬å¥–åŠ±æƒé‡ï¼Œèƒ½ç²¾ç»†æ§åˆ¶æ¨¡å‹çš„ç²¾ç¡®ç‡ä¸å¬å›ç‡å€¾å‘ï¼Œå®ç°ä»»åŠ¡é€‚é…çš„æ‰¹åˆ¤ç­–ç•¥å®šåˆ¶ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°æ€§åœ¨äºå°†â€œæ‰¹åˆ¤â€ä»è¾…åŠ©æ¨¡å—å‡æ ¼ä¸ºä¸â€œç”Ÿæˆâ€å¯¹ç­‰çš„æ ¸å¿ƒè®­ç»ƒç›®æ ‡ï¼Œå¹¶é€šè¿‡äºŒé˜¶å±•å¼€å®ç°é›¶æˆæœ¬æ•°æ®å¢å¼ºâ€”â€”æ‰€æœ‰æ‰¹åˆ¤æ•°æ®å‡ç”±æ¨¡å‹è‡ªèº«åœ¨ä¸€é˜¶å±•å¼€ç»“æœä¸ŠåŠ¨æ€ç”Ÿæˆï¼Œæ— éœ€äººå·¥æ ‡æ³¨æˆ–é¢å¤–æ•°æ®æºã€‚å…¶æ¡†æ¶æœ¬è´¨æ˜¯éšå¼å»ºæ¨¡ç”Ÿæˆä¸æ‰¹åˆ¤çš„èƒ½åŠ›è€¦åˆï¼Œå®éªŒè¯æ˜è”åˆè®­ç»ƒä¸ä»…ä¸äº’æ–¥ï¼Œåè€Œç›¸äº’å¢ç›Šï¼Œä¸ºLLMå¼ºåŒ–å­¦ä¹ æä¾›äº†æ–°èŒƒå¼ã€‚\n    "
    },
    {
        "title": "pQuant: Towards Effective Low-Bit Language Models via Decoupled Linear Quantization-Aware Training",
        "authors": [
            "Wenzheng Zhang",
            "Bingzheng Liu",
            "Yang Hu",
            "Xiaoying Bai",
            "Wentao Zhang",
            "Bin Cui"
        ],
        "categories": [
            "cs.LG",
            "cs.CL"
        ],
        "published_date": "2026-02-26",
        "arxiv_id": "2602.22592v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22592v1",
        "summary": "Quantization-Aware Training from scratch has emerged as a promising approach for building efficient large language models (LLMs) with extremely low-bit weights (sub 2-bit), which can offer substantial advantages for edge deployment. However, existing methods still fail to achieve satisfactory accuracy and scalability. In this work, we identify a parameter democratization effect as a key bottleneck: the sensitivity of all parameters becomes homogenized, severely limiting expressivity. To address this, we propose pQuant, a method that decouples parameters by splitting linear layers into two specialized branches: a dominant 1-bit branch for efficient computation and a compact high-precision branch dedicated to preserving the most sensitive parameters. Through tailored feature scaling, we explicitly guide the model to allocate sensitive parameters to the high-precision branch. Furthermore, we extend this branch into multiple, sparsely-activated experts, enabling efficient capacity scaling. Extensive experiments indicate our pQuant achieves state-of-the-art performance in extremely low-bit quantization.",
        "tag": "ä½æ¯”ç‰¹é‡åŒ–",
        "success": true,
        "file_path": "./output/2026-02-26/papers\\2602.22592v1ã€ä½æ¯”ç‰¹é‡åŒ–-åŒ—äº¬å¤§å­¦ã€‘pQuant_ Towards Effective Low-Bit Language Models via Decoupled Linear Quantization-Aware Training.pdf",
        "institution_status": "keep",
        "institution": "åŒ—äº¬å¤§å­¦ã€å¤æ—¦å¤§å­¦",
        "first_institution": "åŒ—äº¬å¤§å­¦",
        "institution_category": "å›½å†…å­¦æœ¯ç•Œ",
        "note": "ğŸ“–æ ‡é¢˜ï¼špQuant: Towards Effective Low-Bit Language Models via Decoupled Linear Quantization-Aware Training\nğŸŒæ¥æºï¼šarXiv, 2602.22592v1\n\nç¬”è®°æ ‡é¢˜ï¼šè§£è€¦æ•æ„Ÿå‚æ•°æå‡ä½æ¯”ç‰¹æ¨¡å‹è¡¨è¾¾åŠ›\n\nğŸ›ï¸æ–‡ç« ç®€ä»‹  \nğŸ”¸ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è§£å†³æä½æ¯”ç‰¹ï¼ˆäºš2æ¯”ç‰¹ï¼‰è¯­è¨€æ¨¡å‹åœ¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒä¸­å› å‚æ•°æ•æ„Ÿæ€§å‡è´¨åŒ–è€Œå¯¼è‡´çš„è¡¨è¾¾åŠ›ä¸å¯æ‰©å±•æ€§ç“¶é¢ˆï¼Ÿ  \nğŸ”¸ä¸»è¦è´¡çŒ®ï¼šæå‡ºpQuantæ–¹æ³•ï¼Œé€šè¿‡è§£è€¦çº¿æ€§å±‚ä¸º1æ¯”ç‰¹ä¸»å¹²ä¸é«˜ç²¾åº¦æ•æ„Ÿåˆ†æ”¯ï¼Œå¹¶å¼•å…¥ç‰¹å¾ç¼©æ”¾æœºåˆ¶åŠ¨æ€å¼•å¯¼æ•æ„Ÿå‚æ•°åˆ†é…ï¼Œæ˜¾è‘—æå‡æä½æ¯”ç‰¹æ¨¡å‹çš„å‡†ç¡®ç‡ä¸å¯æ‰©å±•æ€§ã€‚\n\nğŸ“é‡ç‚¹æ€è·¯  \nğŸ”¸è¯†åˆ«å¹¶å®šä¹‰â€œå‚æ•°æ°‘ä¸»åŒ–â€ç°è±¡â€”â€”å³æç«¯é‡åŒ–ä¸‹æƒé‡æ•æ„Ÿæ€§è¶‹äºå‡åŒ€ï¼ŒæŸå®³æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ã€‚  \nğŸ”¸è®¾è®¡è§£è€¦çº¿æ€§å±‚ï¼šåœ¨FFNä¸­å°†æƒé‡çŸ©é˜µæ‹†åˆ†ä¸º1æ¯”ç‰¹ä¸»åˆ†æ”¯ï¼ˆä¿éšœæ•ˆç‡ï¼‰å’Œ8æ¯”ç‰¹é«˜ç²¾åº¦å­åˆ†æ”¯ï¼ˆä¿ç•™æ•æ„Ÿå‚æ•°ï¼‰ï¼ŒäºŒè€…å¹¶è¡Œè®¡ç®—ååŠ æƒèåˆã€‚  \nğŸ”¸å¼•å…¥å¯å­¦ä¹ ç‰¹å¾ç¼©æ”¾ï¼ˆÎ±â‰«Î²åˆå§‹åŒ–ï¼‰ï¼Œä½¿æ¢¯åº¦ä¼˜å…ˆæµå‘é«˜ç²¾åº¦åˆ†æ”¯ï¼Œæ˜¾å¼å¼•å¯¼æ¨¡å‹å°†å…³é”®å‚æ•°åˆ†é…è‡³è¯¥è·¯å¾„ã€‚  \nğŸ”¸å°†é«˜ç²¾åº¦åˆ†æ”¯æ‰©å±•ä¸ºç¨€ç–æ¿€æ´»çš„å¤šä¸“å®¶ç»“æ„ï¼ˆTop-1è·¯ç”±ï¼‰ï¼Œå®ç°å®¹é‡é«˜æ•ˆæ‰©å±•è€Œä¸å¢åŠ æ¨ç†è´Ÿæ‹…ã€‚  \nğŸ”¸åœ¨MHAä¸­ç»Ÿä¸€é‡‡ç”¨1æ¯”ç‰¹é‡åŒ–ï¼Œåœ¨FFNä¸­èšç„¦è§£è€¦è®¾è®¡ï¼Œå…¼é¡¾æ•ˆç‡ä¸æ•æ„Ÿæ€§ä¿ç•™çš„æ¨¡å—å·®å¼‚åŒ–ç­–ç•¥ã€‚\n\nğŸ”åˆ†ææ€»ç»“  \nğŸ”¸pQuantåœ¨1.3Bè§„æ¨¡ä¸‹å°†å›°æƒ‘åº¦é™ä½32.0%ï¼Œå¹³å‡å‡†ç¡®ç‡è¶…è¶ŠBitNet 1-bitè¾¾2.4ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”é€¼è¿‘2-bit BitNet1.58æ€§èƒ½ã€‚  \nğŸ”¸æ•æ„Ÿæ€§åˆ†æè¯å®ï¼špQuantæˆåŠŸæ¢å¤å·®å¼‚åŒ–æ•æ„Ÿåˆ†å¸ƒï¼Œè€ŒBitNetå‘ˆç°å¹³å¦å‡è´¨åŒ–ï¼ŒéªŒè¯å‚æ•°æ°‘ä¸»åŒ–è¢«æœ‰æ•ˆç¼“è§£ã€‚  \nğŸ”¸æ‰©å±•è‡³8ä¸ª8æ¯”ç‰¹ä¸“å®¶æ—¶ï¼ŒpQuantåœ¨1.3Bå‚æ•°é‡ä¸‹åŒ¹é…FP16 LLaMA-2çš„ä¸‹æ¸¸æ€§èƒ½ï¼ŒåŒæ—¶æ¨ç†ååæå‡18.2%ã€‚  \nğŸ”¸å†…å­˜å ç”¨ä»…0.98GBï¼ˆ1.3B pQuantï¼‰ï¼Œè¾ƒLLaMA-2é™ä½92%ï¼Œè¾ƒBitNet1.58ä½31%ï¼Œä¸”å•æ¬¡å‰å‘ä»…æ¿€æ´»ä¸€ä¸ª8æ¯”ç‰¹åˆ†æ”¯ï¼Œå¸¦å®½å‹å¥½ã€‚  \nğŸ”¸æ¶ˆèå®éªŒè¯æ˜ï¼šç‰¹å¾ç¼©æ”¾ä¸å¯æˆ–ç¼ºï¼›å›ºå®šé«˜ç²¾åº¦ä½ç½®æˆ–é€šé“/åˆ†ç»„é‡åŒ–æ•ˆæœè¿œé€ŠäºåŠ¨æ€è§£è€¦+ç¼©æ”¾æœºåˆ¶ã€‚\n\nğŸ’¡ä¸ªäººè§‚ç‚¹  \nè®ºæ–‡åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ­ç¤ºå¹¶å‘½åâ€œå‚æ•°æ°‘ä¸»åŒ–â€è¿™ä¸€åˆ¶çº¦æä½æ¯”ç‰¹è®­ç»ƒçš„æ ¹æœ¬ç°è±¡ï¼Œå¹¶æå‡ºç»“æ„åŒ–è§£è€¦+åŠ¨æ€å¼•å¯¼çš„åŒé‡æœºåˆ¶äºˆä»¥ç ´è§£ï¼›å…¶å°†MoEæ€æƒ³è¿ç§»è‡³é‡åŒ–æ¶æ„è®¾è®¡ï¼Œä»¥æå°ç¡¬ä»¶å¼€é”€ï¼ˆå•ä¸“å®¶æ¿€æ´»ï¼‰æ¢å–æ˜¾è‘—è¡¨è¾¾åŠ›æå‡ï¼Œå…¼å…·ç†è®ºæ´å¯Ÿä¸å·¥ç¨‹å®æ•ˆæ€§ã€‚\n    "
    }
]